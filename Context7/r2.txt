================
CODE SNIPPETS
================
### Initialize uv Project and Install Dependencies

Source: https://developers.cloudflare.com/r2/data-catalog/get-started

These commands set up a new Python project using uv, create a virtual environment, and install the marimo package along with its required dependencies. This prepares the environment for creating and running the data catalog notebook.

```Bash
mkdir r2-data-catalog-tutorial
cd r2-data-catalog-tutorial
uv init
uv add marimo
```

--------------------------------

### Install Wrangler with Yarn

Source: https://developers.cloudflare.com/r2/get-started

This command installs the Wrangler CLI tool using Yarn, an alternative package manager for Node.js. Wrangler facilitates the deployment and management of Cloudflare services.

```bash
yarn global add @cloudflare/wrangler
```

--------------------------------

### Install Wrangler with pnpm

Source: https://developers.cloudflare.com/r2/get-started

This command installs the Wrangler CLI tool using pnpm, another package manager for Node.js. Wrangler is used for deploying and managing Cloudflare resources.

```bash
pnpm add -g @cloudflare/wrangler
```

--------------------------------

### Create R2 Bucket using Wrangler CLI

Source: https://developers.cloudflare.com/r2/data-catalog/get-started

This command creates a new R2 bucket named 'r2-data-catalog-tutorial' using the Wrangler CLI. Ensure you have Wrangler installed and are logged into your Cloudflare account.

```Bash
wrangler r2 bucket create r2-data-catalog-tutorial
```

--------------------------------

### Install Wrangler with npm

Source: https://developers.cloudflare.com/r2/get-started

This command installs the Wrangler CLI tool using npm, a package manager for Node.js. Wrangler is essential for deploying applications and managing Cloudflare services, including R2.

```bash
npm install -g @cloudflare/wrangler
```

--------------------------------

### Python Notebook for R2 Data Catalog Interaction

Source: https://developers.cloudflare.com/r2/data-catalog/get-started

This Python code snippet, intended for a marimo notebook, demonstrates how to connect to the R2 Data Catalog, create a namespace, define a PyArrow table, create an Iceberg table, append data, and query it. Replace placeholder values for CATALOG_URI, WAREHOUSE, and TOKEN with your actual credentials.

```Python
import marimo as mo
from pyiceberg.catalog import Catalog
from pyiceberg.schema import Schema
from pyiceberg.types import StructType, StringType, IntegerType
import pyarrow as pa

# Replace with your actual values from sections 2 and 3
CATALOG_URI = "<your-catalog-uri>"
WAREHOUSE = "<your-warehouse-name>"
TOKEN = "<your-api-token>"

config = {
    "uri": CATALOG_URI,
    "warehouse": WAREHOUSE,
    "token": TOKEN,
}

# Connect to the catalog
catalog = Catalog("rest", **config)

# Create a namespace if it doesn't exist
if "default" not in catalog.list_namespaces():
    catalog.create_namespace("default")

# Define a PyArrow table
my_arrow_table = pa.Table.from_pydict({
    "name": ["Alice", "Bob", "Charlie"],
    "age": [30, 25, 35],
})

# Define the Iceberg schema
schema = Schema.from_pyarrow(my_arrow_table.schema)

# Create or load the 'people' table in the 'default' namespace
table = catalog.create_table("default.people", schema)

# Append sample data to the table
table.append(my_arrow_table)

# Print the contents of the table
mo.display(table.scan().to_arrow())

# Optional: Drop the table
# catalog.drop_table("default.people")

```

--------------------------------

### Install SDKMAN Dependencies (Bash)

Source: https://developers.cloudflare.com/r2/llms-full

This bash script demonstrates how to install Java, Spark, and sbt using SDKMAN, which are prerequisites for building and running the R2 Data Catalog Demo project.

```bash
sdk install java 17.0.14-amzn
sdk install spark 3.5.3
sdk install sbt 1.10.11
```

--------------------------------

### Enable R2 Data Catalog using Wrangler CLI

Source: https://developers.cloudflare.com/r2/data-catalog/get-started

This command enables the data catalog for a specified R2 bucket using the Wrangler CLI. It outputs 'Warehouse' and 'Catalog URI' which are necessary for subsequent steps.

```Bash
wrangler r2 catalog enable r2-data-catalog-tutorial
```

--------------------------------

### Authenticate Wrangler

Source: https://developers.cloudflare.com/r2/get-started

This command initiates the authentication process for Wrangler, connecting it to your Cloudflare account. It typically opens a browser window for you to authorize access via an API token.

```bash
wrangler login
```

--------------------------------

### Install AWS SDK for Go

Source: https://developers.cloudflare.com/r2/llms-full

Command to install the necessary AWS SDK for Go packages required for interacting with Cloudflare R2. This includes packages for configuration, credentials, and the S3 service.

```shell
go get github.com/aws/aws-sdk-go-v2/config
go get github.com/aws/aws-sdk-go-v2/credentials
go get github.com/aws/aws-sdk-go-v2/service/s3
```

--------------------------------

### Create Cloudflare Worker Project (npm)

Source: https://developers.cloudflare.com/r2/llms-full

This command initiates the creation of a new Cloudflare Worker project using npm. It guides you through project setup, including choosing an example template, language, and version control.

```sh
npm create cloudflare@latest -- pdf-summarizer
```

--------------------------------

### Connect to R2 Data Catalog with DuckDB

Source: https://developers.cloudflare.com/r2/llms-full

This example demonstrates how to use the DuckDB CLI to connect to an R2 Data Catalog. It includes steps for installing and loading the Iceberg extension, creating a secret for R2 credentials, attaching the catalog, and querying data.

```sql
INSTALL iceberg;
LOAD iceberg;

CREATE SECRET r2_secret (
    TYPE ICEBERG,
    TOKEN '<token>'
);

ATTACH '<warehouse_name>' AS my_r2_catalog (
    TYPE ICEBERG,
    ENDPOINT '<catalog_uri>'
);

SHOW ALL TABLES;

SELECT * FROM my_r2_catalog.default.my_iceberg_table;
```

--------------------------------

### Create Cloudflare Worker Project (pnpm)

Source: https://developers.cloudflare.com/r2/llms-full

This command initiates the creation of a new Cloudflare Worker project using pnpm. It guides you through project setup, including choosing an example template, language, and version control.

```sh
pnpm create cloudflare@latest pdf-summarizer
```

--------------------------------

### Enable Sippy via Wrangler CLI

Source: https://developers.cloudflare.com/r2/llms-full

Enables the Sippy on-demand migration feature for a specified R2 bucket using the Wrangler CLI. This command prompts the user to select a cloud storage provider and guides them through the setup process.

```sh
npx wrangler r2 bucket sippy enable <BUCKET_NAME>
```

--------------------------------

### Create Cloudflare Worker Project (yarn)

Source: https://developers.cloudflare.com/r2/llms-full

This command initiates the creation of a new Cloudflare Worker project using yarn. It guides you through project setup, including choosing an example template, language, and version control.

```sh
yarn create cloudflare pdf-summarizer
```

--------------------------------

### Authenticate R2 API using Auth Tokens (Examples)

Source: https://developers.cloudflare.com/r2/llms-full

Provides examples for authenticating against the R2 API using authentication tokens in JavaScript, Python, and Go.

```javascript
// JavaScript example for R2 authentication
// Refer to: https://developers.cloudflare.com/r2/examples/authenticate-r2-auth-tokens/
```

```python
# Python example for R2 authentication
# Refer to: https://developers.cloudflare.com/r2/examples/authenticate-r2-auth-tokens/
```

```go
// Go example for R2 authentication
// Refer to: https://developers.cloudflare.com/r2/examples/authenticate-r2-auth-tokens/
```

--------------------------------

### Start Development Server

Source: https://developers.cloudflare.com/r2/llms-full

This command restarts the development server, allowing you to test the implemented changes for file uploads and R2 integration.

```sh
npm run dev
```

--------------------------------

### Install unpdf Library

Source: https://developers.cloudflare.com/r2/llms-full

Installs the 'unpdf' library, which is used for working with PDF files, using npm, yarn, or pnpm.

```sh
npm i unpdf
```

```sh
yarn add unpdf
```

```sh
pnpm add unpdf
```

--------------------------------

### Install AWS SDK for JavaScript

Source: https://developers.cloudflare.com/r2/llms-full

Installs the `aws-sdk` package required for interacting with the S3 API in a JavaScript environment.

```sh
npm install aws-sdk
```

--------------------------------

### Initialize Worker Project with C3

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-usage

This section details how to create a new Cloudflare Workers project using the C3 CLI. It outlines the setup options for creating a 'Hello World' example with a 'Worker only' template, using JavaScript, and configuring version control and deployment settings.

```bash
npx c3 hello-world

```

--------------------------------

### Create R2 Worker Application with C3

Source: https://developers.cloudflare.com/r2/llms-full

Commands to create a new Cloudflare Worker application with R2 integration using npm, yarn, or pnpm. It guides through the initial setup prompts for the application.

```sh
npm create cloudflare@latest -- r2-worker
```

```sh
yarn create cloudflare r2-worker
```

```sh
pnpm create cloudflare@latest r2-worker
```

--------------------------------

### Boto3 Example Script for Cloudflare R2

Source: https://developers.cloudflare.com/r2/examples/aws/boto3

An example Python script demonstrating basic operations with Cloudflare R2 using Boto3. This includes uploading a file and listing objects in a bucket.

```Python
import boto3

r2_client = boto3.client(
    "s3",
    endpoint_url="https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com",
    aws_access_key_id="YOUR_ACCESS_KEY_ID",
    aws_secret_access_key="YOUR_SECRET_ACCESS_KEY"
)

# Example: Upload a file
with open("my_file.txt", "rb") as f:
    r2_client.upload_fileobj(f, "my-bucket", "my_file.txt")

# Example: List objects in a bucket
response = r2_client.list_objects_v2(Bucket="my-bucket")
for obj in response.get('Contents', []):
    print(obj['Key'])
```

--------------------------------

### Install boto3 for Python

Source: https://developers.cloudflare.com/r2/llms-full

Installs the `boto3` package, which is the Amazon Web Services (AWS) SDK for Python, used here for interacting with the R2 API.

```sh
pip install boto3
```

--------------------------------

### Fetch Object from Cloudflare R2 using Go SDK

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to fetch an object from a Cloudflare R2 bucket using the AWS SDK for Go. It includes setting up the R2 client with environment variables for account ID, access key, and bucket name, hashing the secret key, and configuring the endpoint resolver. The example shows how to get an object by its key and read its content.

```go
package main


import (
    "context"
    "crypto/sha256"
    "encoding/hex"
    "fmt"
    "io"
    "log"
    "os"


    "github.com/aws/aws-sdk-go-v2/aws"
    "github.com/aws/aws-sdk-go-v2/config"
    "github.com/aws/aws-sdk-go-v2/credentials"
    "github.com/aws/aws-sdk-go-v2/service/s3"
)


func main() {
    // Load environment variables
    accountID := os.Getenv("R2_ACCOUNT_ID")
    accessKeyID := os.Getenv("R2_ACCESS_KEY_ID")
    secretAccessKey := os.Getenv("R2_SECRET_ACCESS_KEY")
    bucketName := os.Getenv("R2_BUCKET_NAME")


    // Hash the secret access key
    hasher := sha256.New()
    hasher.Write([]byte(secretAccessKey))
    hashedSecretKey := hex.EncodeToString(hasher.Sum(nil))


    // Configure the S3 client for Cloudflare R2
    r2Resolver := aws.EndpointResolverWithOptionsFunc(func(service, region string, options ...interface{}) (aws.Endpoint, error) {
        return aws.Endpoint{
            URL: fmt.Sprintf("https://%s.r2.cloudflarestorage.com", accountID),
        }, nil
    })


    cfg, err := config.LoadDefaultConfig(context.TODO(),
        config.WithEndpointResolverWithOptions(r2Resolver),
        config.WithCredentialsProvider(credentials.NewStaticCredentialsProvider(accessKeyID, hashedSecretKey, "")),
        config.WithRegion("auto"), // Cloudflare R2 doesn't use regions, but this is required by the SDK
    )
    if err != nil {
        log.Fatalf("Unable to load SDK config, %v", err)
    }


    // Create an S3 client
    client := s3.NewFromConfig(cfg)


    // Specify the object key
    objectKey := "2024/08/02/ingested_0001.parquet"


    // Fetch the object
    output, err := client.GetObject(context.TODO(), &s3.GetObjectInput{
        Bucket: aws.String(bucketName),
        Key:    aws.String(objectKey),
    })
    if err != nil {
        log.Fatalf("Unable to fetch object, %v", err)
    }
    defer output.Body.Close()


    fmt.Println("Successfully fetched the object")


    // Process the object content as needed
    // For example, to save the file:
    // file, err := os.Create("ingested_0001.parquet")
    // if err != nil {
    //   log.Fatalf("Unable to create file, %v", err)
    // }
    // defer file.Close()
    // _, err = io.Copy(file, output.Body)
    // if err != nil {
    //   log.Fatalf("Unable to write file, %v", err)
    // }


    // Or to read the content:
    content, err := io.ReadAll(output.Body)
    if err != nil {
        log.Fatalf("Unable to read object content, %v", err)
    }
    fmt.Printf("Object content length: %d bytes\n", len(content))
}

```

--------------------------------

### Create R2 Bucket (CLI Example)

Source: https://developers.cloudflare.com/r2/tutorials/cloudflare-access

This snippet demonstrates how to create a new Cloudflare R2 bucket using the Cloudflare CLI. Ensure you have the CLI installed and configured with your account credentials.

```bash
wrangler r2 bucket create my-new-bucket
```

--------------------------------

### Connect to R2 Data Catalog with PyIceberg

Source: https://developers.cloudflare.com/r2/llms-full

This Python example shows how to use the PyIceberg library to connect to an R2 Data Catalog. It outlines the necessary imports and defines variables for catalog connection details.

```python
import pyarrow as pa
from pyiceberg.catalog.rest import RestCatalog
from pyiceberg.exceptions import NamespaceAlreadyExistsError


# Define catalog connection details (replace variables)
WAREHOUSE = "<WAREHOUSE>"
TOKEN = "<TOKEN>"
CATALOG_URI = "<CATALOG_URI>"

```

--------------------------------

### Start Local Development Server

Source: https://developers.cloudflare.com/r2/llms-full

This command initiates a local development server for your Cloudflare Worker project. It allows you to test your front-end and back-end code locally before deploying.

```sh
npm run dev
```

--------------------------------

### Install Wrangler with pnpm

Source: https://developers.cloudflare.com/r2/llms-full

Installs the latest version of Wrangler as a development dependency in your project using pnpm.

```sh
pnpm add -D wrangler@latest
```

--------------------------------

### Install Wrangler with yarn

Source: https://developers.cloudflare.com/r2/llms-full

Installs the latest version of Wrangler as a development dependency in your project using yarn.

```sh
yarn add -D wrangler@latest
```

--------------------------------

### Install Wrangler with npm

Source: https://developers.cloudflare.com/r2/llms-full

Installs the latest version of Wrangler as a development dependency in your project using npm.

```sh
npm i -D wrangler@latest
```

--------------------------------

### Setup and List Buckets with AWS SDK for Ruby (Ruby)

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to set up the AWS SDK for Ruby to interact with Cloudflare R2. This includes adding the 'aws-sdk-s3' gem to your Gemfile, initializing the S3 client with R2 specific endpoint and credentials, and listing all buckets in your account.

```ruby
gem "aws-sdk-s3"
```

```ruby
require "aws-sdk-s3"


@r2 = Aws::S3::Client.new(
  access_key_id: "#{access_key_id}",
  secret_access_key: "#{secret_access_key}",
  endpoint: "https://#{cloudflare_account_id}.r2.cloudflarestorage.com",
  region: "auto",
)


# List all buckets on your account
puts @r2.list_buckets
```

--------------------------------

### Install Wrangler CLI

Source: https://developers.cloudflare.com/r2/data-migration/sippy

This command installs Wrangler, the Cloudflare Developer Platform CLI, using npm. Wrangler is required to manage R2 buckets and enable/disable features like Sippy via the command line.

```bash
npm install -g @cloudflare/wrangler
```

--------------------------------

### List R2 Buckets using Postman

Source: https://developers.cloudflare.com/r2/tutorials/postman

Verify your R2 setup by listing existing buckets using the `GET` ListBuckets request in Postman. This request uses AWS SigV4 authentication and should return a 200 OK response with a list of buckets.

```Postman
Navigate to **Cloudflare R2** > **Buckets** > **`GET`ListBuckets** and select **Send**.
```

--------------------------------

### Example R2 Audit Log Entry

Source: https://developers.cloudflare.com/r2/llms-full

An example of an audit log entry in JSON format, demonstrating the creation of a new R2 bucket. It includes details about the action, actor, resource, and timestamps.

```json
{
  "action": { "info": "CreateBucket", "result": true, "type": "create" },
  "actor": {
    "email": "<ACTOR_EMAIL>",
    "id": "3f7b730e625b975bc1231234cfbec091",
    "ip": "fe32:43ed:12b5:526::1d2:13",
    "type": "user"
  },
  "id": "5eaeb6be-1234-406a-87ab-1971adc1234c",
  "interface": "API",
  "metadata": { "zone_name": "r2.cloudflarestorage.com" },
  "newValue": "",
  "newValueJson": {},
  "oldValue": "",
  "oldValueJson": {},
  "owner": { "id": "1234d848c0b9e484dfc37ec392b5fa8a" },
  "resource": { "id": "my-bucket", "type": "r2.bucket" },
  "when": "2024-07-15T16:32:52.412Z"
}
```

--------------------------------

### R2 Bucket GET Operation Example

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Shows how to retrieve an object's metadata or body using the `get` method on an R2 bucket binding. Handles metadata-only retrieval and streaming.

```javascript
/**
 * @param {string} key
 * @param {R2GetOptions} [options]
 * @returns {Promise<R2ObjectBody | R2Object | null>}
 */
async function getObject(key, options) {
  // Assuming MY_BUCKET is bound in wrangler.toml
  const object = await MY_BUCKET.get(key, options);
  return object;
}
```

--------------------------------

### List Cloudflare R2 Buckets and Objects in Java

Source: https://developers.cloudflare.com/r2/llms-full

This example demonstrates how to list all buckets in Cloudflare R2 and then list objects within a specific bucket using the Java SDK. It requires the R2 client to be initialized.

```Java
import com.cloudflare.r2.R2Client;
import com.cloudflare.r2.R2Object;
import com.cloudflare.r2.R2Bucket;

import java.util.List;

public class R2Example {

    public static void main(String[] args) {
        // Replace with your actual R2 credentials and endpoint
        String accountId = System.getenv("CLOUDFLARE_ACCOUNT_ID");
        String accessKeyId = System.getenv("CLOUDFLARE_ACCESS_KEY_ID");
        String secretAccessKey = System.getenv("CLOUDFLARE_SECRET_ACCESS_KEY");
        String endpoint = "https://" + accountId + ".r2.cloudflarestorage.com";

        R2Client r2Client = new R2Client(accountId, accessKeyId, secretAccessKey, endpoint);

        // List all buckets
        System.out.println("Listing all buckets:");
        r2Client.listBuckets().forEach(bucket ->
            System.out.println("* " + bucket.name())
        );


        // List objects in a specific bucket
        String bucketName = "demos";
        System.out.println("\nObjects in bucket '" + bucketName + "':");
        r2Client.listObjects(bucketName).forEach(object ->
            System.out.printf("* %s (size: %d bytes, modified: %s)%n",
                object.key(),
                object.size(),
                object.lastModified())
        );
    }
}
```

--------------------------------

### Cloudflare R2 S3 API Examples

Source: https://developers.cloudflare.com/r2/llms-full

Provides examples of how to interact with the Cloudflare R2 storage using the S3 API, including signing requests. This is useful for developers migrating from or integrating with AWS S3.

```bash
# Example using curl for S3 API requests
curl -X PUT -T "./my-local-file.txt" \
  --header "Authorization: AWS4-HMAC-SHA256 Credential=YOUR_ACCESS_KEY_ID/20230101/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=YOUR_SIGNATURE" \
  https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com/YOUR_BUCKET_NAME/my-remote-file.txt
```

--------------------------------

### Configure R2 for Mastodon Instance

Source: https://developers.cloudflare.com/r2/llms-full

This guide explains how to configure Cloudflare R2 as the object storage for a self-hosted Mastodon instance. It covers setup for new instances and migration for existing ones, referencing Mastodon's official documentation for initial setup.

```Shell
chcp 65001
```

--------------------------------

### List Buckets with AWS SDK v3

Source: https://developers.cloudflare.com/r2/llms-full

Provides an example of listing all buckets in a Cloudflare R2 account using the `ListBucketsCommand` with the AWS SDK for JavaScript v3.

```typescript
console.log(await S3.send(new ListBucketsCommand({})));
```

--------------------------------

### Example R2 Access Policy

Source: https://developers.cloudflare.com/r2/llms-full

An example of an access policy for R2, specifying allowed resources and permission groups. This policy grants read access to objects in specific buckets.

```json
[
  {
    "id": "f267e341f3dd4697bd3b9f71dd96247f",
    "effect": "allow",
    "resources": {
      "com.cloudflare.edge.r2.bucket.4793d734c0b8e484dfc37ec392b5fa8a_default_my-bucket": "*",
      "com.cloudflare.edge.r2.bucket.4793d734c0b8e484dfc37ec392b5fa8a_eu_my-eu-bucket": "*"
    },
    "permission_groups": [
      {
        "id": "6a018a9f2fc74eb6b293b0c548f38b39",
        "name": "Workers R2 Storage Bucket Item Read"
      }
    ]
  }
]
```

--------------------------------

### Example Access Policy for API Token Creation

Source: https://developers.cloudflare.com/r2/data-catalog/manage-catalogs

An example of an Access Policy configuration for creating an API token programmatically. This policy must include specific permission groups for both R2 Data Catalog and R2 storage access.

```json
{
  "name": "R2 Catalog Access",
  "description": "Access to R2 Data Catalog and Storage",
  "policies": [
    {
      "permission_groups": [
        "R2_CATALOG_READ_WRITE",
        "R2_STORAGE_READ_WRITE"
      ]
    }
  ]
}
```

--------------------------------

### Enable Sippy on R2 bucket using Wrangler

Source: https://developers.cloudflare.com/r2/data-migration/sippy

This command enables the Sippy feature on your R2 bucket using the Wrangler CLI. It will prompt you to select a cloud storage provider and guide you through the setup process, including authentication.

```bash
wrangler login
r2 bucket sippy enable
```

--------------------------------

### R2 Multipart Upload Creation Example

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Provides an example of initiating a multipart upload for a large object using the `createMultipartUpload` method.

```javascript
/**
 * @param {string} key
 * @param {R2MultipartOptions} [options]
 * @returns {Promise<R2MultipartUpload>}
 */
async function createUpload(key, options) {
  // Assuming MY_BUCKET is bound in wrangler.toml
  const multipartUpload = await MY_BUCKET.createMultipartUpload(key, options);
  return multipartUpload;
}
```

--------------------------------

### Authenticate R2 API with S3 API using Go

Source: https://developers.cloudflare.com/r2/examples/authenticate-r2-auth-tokens

This Go example shows how to authenticate with Cloudflare R2 using the S3 API and an API token. It requires the aws-sdk-go-v2 package and proper setup of environmental variables.

```go
package main

import (
	"context"
	"fmt"
	"log"
	"os"

	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/service/s3"
)

func main() {
	// Ensure CLOUDFLARE_ACCOUNT_ID, ACCESS_KEY_ID, and SECRET_ACCESS_KEY are set as environment variables
	// For R2, the region should be the name of your R2 bucket.

	cfg, err := config.LoadDefaultConfig(context.TODO(),
		config.WithRegion("us-east-1"), // This region is often ignored for R2 endpoint configuration
		config.WithEndpointResolverV2(aws.EndpointResolverFunc(func(service, region string) (aws.Endpoint, error) {
			return aws.Endpoint{
				URL: fmt.Sprintf("https://%s.r2.cloudflarestorage.com", os.Getenv("CLOUDFLARE_ACCOUNT_ID")),
			},
			nil
		})),
		config.WithCredentialsProvider(aws.NewStaticCredentialsProvider(
			os.Getenv("ACCESS_KEY_ID"),
			os.Getenv("SECRET_ACCESS_KEY"),
			"", // Session token, not typically used with R2 API tokens
		)),
	)
	if err != nil {
		log.Fatalf("unable to load SDK config, %v", err)
	}

	sclient := s3.NewFromConfig(cfg)

	bucketName := "YOUR_R2_BUCKET_NAME"
	objectKey := "your-object-key"

	input := &s3.GetObjectInput{
		Bucket: aws.String(bucketName),
		Key:    aws.String(objectKey),
	}

	result, err := sclient.GetObject(context.TODO(), input)
	if err != nil {
		log.Fatalf("unable to get object, %v", err)
	}

	defer result.Body.Close()
	fmt.Printf("Successfully retrieved object: %s/%s\n", bucketName, objectKey)
	// Process the object body (result.Body)
	// bytes, err := io.ReadAll(result.Body)
	// if err != nil {
	// 	log.Fatalf("unable to read object body, %v", err)
	// }
	// fmt.Printf("%s\n", string(bytes))
}

```

--------------------------------

### Example R2 Data Catalog API Access Policy

Source: https://developers.cloudflare.com/r2/llms-full

An example JSON Access Policy for creating an API token programmatically. This policy grants permissions for both R2 Data Catalog write operations and R2 storage bucket item write operations.

```json
[
  {
    "id": "f267e341f3dd4697bd3b9f71dd96247f",
    "effect": "allow",
    "resources": {
      "com.cloudflare.edge.r2.bucket.4793d734c0b8e484dfc37ec392b5fa8a_default_my-bucket": "*",
      "com.cloudflare.edge.r2.bucket.4793d734c0b8e484dfc37ec392b5fa8a_eu_my-eu-bucket": "*"
    },
    "permission_groups": [
      {
        "id": "d229766a2f7f4d299f20eaa8c9b1fde9",
        "name": "Workers R2 Data Catalog Write"
      },
      {
        "id": "2efd5506f9c8494dacb1fa10a3e7d5b6",
        "name": "Workers R2 Storage Bucket Item Write"
      }
    ]
  }
]
```

--------------------------------

### Setup AWS SDK for .NET Client for R2

Source: https://developers.cloudflare.com/r2/llms-full

Provides C# code to configure and initialize the AWS SDK for .NET client for Cloudflare R2. It demonstrates how to explicitly pass access key ID and secret access key, and set the `ServiceURL` for R2.

```csharp
private static IAmazonS3 s3Client;


public static void Main(string[] args)
{
  var accessKey = "<ACCESS_KEY>";
  var secretKey = "<SECRET_KEY>";
  var credentials = new BasicAWSCredentials(accessKey, secretKey);
  s3Client = new AmazonS3Client(credentials, new AmazonS3Config
    {
      ServiceURL = "https://<ACCOUNT_ID>.r2.cloudflarestorage.com",
    });
}
```

--------------------------------

### Install TypeScript Type Definitions

Source: https://developers.cloudflare.com/r2/llms-full

This command installs the necessary type definitions for Cloudflare Workers, which helps in avoiding errors when working with TypeScript, particularly when accessing request properties.

```sh
npm run cf-typegen
```

--------------------------------

### Manage Cloudflare R2 Buckets with Terraform

Source: https://developers.cloudflare.com/r2/llms-full

This example shows how to configure Cloudflare R2 buckets using the Cloudflare Terraform provider. It requires Terraform to be installed and an API token to be provided. The configuration defines a new R2 bucket with a specified name and location.

```hcl
terraform {
  required_providers {
    cloudflare = {
      source = "cloudflare/cloudflare"
      version = "~> 4"
    }
  }
}


provider "cloudflare" {
  api_token = "<YOUR_API_TOKEN>"
}


resource "cloudflare_r2_bucket" "cloudflare-bucket" {
  account_id = "<YOUR_ACCOUNT_ID>"
  name       = "my-tf-test-bucket"
  location   = "WEUR"
}

```

--------------------------------

### Upload and Retrieve Objects with Rclone Copy

Source: https://developers.cloudflare.com/r2/llms-full

Provides examples of using the `rclone copy` command to upload a file to an R2 bucket and download a file from an R2 bucket. It also shows how to verify the upload using `rclone tree`.

```sh
# Upload dog.txt to the user-uploads bucket
rclone copy dog.txt r2:user-uploads/
rclone tree r2:user-uploads
# / 
# ├── foobar.png
# └── dog.txt


# Download dog.txt from the user-uploads bucket
rclone copy r2:user-uploads/dog.txt .
```

--------------------------------

### Calculate Multipart Upload ETag Example

Source: https://developers.cloudflare.com/r2/objects/multipart-objects

This example demonstrates how to calculate the ETag for a multipart upload in Cloudflare R2, mimicking S3 behavior. It involves concatenating the binary MD5 sums of individual parts and then hashing them, followed by the part count.

```bash
echo -n -e '\xbe\xce\xbf\x66\xae\xb7\x6c\x70\x40\x46\xdd\x5f\x4e\xcc\xb7\x86' | md5sum
echo -n -e '\x81\x65\x44\x9f\xc1\x5b\xbf\x43\xd3\xb6\x74\x59\x5c\xbc\xc4\x06' | md5sum
# Concatenate binary MD5 sums and hash them, then append part count.
```

--------------------------------

### Initialize uv Project and Add Dependencies

Source: https://developers.cloudflare.com/r2/llms-full

Initializes a new project using uv and adds marimo and other required Python packages.

```plaintext
uv init
```

```python
uv add marimo pyiceberg pyarrow pandas
```

--------------------------------

### Submit Spark Application (Bash)

Source: https://developers.cloudflare.com/r2/llms-full

This bash script provides an example of how to submit the built Spark application using spark-submit. It includes necessary Java compatibility flags for running on Java 17.

```bash
# We need to set these "--add-opens" so that Spark can run on Java 17 (it needs access to
```

--------------------------------

### List Objects and Buckets with aws-sdk-go v2

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to initialize the aws-sdk-go v2 client for Cloudflare R2, list objects within a specified bucket, and list all available buckets. It requires setting up static credentials and the R2 base endpoint.

```go
package main\n\n\nimport (\n  "context"\n  "encoding/json"\n  "fmt"\n  "github.com/aws/aws-sdk-go-v2/aws"\n  "github.com/aws/aws-sdk-go-v2/config"\n  "github.com/aws/aws-sdk-go-v2/credentials"\n  "github.com/aws/aws-sdk-go-v2/service/s3"\n  "log"\n)\n\n\nfunc main() {\n  var bucketName = "sdk-example"\n  var accountId = "<accountid>"\n  var accessKeyId = "<access_key_id>"\n  var accessKeySecret = "<access_key_secret>"\n\n\n  cfg, err := config.LoadDefaultConfig(context.TODO(),\n    config.WithCredentialsProvider(credentials.NewStaticCredentialsProvider(accessKeyId, accessKeySecret, "")),\n    config.WithRegion("auto"),\n  )\n  if err != nil {\n    log.Fatal(err)\n  }\n\n\n  client := s3.NewFromConfig(cfg, func(o *s3.Options) {\n      o.BaseEndpoint = aws.String(fmt.Sprintf("https://%s.r2.cloudflarestorage.com", accountId))\n  })\n\n\n  listObjectsOutput, err := client.ListObjectsV2(context.TODO(), &s3.ListObjectsV2Input{\n    Bucket: &bucketName,\n  })\n  if err != nil {\n    log.Fatal(err)\n  }\n\n\n  for _, object := range listObjectsOutput.Contents {\n    obj, _ := json.MarshalIndent(object, "", "\	")\n    fmt.Println(string(obj))\n  }\n\n\n  //  {\n  //    "ChecksumAlgorithm": null,\n  //    "ETag": \"eb2b891dc67b81755d2b726d9110af16\",\n  //    "Key": "ferriswasm.png",\n  //    "LastModified": "2022-05-18T17:20:21.67Z",\n  //    "Owner": null,\n  //    "Size": 87671,\n  //    "StorageClass": "STANDARD"\n  //  }\n\n\n  listBucketsOutput, err := client.ListBuckets(context.TODO(), &s3.ListBucketsInput{})\n  if err != nil {\n    log.Fatal(err)\n  }\n\n\n  for _, object := range listBucketsOutput.Buckets {\n    obj, _ := json.MarshalIndent(object, "", "\	")\n    fmt.Println(string(obj))\n  }\n\n\n  // {\n  //     "CreationDate": "2022-05-18T17:19:59.645Z",\n  //     "Name": "sdk-example"\n  // }\n}\n
```

--------------------------------

### Install unpdf Library (npm, yarn, pnpm)

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Instructions for installing the 'unpdf' library, which is essential for extracting text content from PDF files within a Cloudflare Worker.

```bash
npm install unpdf
```

```bash
yarn add unpdf
```

```bash
pnpm add unpdf
```

--------------------------------

### List Buckets and Objects with Rclone Tree

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to use the `rclone tree` command to list the contents of a Cloudflare R2 remote. Examples show listing the root of the remote and a specific bucket.

```sh
rclone tree r2:
# / 
# ├── user-uploads
# │   └── foobar.png
# └── my-bucket-name
#     ├── cat.png
#     └── todos.txt

rclone tree r2:my-bucket-name
# /
# ├── cat.png
# └── todos.txt
```

--------------------------------

### Create Directory and Navigate

Source: https://developers.cloudflare.com/r2/llms-full

Commands to create a new directory for the project and change into it.

```plaintext
mkdir r2-data-catalog-notebook
```

```plaintext
cd r2-data-catalog-notebook
```

--------------------------------

### S3 Bucket Listing Dummy Implementations

Source: https://developers.cloudflare.com/r2/llms-full

Added dummy implementations for `GetBucketVersioning`, `GetBucketLifecycleConfiguration`, `GetBucketReplication`, `GetBucketTagging`, and `GetObjectLockConfiguration` operations, mimicking responses from a newly created S3 bucket.

```xml
<!-- Example for GetBucketVersioning -->
<VersioningConfiguration>
  <Status>Disabled</Status>
</VersioningConfiguration>

<!-- Example for GetBucketTagging -->
<Tagging>
  <TagSet/>
</Tagging>
```

--------------------------------

### Create Consumer Worker with pnpm

Source: https://developers.cloudflare.com/r2/llms-full

This command uses pnpm to create a new Cloudflare Worker project named 'consumer-worker'. It sets up a basic 'Hello World' example using TypeScript and configures Git for version control.

```typescript
pnpm create cloudflare@latest consumer-worker
```

--------------------------------

### Sippy Configuration Validation API

Source: https://developers.cloudflare.com/r2/llms-full

Validate the configuration of Sippy, a data migration tool for R2, through a dedicated API endpoint. This helps ensure correct setup before migration.

```bash
curl https://api.cloudflare.com/client/v4/accounts/:account_id/r2/buckets/:bucket_name/sippy/validate
```

--------------------------------

### Create New Worker Project (npm)

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Commands to create a new Cloudflare Worker project using npm. The project is set up with a 'Hello World example' template, 'Worker only' configuration, TypeScript language, and Git version control.

```bash
npm create cloudflare@latest

```

--------------------------------

### Generate Presigned URLs with AWS SDK for Go

Source: https://developers.cloudflare.com/r2/llms-full

This snippet demonstrates how to generate presigned URLs for Cloudflare R2 using the AWS SDK for Go. It covers operations like GET, HEAD, PUT, and DELETE.

```Go
package main

import (
	"context"
	"fmt"
	"log"

	"github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/service/s3"
	"github.com/aws/aws-sdk-go-v2/service/s3/presignedesign"
)

func main() {
	// Load the AWS configuration, specifying the R2 endpoint
	cfg, err := config.LoadDefaultConfig(context.TODO(),
		config.WithRegion("auto"), // Region is often ignored for R2, but required by SDK
		config.WithEndpointResolverV2(presignedesign.EndpointResolver{}),
	)
	if err != nil {
		log.Fatalf("unable to load SDK config, %v", err)
	}

	// Create an S3 client
	client := s3.NewFromConfig(cfg)

	// Define parameters for the presigned URL
	bucketName := "your-bucket-name"
	objectKey := "your-object-key"

	// Create a presigned GET request
	presignedClient := presignedesign.NewPresignClient(client)
	getReq, err := http.NewRequestWithContext(context.TODO(), "GET", "", nil) // Method and URL are placeholders
	if err != nil {
		log.Fatalf("failed to create presigned request, %v", err)
	}
	getReq.URL.Path = "/" + bucketName + "/" + objectKey

	presignedURL, err := presignedClient.PresignGetBucketObject(context.TODO(), &s3.GetObjectInput{
		Bucket: aws.String(bucketName),
		Key:    aws.String(objectKey),
	}, presignedesign.WithPresignRequest(getReq))
	if err != nil {
		log.Fatalf("unable to presign request, %v", err)
	}

	fmt.Printf("Presigned URL for GET: %s\n", presignedURL)

	// Example for PUT (upload)
	putReq, err := http.NewRequestWithContext(context.TODO(), "PUT", "", nil)
	if err != nil {
		log.Fatalf("failed to create presigned request, %v", err)
	}
	putReq.URL.Path = "/" + bucketName + "/" + objectKey

	presignedURLPut, err := presignedClient.PresignPutObject(context.TODO(), &s3.PutObjectInput{
		Bucket: aws.String(bucketName),
		Key:    aws.String(objectKey),
	}, presignedesign.WithPresignRequest(putReq))
	if err != nil {
		log.Fatalf("unable to presign request, %v", err)
	}

	fmt.Printf("Presigned URL for PUT: %s\n", presignedURLPut)
}

```

--------------------------------

### Create Consumer Worker with npm

Source: https://developers.cloudflare.com/r2/llms-full

This command uses npm to create a new Cloudflare Worker project named 'consumer-worker'. It sets up a basic 'Hello World' example using TypeScript and configures Git for version control.

```typescript
npm create cloudflare@latest -- consumer-worker
```

--------------------------------

### R2 Multipart Upload Resumption Example

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Demonstrates how to resume an ongoing multipart upload by providing the key and upload ID to the `resumeMultipartUpload` method.

```javascript
/**
 * @param {string} key
 * @param {string} uploadId
 * @returns {R2MultipartUpload}
 */
function resumeUpload(key, uploadId) {
  // Assuming MY_BUCKET is bound in wrangler.toml
  const multipartUpload = MY_BUCKET.resumeMultipartUpload(key, uploadId);
  return multipartUpload;
}
```

--------------------------------

### Create New Worker Project (pnpm)

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Commands to create a new Cloudflare Worker project using pnpm. The project is set up with a 'Hello World example' template, 'Worker only' configuration, TypeScript language, and Git version control.

```bash
pnpm create cloudflare

```

--------------------------------

### Test Presigned URL with cURL

Source: https://developers.cloudflare.com/r2/llms-full

Tests a presigned URL by uploading an object to R2 using cURL. This example demonstrates uploading text data with a specified Content-Type.

```sh
curl --request PUT <URL> --header "Content-Type: text/plain" --data "123"
```

--------------------------------

### Create Consumer Worker with yarn

Source: https://developers.cloudflare.com/r2/llms-full

This command uses yarn to create a new Cloudflare Worker project named 'consumer-worker'. It sets up a basic 'Hello World' example using TypeScript and configures Git for version control.

```typescript
yarn create cloudflare consumer-worker
```

--------------------------------

### Create New Worker Project (yarn)

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Commands to create a new Cloudflare Worker project using yarn. The project is set up with a 'Hello World example' template, 'Worker only' configuration, TypeScript language, and Git version control.

```bash
yarn create cloudflare

```

--------------------------------

### Build Project with sbt (Bash)

Source: https://developers.cloudflare.com/r2/llms-full

This bash command builds the project using sbt, compiling the code and creating a fat JAR file that includes all necessary dependencies.

```bash
sbt clean assembly
```

--------------------------------

### R2 Bucket LIST Operation Example

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Demonstrates listing objects within an R2 bucket using the `list` method. Allows specifying options like `limit` and returns objects lexicographically.

```javascript
/**
 * @param {R2ListOptions} [options]
 * @returns {Promise<R2Objects>}
 */
async function listObjects(options) {
  // Assuming MY_BUCKET is bound in wrangler.toml
  const objects = await MY_BUCKET.list(options);
  return objects;
}
```

--------------------------------

### Interact with R2 using aws4fetch (JavaScript/TypeScript)

Source: https://developers.cloudflare.com/r2/llms-full

This example demonstrates how to use the aws4fetch npm package in JavaScript or TypeScript to interact with Cloudflare R2. It shows how to instantiate the AwsClient with R2 credentials and make requests to list buckets and objects. The package utilizes the fetch and SubtleCrypto APIs.

```typescript
import { AwsClient } from "aws4fetch";


const R2_URL = `https://${ACCOUNT_ID}.r2.cloudflarestorage.com`;


const client = new AwsClient({
  accessKeyId: ACCESS_KEY_ID,
  secretAccessKey: SECRET_ACCESS_KEY,
});


const ListBucketsResult = await client.fetch(R2_URL);
console.log(await ListBucketsResult.text());
// <ListAllMyBucketsResult>
//     <Buckets>
//         <Bucket>
//             <CreationDate>2022-04-13T21:23:47.102Z</CreationDate>
//             <Name>user-uploads</Name>
//         </Bucket>
//         <Bucket>
//             <CreationDate>2022-05-07T02:46:49.218Z</CreationDate>
//             <Name>my-bucket-name</Name>
//         </Bucket>
//     </Buckets>
//     <Owner>
//         <DisplayName>...</DisplayName>
//         <ID>...</ID>
//     </Owner>
// </ListAllMyBucketsResult>


const ListObjectsV2Result = await client.fetch(
  `${R2_URL}/my-bucket-name?list-type=2`,
);
console.log(await ListObjectsV2Result.text());
// <ListBucketResult>
//   <Name>my-bucket-name</Name>
//   <Contents>
//     <Key>cat.png</Key>
//     <Size>751832</Size>
//     <LastModified>2022-05-07T02:50:45.616Z</LastModified>
//     <ETag>"c4da329b38467509049e615c11b0c48a"</ETag>
//     <StorageClass>STANDARD</StorageClass>
//   </Contents>
//   <Contents>
//     <Key>todos.txt</Key>
//     <Size>278</Size>
//     <LastModified> 2022-05-07T21:37:17.150Z</LastModified>
//     <ETag>"29d911f495d1ba7cb3a4d7d15e63236a"</ETag>
//     <StorageClass>STANDARD</StorageClass>
//   </Contents>
//   <IsTruncated>false</IsTruncated>
//   <MaxKeys>1000</MaxKeys>
//   <KeyCount>2</KeyCount>
// </ListBucketResult>
```

--------------------------------

### Python: Connect to R2 Data Catalog and Create Table

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to connect to R2 Data Catalog using Python, create a default namespace, and create an Iceberg table with PyArrow.

```Python
from pyiceberg.catalog import RestCatalog
import pyarrow as pa

# Replace with your actual values
WAREHOUSE = "<WAREHOUSE>"
TOKEN = "<TOKEN>"
CATALOG_URI = "<CATALOG_URI>"

catalog = RestCatalog(
    name="my_catalog",
    warehouse=WAREHOUSE,
    uri=CATALOG_URI,
    token=TOKEN,
)

# Create default namespace
catalog.create_namespace("default")

# Create simple PyArrow table
df = pa.table({
    "id": [1, 2, 3],
    "name": ["Alice", "Bob", "Charlie"],
})

# Create an Iceberg table
test_table = ("default", "my_table")
table = catalog.create_table(
    test_table,
    schema=df.schema,
)
```

--------------------------------

### Configure R2 Bucket Lifecycle Rules with S3 API

Source: https://developers.cloudflare.com/r2/llms-full

Configures lifecycle rules for an R2 bucket using the S3 API. This example demonstrates setting rules for deleting old documents, transitioning objects, and aborting incomplete multipart uploads.

```javascript
await client
  .putBucketLifecycleConfiguration({
    Bucket: "testBucket",
    LifecycleConfiguration: {
      Rules: [
        // Example: deleting objects on a specific date
        // Delete 2019 documents in 2024
        {
          ID: "Delete 2019 Documents",
          Status: "Enabled",
          Filter: {
            Prefix: "2019/",
          },
          Expiration: {
            Date: new Date("2024-01-01"),
          },
        },
        // Example: transitioning objects to Infrequent Access storage by age
        // Transition objects older than 30 days to Infrequent Access storage
        {
          ID: "Transition Objects To Infrequent Access",
          Status: "Enabled",
          Transitions: [
            {
              Days: 30,
              StorageClass: "STANDARD_IA",
            },
          ],
        },
        // Example: deleting objects by age
        // Delete logs older than 90 days
        {
          ID: "Delete Old Logs",
          Status: "Enabled",
          Filter: {
            Prefix: "logs/",
          },
          Expiration: {
            Days: 90,
          },
        },
        // Example: abort all incomplete multipart uploads after a week
        {
          ID: "Abort Incomplete Multipart Uploads",
          Status: "Enabled",
          AbortIncompleteMultipartUpload: {
            DaysAfterInitiation: 7,
          },
        },
        // Example: abort user multipart uploads after a day
        {
          ID: "Abort User Incomplete Multipart Uploads",
          Status: "Enabled",
          Filter: {
            Prefix: "useruploads/",
          },
          AbortIncompleteMultipartUpload: {
            // For uploads matching the prefix, this rule will take precedence
            // over the one above due to its earlier expiration.
            DaysAfterInitiation: 1,
          },
        },
      ],
    },
  })
  .promise();
```

--------------------------------

### Cloudflare R2 Workers: GET, POST, PUT Operations with SSE-C

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to perform GET, POST (including multipart uploads), and PUT operations on Cloudflare R2 buckets using Workers. It shows how to handle requests, interact with the R2 API, and manage Server-Side Encryption with Customer-Provided Keys (SSE-C). The code is provided in both TypeScript and JavaScript.

```TypeScript
interface Environment {
    R2: R2Bucket
    /**
     * In this example, your SSE-C is stored as a hexadecimal string (preferably a secret).
     * The R2 API also supports providing an ArrayBuffer directly, if you want to generate/
     * store your keys dynamically.
    */
    SSEC_KEY: string
}
export default {
    async fetch(req: Request, env: Env) {
      const { SSEC_KEY, R2 } = env;
      const { pathname: filename } = new URL(req.url);
      switch(req.method) {
        case "GET": {
          const maybeObj = await env.BUCKET.get(filename, {
            onlyIf: req.headers,
            ssecKey: SSEC_KEY,
          });
          if(!maybeObj) {
            return new Response("Not Found", {
              status: 404
            });
          }
          const headers = new Headers();
          maybeObj.writeHttpMetadata(headers);
          return new Response(body, {
            headers
          });
        }
        case 'POST': {
          const multipartUpload = await env.BUCKET.createMultipartUpload(filename, {
            httpMetadata: req.headers,
            ssecKey: SSEC_KEY,
          });
          /**
           * This example only provides a single-part "multipart" upload.
           * For multiple parts, the process is the same(the key must be provided)
           * for every part.
          */
          const partOne = await multipartUpload.uploadPart(1, req.body, ssecKey);
          const obj = await multipartUpload.complete([partOne]);
          const headers = new Headers();
          obj.writeHttpMetadata(headers);
          return new Response(null, {
            headers,
            status: 201
          });
        }
        case 'PUT': {
          const obj = await env.BUCKET.put(filename, req.body, {
            httpMetadata: req.headers,
            ssecKey: SSEC_KEY,
          });
          const headers = new Headers();
          maybeObj.writeHttpMetadata(headers);
          return new Response(null, {
            headers,
            status: 201
          });
        }
        default: {
          return new Response("Method not allowed", {
            status: 405
          });
        }
      }
    }
  }
```

```JavaScript
/**
   * In this example, your SSE-C is stored as a hexadecimal string(preferably a secret).
   * The R2 API also supports providing an ArrayBuffer directly, if you want to generate/
   * store your keys dynamically.
*/
export default {
    async fetch(req, env) {
      const { SSEC_KEY, R2 } = env;
      const { pathname: filename } = new URL(req.url);
      switch(req.method) {
        case "GET": {
          const maybeObj = await env.BUCKET.get(filename, {
            onlyIf: req.headers,
            ssecKey: SSEC_KEY,
          });
          if(!maybeObj) {
            return new Response("Not Found", {
              status: 404
            });
          }
          const headers = new Headers();
          maybeObj.writeHttpMetadata(headers);
          return new Response(body, {
            headers
          });
        }
        case 'POST': {
          const multipartUpload = await env.BUCKET.createMultipartUpload(filename, {
            httpMetadata: req.headers,
            ssecKey: SSEC_KEY,
          });
          /**
           * This example only provides a single-part "multipart" upload.
           * For multiple parts, the process is the same(the key must be provided)
           * for every part.
          */
          const partOne = await multipartUpload.uploadPart(1, req.body, ssecKey);
          const obj = await multipartUpload.complete([partOne]);
          const headers = new Headers();
          obj.writeHttpMetadata(headers);
          return new Response(null, {
            headers,
            status: 201
          });
        }
        case 'PUT': {
          const obj = await env.BUCKET.put(filename, req.body, {
            httpMetadata: req.headers,
            ssecKey: SSEC_KEY,
          });
          const headers = new Headers();
          maybeObj.writeHttpMetadata(headers);
          return new Response(null, {
            headers,
            status: 201
          });
        }
        default: {
          return new Response("Method not allowed", {
            status: 405
          });
        }
      }
    }
  }
```

--------------------------------

### Add Necessary Type Definitions

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Command to install necessary type definitions for Cloudflare Workers and Hono, which helps in avoiding TypeScript errors during development.

```bash
npm install --save-dev @cloudflare/workers-types hono

```

--------------------------------

### Test R2 Bucket Access Control with Curl (Bash)

Source: https://developers.cloudflare.com/r2/llms-full

This section provides several `curl` command examples to test the implemented authorization logic for your R2 bucket. It covers scenarios like attempting PUT requests without the correct `X-Custom-Auth-Key` header, using an incorrect key, using the correct key, attempting to read a disallowed file, and successfully reading an allowed file. These tests verify that the Worker's security measures are functioning as expected.

```bash
# Attempt to write an object without providing the "X-Custom-Auth-Key" header
curl https://your-worker.dev/cat-pic.jpg -X PUT --data-binary 'test'
#=> Forbidden
# Expected because header was missing


# Attempt to write an object with the wrong "X-Custom-Auth-Key" header value
curl https://your-worker.dev/cat-pic.jpg -X PUT --header "X-Custom-Auth-Key: hotdog" --data-binary 'test'
#=> Forbidden
# Expected because header value did not match the AUTH_KEY_SECRET value


# Attempt to write an object with the correct "X-Custom-Auth-Key" header value
# Note: Assume that "*********" is the value of your AUTH_KEY_SECRET Wrangler secret
curl https://your-worker.dev/cat-pic.jpg -X PUT --header "X-Custom-Auth-Key: *********" --data-binary 'test'
#=> Put cat-pic.jpg successfully!


# Attempt to read object called "foo"
curl https://your-worker.dev/foo
#=> Forbidden
# Expected because "foo" is not in the ALLOW_LIST


# Attempt to read an object called "cat-pic.jpg"
curl https://your-worker.dev/cat-pic.jpg
#=> test
# Note: This is the value that was successfully PUT above
```

--------------------------------

### Example R2 Event Notification Message Format

Source: https://developers.cloudflare.com/r2/buckets/event-notifications

This is an example of the message body that a consumer Worker will receive when an event notification is triggered in an R2 bucket. It includes details about the account, action, bucket, object, and event time.

```javascript
{
  "account": "<account-id>",
  "action": "PutObject",
  "bucket": "<bucket-name>",
  "object": {
    "key": "images/cat.jpg",
    "size": 1024,
    "eTag": "<etag>"
  },
  "eventTime": "2023-01-01T12:00:00Z"
}
```

--------------------------------

### Generate Presigned URL for R2 Upload with Rust SDK

Source: https://developers.cloudflare.com/r2/examples/aws/aws-sdk-rust

Provides an example of generating a presigned PUT URL for temporary upload access to a Cloudflare R2 bucket using the AWS SDK for Rust.

```rust
use aws_sdk_s3::Client;

// Assume access_key_id and access_key_secret are set as environment variables or provided.
// let config = aws_config::load_from_env().await;
// let s3_client = Client::new(&config);

// Placeholder for actual client instantiation with R2 configuration
// let s3_client = Client::with_config(config, aws_sdk_s3::config::Region::new("auto"));

// async fn generate_presigned_put_url(s3_client: &Client, bucket_name: &str, object_key: &str) -> String {
//     let presigned_request = s3_client
//         .put_object()
//         .bucket(bucket_name)
//         .key(object_key)
//         .send()
//         .await;
//
//     // The presigned_request itself is not directly a URL string, 
//     // you would typically use a presigning utility or a method that returns the URL.
//     // This is a conceptual representation.
//     "<presigned_put_url>".to_string()
// }

```

--------------------------------

### Example lifecycle configuration JSON

Source: https://developers.cloudflare.com/r2/buckets/object-lifecycles

This JSON structure represents a lifecycle configuration for an R2 bucket, including rules for expiration and storage class transition. It demonstrates how to define rules for different prefixes and actions.

```json
{
  "Rules": [
    {
      "ID": "Rule1",
      "Prefix": "logs/",
      "Status": "Enabled",
      "Expiration": {
        "Days": 30
      }
    },
    {
      "ID": "Rule2",
      "Prefix": "temp/",
      "Status": "Enabled",
      "Transition": {
        "Days": 7,
        "StorageClass": "INFREQUENT_ACCESS"
      }
    }
  ]
}
```

--------------------------------

### ListBuckets with Search Parameters

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to list buckets in Cloudflare R2 using various search parameters like prefix, start-after, continuation-token, and max-keys. These parameters can be passed directly or via HTTP headers.

```Cloudflare CLI
wrangler r2 ls --prefix my-prefix --start-after bucket-name --continuation-token token123 --max-keys 500
```

--------------------------------

### R2 Worker Bindings: Ranged GET Requests

Source: https://developers.cloudflare.com/r2/llms-full

Ensures that ranged GET requests to R2 via worker bindings function as expected, allowing clients to retrieve specific byte ranges of an object.

```JavaScript
export default {
	async fetch(request, env) {
		const url = new URL(request.url);
		if (url.pathname === "/get-ranged") {
			const rangeHeader = request.headers.get("range");
			if (rangeHeader) {
				// Example: Retrieving a specific range of an object
				// const object = await env.MY_BUCKET.get("my-object", {
				// 	range: rangeHeader
				// });
				// return new Response(object.body, {
				// 	headers: {
				// 		"content-range": `bytes 0-99/1000` // Example content-range
				// 	}
				// });
				return new Response("Ranged GET request processed.", { status: 206 });
			}
		}
		return new Response("Hello World");
	}
}
```

--------------------------------

### Generate Presigned URL for R2 Download with Rust SDK

Source: https://developers.cloudflare.com/r2/examples/aws/aws-sdk-rust

Demonstrates generating a presigned GET URL for temporary read access to an object in a Cloudflare R2 bucket using the AWS SDK for Rust.

```rust
use aws_sdk_s3::Client;

// Assume access_key_id and access_key_secret are set as environment variables or provided.
// let config = aws_config::load_from_env().await;
// let s3_client = Client::new(&config);

// Placeholder for actual client instantiation with R2 configuration
// let s3_client = Client::with_config(config, aws_sdk_s3::config::Region::new("auto"));

// async fn generate_presigned_get_url(s3_client: &Client, bucket_name: &str, object_key: &str) -> String {
//     let presigned_request = s3_client
//         .get_object()
//         .bucket(bucket_name)
//         .key(object_key)
//         .send()
//         .await;
//
//     // The presigned_request itself is not directly a URL string, 
//     // you would typically use a presigning utility or a method that returns the URL.
//     // This is a conceptual representation.
//     "<presigned_get_url>".to_string()
// }

```

--------------------------------

### Upload/Download using Presigned URLs with cURL

Source: https://developers.cloudflare.com/r2/llms-full

These bash commands demonstrate how to use presigned URLs generated by Cloudflare R2 for uploading and downloading objects. The PUT command uploads a local file to a presigned upload URL, while the GET command downloads an object from a presigned download URL.

```bash
curl -X PUT "https://<your-presigned-put-url>" -H "Content-Type: application/octet-stream" --data-binary "@local-file.txt"
```

```bash
curl -X GET "https://<your-presigned-get-url>" -o downloaded-file.txt
```

--------------------------------

### Get Bucket Lifecycle Configuration with S3 API

Source: https://developers.cloudflare.com/r2/llms-full

Fetches the lifecycle configuration for a specified R2 bucket using the S3 API. This involves calling the `getBucketLifecycleConfiguration` method on the initialized S3 client.

```javascript
import S3 from "aws-sdk/clients/s3.js";


// Configure the S3 client to talk to R2.
const client = new S3({
  endpoint: "https://<account_id>.r2.cloudflarestorage.com",
  credentials: {
    accessKeyId: "<access_key_id>",
    secretAccessKey: "<access_key_secret>",
  },
  region: "auto",
});


// Get lifecycle configuration for bucket
console.log(
  await client
    .getBucketLifecycleConfiguration({
      Bucket: "bucketName",
    })
    .promise(),
);
```

--------------------------------

### API Example: Set Bucket Lock Configuration

Source: https://developers.cloudflare.com/r2/buckets/bucket-locks

An example of setting a bucket lock configuration for an R2 bucket via the Cloudflare API. This configuration includes two rules: one for retaining logs for 7 days and another for indefinitely retaining images.

```json
{
  "rules": [
    {
      "name": "lock-logs-7d",
      "prefix": "logs/",
      "retention": {
        "days": 7
      }
    },
    {
      "name": "lock-images-indefinite",
      "prefix": "images/",
      "retention": {
        "indefinite": true
      }
    }
  ]
}
```

--------------------------------

### R2 Object Checksum Availability in get() and head()

Source: https://developers.cloudflare.com/r2/llms-full

User-specified object checksums, including MD5 by default for non-multipart uploads, are now available in the response of R2 `get()` and `head()` bindings. This allows for verification of data integrity after retrieval.

```javascript
// Example of accessing checksum in response (conceptual)
const { body, checksum, etag } = await env.BUCKET.get(key);
console.log(checksum);
```

--------------------------------

### Generate Presigned URLs with AWS SDK for PHP

Source: https://developers.cloudflare.com/r2/llms-full

This snippet demonstrates how to generate presigned URLs for Cloudflare R2 using the AWS SDK for PHP. It covers GET and PUT operations.

```PHP
<?php
require 'vendor/autoload.php';

use Aws\S3\S3Client;
use Aws\Exception\AwsException;

$r2Endpoint = 'https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com';
$accessKeyId = 'YOUR_ACCESS_KEY_ID';
$secretAccessKey = 'YOUR_SECRET_ACCESS_KEY';
$bucketName = 'your-bucket-name';
$objectKey = 'your-object-key';

$s3Client = new S3Client([
    'version' => 'latest',
    'region' => 'auto', // Region is often ignored for R2, but required by SDK
    'endpoint' => $r2Endpoint,
    'use_path_style_endpoint' => true, // Required for R2
    'credentials' => [
        'key'    => $accessKeyId,
        'secret' => $secretAccessKey,
    ]
]);

try {
    // Generate presigned URL for GET (download)
    $cmdGet = $s3Client->getCommand('GetObject', [
        'Bucket' => $bucketName,
        'Key'    => $objectKey
    ]);

    $requestGet = $s3Client->createPresignedRequest($cmdGet, '+1 hour');
    $presignedUrlGet = (string) $requestGet->getPresignedUrl();
    echo "Presigned URL for GET: " . $presignedUrlGet . "\n";

    // Generate presigned URL for PUT (upload)
    $cmdPut = $s3Client->getCommand('PutObject', [
        'Bucket' => $bucketName,
        'Key'    => $objectKey
    ]);

    $requestPut = $s3Client->createPresignedRequest($cmdPut, '+1 hour');
    $presignedUrlPut = (string) $requestPut->getPresignedUrl();
    echo "Presigned URL for PUT: " . $presignedUrlPut . "\n";

} catch (AwsException $e) {
    // Display error message
    echo $e->getMessage() . "\n";
}
?>

```

--------------------------------

### Upload and Retrieve Objects with AWS SDK for .NET

Source: https://developers.cloudflare.com/r2/examples/aws/aws-sdk-net

Provides examples for uploading a file to and downloading a file from a Cloudflare R2 bucket using the `PutObjectAsync` and `GetObjectAsync` methods of the AWS SDK for .NET.

```csharp
using Amazon.S3.Model;
using System.IO;

// Upload an object
var putRequest = new PutObjectRequest
{
    BucketName = "your-bucket-name",
    Key = "sdk-example/file.txt",
    InputStream = new MemoryStream(System.Text.Encoding.UTF8.GetBytes("Hello R2!"))
};
await s3Client.PutObjectAsync(putRequest);

// Retrieve an object
var getRequest = new GetObjectRequest
{
    BucketName = "your-bucket-name",
    Key = "sdk-example/file.txt"
};
using (var getResponse = await s3Client.GetObjectAsync(getRequest))
using (var reader = new StreamReader(getResponse.ResponseStream))
{
    var content = await reader.ReadToEndAsync();
    Console.WriteLine($"Object content: {content}");
}
```

--------------------------------

### Create R2 Bucket in Specific Jurisdiction using AWS SDK (JavaScript)

Source: https://developers.cloudflare.com/r2/llms-full

This JavaScript code example uses the `@aws-sdk/client-s3` package to create an R2 bucket in the 'eu' jurisdiction. It requires specifying the account ID, jurisdiction, and access credentials.

```javascript
import { S3Client, CreateBucketCommand } from "@aws-sdk/client-s3";
const S3 = new S3Client({
  endpoint: "https://<account_id>.eu.r2.cloudflarestorage.com",
  credentials: {
    accessKeyId: "<access_key_id",
    secretAccessKey: "<access_key_secret>",
  },
  region: "auto",
});
await S3.send(
  new CreateBucketCommand({
    Bucket: "YOUR_BUCKET_NAME",
  }),
);
```

--------------------------------

### Cloudflare R2 Worker Bindings: Ranged GET Requests

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixes an issue with the R2 bindings API where passing a `range` header to the `get` operation was not working as expected. This ensures correct handling of partial content retrieval.

```javascript
const response = await env.MY_BUCKET.get('my-object', {
  range: {
    offset: 0,
    length: 1024
  }
});

```

--------------------------------

### List Objects in R2 Bucket with AWS SDK for .NET

Source: https://developers.cloudflare.com/r2/llms-full

Shows how to list objects within a specified bucket in Cloudflare R2 using the `ListObjectsV2Async` method of the AWS SDK for .NET. The example specifies the bucket name and iterates through the objects, printing their keys.

```csharp
static async Task ListObjectsV2()
{
  var request = new ListObjectsV2Request
  {
    BucketName = "sdk-example"
  };


  var response = await s3Client.ListObjectsV2Async(request);


  foreach (var s3Object in response.S3Objects)
  {
    Console.WriteLine("{0}", s3Object.Key);
  }
}
// dog.png
// cat.png
```

--------------------------------

### Generate Presigned URLs with AWS CLI

Source: https://developers.cloudflare.com/r2/llms-full

This snippet shows how to generate presigned URLs for Cloudflare R2 using the AWS Command Line Interface (CLI). It covers GET and PUT operations.

```Bash
# Configure AWS CLI for R2
# Ensure you have AWS CLI installed and configured with R2 credentials
# aws configure --profile r2
# AWS Access Key ID [YOUR_ACCESS_KEY_ID]: YOUR_ACCESS_KEY_ID
# AWS Secret Access Key [YOUR_SECRET_ACCESS_KEY]: YOUR_SECRET_ACCESS_KEY
# Default region name [None]: auto
# Default output format [None]: json

# Set R2 endpoint URL
export AWS_ENDPOINT_URL="https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com"

# Define bucket and object key
BUCKET_NAME="your-bucket-name"
OBJECT_KEY="your-object-key"

# Generate presigned URL for GET (download)
s3api get-object --bucket $BUCKET_NAME --key $OBJECT_KEY --expires-in 3600 --profile r2

# Generate presigned URL for PUT (upload)
s3api put-object --bucket $BUCKET_NAME --key $OBJECT_KEY --expires-in 3600 --profile r2

# Note: The output will be JSON containing the presigned URL.
# For example, for get-object, it might look like:
# {
#     "PresignedUrl": "https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com/your-bucket-name/your-object-key?AWSAccessKeyId=YOUR_ACCESS_KEY_ID&Expires=1678886400&Signature=..."
# }

```

--------------------------------

### Generate Presigned URLs with AWS SDK for JS

Source: https://developers.cloudflare.com/r2/llms-full

This snippet illustrates generating presigned URLs for Cloudflare R2 using the older AWS SDK for JavaScript (v2). It covers GET and PUT operations.

```JavaScript
var AWS = require('aws-sdk');

const r2Endpoint = "https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com";
const accessKeyId = "YOUR_ACCESS_KEY_ID";
const secretAccessKey = "YOUR_SECRET_ACCESS_KEY";
const bucketName = "your-bucket-name";
const objectKey = "your-object-key";

AWS.config.update({
    region: "auto", // Region is often ignored for R2, but required by SDK
    endpoint: r2Endpoint,
    credentials: {
        accessKeyId: accessKeyId,
        secretAccessKey: secretAccessKey
    }
});

var s3 = new AWS.S3();

// Generate presigned URL for GET (download)
var paramsGet = {
    Bucket: bucketName,
    Key: objectKey
};

s3.getSignedUrl('getObject', paramsGet, function(err, url) {
    if (err) {
        console.error("Error generating presigned URL for GET:", err);
    } else {
        console.log("Presigned URL for GET:", url);
    }
});

// Generate presigned URL for PUT (upload)
var paramsPut = {
    Bucket: bucketName,
    Key: objectKey
};

s3.getSignedUrl('putObject', paramsPut, function(err, url) {
    if (err) {
        console.error("Error generating presigned URL for PUT:", err);
    } else {
        console.log("Presigned URL for PUT:", url);
    }
});

```

--------------------------------

### R2 Bucket HEAD Operation Example

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Shows how to retrieve only the metadata for an object using the `head` method on an R2 bucket binding. Returns null if the key does not exist.

```javascript
/**
 * @param {string} key
 * @returns {Promise<R2Object | null>}
 */
async function headObject(key) {
  // Assuming MY_BUCKET is bound in wrangler.toml
  const object = await MY_BUCKET.head(key);
  return object;
}
```

--------------------------------

### Deploy Cloudflare Worker

Source: https://developers.cloudflare.com/r2/llms-full

This command deploys your configured Cloudflare Worker using the Wrangler CLI. Ensure you have Wrangler installed and configured for your project.

```sh
npx wrangler deploy
```

--------------------------------

### S3 GetBucketAcl Dummy Implementation

Source: https://developers.cloudflare.com/r2/llms-full

Added a dummy implementation for the `GetBucketAcl` operation that mimics the response of a basic AWS S3 bucket upon creation.

```xml
<AccessControlPolicy>
  <Owner>
    <ID>example-owner-id</ID>
    <DisplayName>Example Owner</DisplayName>
  </Owner>
  <AccessGrants>
    <Grant>
      <Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="CanonicalUser">
        <ID>example-owner-id</ID>
        <DisplayName>Example Owner</DisplayName>
      </Grantee>
      <Permission>FULL_CONTROL</Permission>
    </Grant>
  </AccessGrants>
</AccessControlPolicy>
```

--------------------------------

### R2 Bucket PUT Operation Example

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Demonstrates how to use the `put` method on an R2 bucket binding to store a value with a given key. The operation is strongly consistent.

```javascript
/**
 * @param {string} key
 * @param {ReadableStream | ArrayBuffer | ArrayBufferView | string | null | Blob} value
 * @param {R2PutOptions} [options]
 * @returns {Promise<R2Object | null>}
 */
async function putObject(key, value, options) {
  // Assuming MY_BUCKET is bound in wrangler.toml
  const object = await MY_BUCKET.put(key, value, options);
  return object;
}
```

--------------------------------

### Scala Spark Application for R2 Data Catalog

Source: https://developers.cloudflare.com/r2/data-catalog/config-examples/spark-scala

This Scala code serves as the main entry point for a Spark application designed to connect to the R2 Data Catalog. It demonstrates how to initialize a Spark session and potentially interact with R2 data.

```Scala
package com.example

import org.apache.spark.sql.SparkSession

object R2DataCatalogDemo {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("R2DataCatalogDemo")
      .getOrCreate()

    // Add your R2 Data Catalog interaction logic here
    // For example, reading data from R2
    // val df = spark.read.format("cloudfiler2").load("your-bucket-name/your-data-path")
    // df.show()

    spark.stop()
  }
}
```

--------------------------------

### Basic R2 Operations with AWS SDK for Rust

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates basic operations with Cloudflare R2 using the AWS SDK for Rust. This includes configuring the client with R2-specific credentials and endpoint, listing buckets, and listing objects within a bucket.

```rust
use aws_sdk_s3 as s3;
use aws_smithy_types::date_time::Format::DateTime;


#[tokio::main]
async fn main() -> Result<(), s3::Error> {
    let bucket_name = "sdk-example";
    let account_id = "<accountid>";
    let access_key_id = "<access_key_id>";
    let access_key_secret = "<access_key_secret>";


    // Configure the client
    let config = aws_config::from_env()
        .endpoint_url(format!("https://{{}}.r2.cloudflarestorage.com", account_id))
        .credentials_provider(aws_sdk_s3::config::Credentials::new(
            access_key_id,
            access_key_secret,
            None, // session token is not used with R2
            None,
            "R2",
        ))
        .region("auto")
        .load()
        .await;


    let client = s3::Client::new(&config);


    // List buckets
    let list_buckets_output = client.list_buckets().send().await?;


    println!("Buckets:");
    for bucket in list_buckets_output.buckets() {
        println!("  - {}: {}",
            bucket.name().unwrap_or_default(),
            bucket.creation_date().map_or_else(
                || "Unknown creation date".to_string(),
                |date| date.fmt(DateTime).unwrap()
            )
        );
    }


    // List objects in a specific bucket
    let list_objects_output = client
        .list_objects_v2()
        .bucket(bucket_name)
        .send()
        .await?;


    println!("\nObjects in {}:", bucket_name);
    for object in list_objects_output.contents() {
        println!("  - {}: {} bytes, last modified: {}",
            object.key().unwrap_or_default(),
            object.size().unwrap_or_default(),
            object.last_modified().map_or_else(
                || "Unknown".to_string(),
                |date| date.fmt(DateTime).unwrap()
            )
        );
    }


    Ok(())
}

```

--------------------------------

### Configure and Use Boto3 for Cloudflare R2 (Python)

Source: https://developers.cloudflare.com/r2/llms-full

Shows how to configure the boto3 library in Python to interact with Cloudflare R2. It covers setting the endpoint URL, access keys (either directly or via environment variables), and demonstrates basic operations like getting object information and uploading files.

```Python
import boto3
import io


s3 = boto3.resource('s3',
  endpoint_url = 'https://<accountid>.r2.cloudflarestorage.com',
  aws_access_key_id = '<access_key_id>',
  aws_secret_access_key = '<access_key_secret>'
)

```

```Python
import boto3
import io


s3 = boto3.client(
    service_name ="s3",
    endpoint_url = 'https://<accountid>.r2.cloudflarestorage.com',
    aws_access_key_id = '<access_key_id>',
    aws_secret_access_key = '<access_key_secret>',
    region_name="<location>", # Must be one of: wnam, enam, weur, eeur, apac, auto
)


# Get object information
object_information = s3.head_object(Bucket=<R2_BUCKET_NAME>, Key=<FILE_KEY_NAME>)


# Upload/Update single file
s3.upload_fileobj(io.BytesIO(file_content), <R2_BUCKET_NAME>, <FILE_KEY_NAME>)

```

--------------------------------

### Enable Sippy with Wrangler CLI

Source: https://developers.cloudflare.com/r2/data-migration/sippy

Example of enabling Sippy for an R2 bucket using the Wrangler CLI. This command requires your AWS Access Key ID, Secret Access Key, and the name of your R2 bucket.

```bash
wrangler r2 sippy enable <BUCKET_NAME> --aws-access-key-id <YOUR_ACCESS_KEY_ID> --aws-secret-access-key <YOUR_SECRET_ACCESS_KEY>
```

--------------------------------

### Enable Sippy via Cloudflare API

Source: https://developers.cloudflare.com/r2/data-migration/sippy

Illustrative example of enabling Sippy for an R2 bucket using a hypothetical Cloudflare API call. This would typically involve sending a POST request with authentication and bucket details.

```javascript
async function enableSippy(bucketName, accessKeyId, secretAccessKey) {
  const response = await fetch('https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/r2/buckets/{BUCKET_NAME}/sippy', {
    method: 'POST',
    headers: {
      'Authorization': 'Bearer {API_TOKEN}',
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      "aws_access_key_id": accessKeyId,
      "aws_secret_access_key": secretAccessKey
    })
  });
  return await response.json();
}
```

--------------------------------

### Get R2 Bucket API

Source: https://developers.cloudflare.com/r2/llms-full

Retrieve information about an R2 bucket using the Cloudflare API. The endpoint now uses `bucket_name` for identification instead of `bucket_id`.

```bash
curl https://api.cloudflare.com/client/v4/accounts/:account_id/r2/buckets/:bucket_name
```

--------------------------------

### Launch Marimo Notebook Editor

Source: https://developers.cloudflare.com/r2/llms-full

Command to launch the marimo editor in the browser, opening the specified Python notebook file.

```plaintext
uv run marimo edit r2-data-catalog-tutorial.py
```

--------------------------------

### Configure Access Application (Self-Hosted)

Source: https://developers.cloudflare.com/r2/tutorials/cloudflare-access

This example outlines the steps to configure a self-hosted Access application in Cloudflare. It involves setting an application name, a public hostname, and defining Access policies for user authentication.

```javascript
// Example configuration for a self-hosted Access application
// This is a conceptual representation, actual implementation uses Cloudflare Dashboard or API.
const accessAppConfig = {
  appName: "R2-Bucket-Access",
  type: "self_hosted",
  domain: "your-cloudflare-domain.com",
  subdomain: "r2-access.your-cloudflare-domain.com",
  policies: [
    {
      name: "Allow internal users",
      action: "allow",
      conditions: [
        {
          type: "email",
          operator: "is",
          value: "user@example.com"
        }
      ]
    }
  ]
};
```

--------------------------------

### Add lifecycle rule using Wrangler

Source: https://developers.cloudflare.com/r2/buckets/object-lifecycles

This command adds a lifecycle rule to your R2 bucket using the Wrangler CLI. Ensure you have Wrangler installed and are logged in.

```bash
wrangler r2 bucket lifecycle add <bucket-name> --rule='{"Expiration": {"Days": 90}}'
```

--------------------------------

### Get R2 Bucket Custom Domain API

Source: https://developers.cloudflare.com/r2/llms-full

Fetch details for a specific custom domain associated with an R2 bucket using the Cloudflare API.

```bash
curl https://api.cloudflare.com/client/v4/accounts/:account_id/r2/buckets/:bucket_name/domains/custom/:domain_name
```

--------------------------------

### Create Cloudflare R2 Bucket with Wrangler

Source: https://developers.cloudflare.com/r2/buckets/create-buckets

Creates a new bucket in Cloudflare R2 using the `r2 bucket create` command. Ensure you have Wrangler installed and configured.

```bash
r2 bucket create <bucket-name>
```

--------------------------------

### List R2 Objects with Options

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to list objects in a Cloudflare R2 bucket using various options such as limit and include. It also shows how to handle pagination using the truncated flag and cursor to retrieve all objects.

```javascript
const options = {
  limit: 500,
  include: ["customMetadata"],
};


const listed = await env.MY_BUCKET.list(options);


let truncated = listed.truncated;
let cursor = truncated ? listed.cursor : undefined;


// ❌ - if your limit can't fit into a single response or your
// bucket has less objects than the limit, it will get stuck here.
while (listed.objects.length < options.limit) {
  // ...
}


// ✅ - use the truncated property to check if there are more
// objects to be returned
while (truncated) {
  const next = await env.MY_BUCKET.list({
    ...options,
    cursor: cursor,
  });
  listed.objects.push(...next.objects);

  truncated = next.truncated;
  cursor = next.cursor;
}
```

--------------------------------

### Configure Worker for Queue Consumer and R2 Binding

Source: https://developers.cloudflare.com/r2/tutorials/upload-logs-event-notifications

Example configuration for a Wrangler project (`wrangler.toml` or `wrangler.jsonc`) to set up a queue consumer for event notifications and bind an R2 bucket for data access.

```toml
[queues]
producers = [
  { queue = "your-queue-name", topic = "your-topic-name" }
]

[[r2_buckets]]
binding = "YOUR_BUCKET_BINDING"
tag = "example-upload-bucket"
```

--------------------------------

### Wrangler: Get R2 Bucket Information

Source: https://developers.cloudflare.com/r2/platform/release-notes

Details the `bucket info` command in Wrangler, which allows users to retrieve information about their R2 buckets. This includes the bucket's location and common performance metrics.

```bash
wrangler r2 bucket info <bucket-name>
```

--------------------------------

### Get rclone Configuration File Location

Source: https://developers.cloudflare.com/r2/llms-full

This shell command displays the location of the rclone configuration file. This is useful for editing existing rclone configurations, particularly when setting up access to Cloudflare R2.

```sh
rclone config file
```

--------------------------------

### Create R2 Buckets with Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

These commands use the Wrangler CLI to create two R2 buckets. The first, 'example-upload-bucket', will receive new objects, and the second, 'example-log-sink-bucket', will store the upload logs.

```sh
npx wrangler r2 bucket create example-upload-bucket
npx wrangler r2 bucket create example-log-sink-bucket
```

--------------------------------

### Retrieve an Object from R2 using Postman

Source: https://developers.cloudflare.com/r2/tutorials/postman

Retrieve an object from your R2 bucket using the `GET` GetObject request in Postman. This allows you to download the object that was previously uploaded.

```Postman
Navigate to **Cloudflare R2** > **Objects** > **`GET`GetObject** and select **Send**.
```

--------------------------------

### R2ObjectBody Get Object Value

Source: https://developers.cloudflare.com/r2/llms-full

The `R2ObjectBody` provides methods to access the object's content in various formats. You can retrieve the object's value as an ArrayBuffer, text, JSON, or Blob.

```javascript
async function getObjectValue(r2ObjectBody) {
  const arrayBuffer = await r2ObjectBody.arrayBuffer();
  const text = await r2ObjectBody.text();
  const json = await r2ObjectBody.json();
  const blob = await r2ObjectBody.blob();

  return { arrayBuffer, text, json, blob };
}
```

--------------------------------

### Interact with Cloudflare R2 using Postman

Source: https://developers.cloudflare.com/r2/llms-full

This guide explains how to use Postman, an API platform, to interact with Cloudflare R2. It covers making authenticated requests to create buckets, upload, and retrieve objects, referencing a Postman collection with a comprehensive list of supported operations.

```json
{
  "variables": {
    "account_id": "YOUR_ACCOUNT_ID",
    "access_key_id": "YOUR_ACCESS_KEY_ID",
    "secret_access_key": "YOUR_SECRET_ACCESS_KEY",
    "bucket_name": "YOUR_BUCKET_NAME"
  },
  "collections": [
    {
      "folder": "Buckets",
      "operations": [
        "Create Bucket",
        "List Buckets"
      ]
    },
    {
      "folder": "Objects",
      "operations": [
        "Upload Object",
        "Get Object",
        "Delete Object"
      ]
    }
  ]
}
```

--------------------------------

### R2GetOptions: Specify retrieval conditions and ranges

Source: https://developers.cloudflare.com/r2/llms-full

Configure R2 GET requests with conditional operations or byte range specifications. Supports conditional headers, byte offsets, lengths, and suffixes for partial object retrieval.

```javascript
const options = {
  onlyIf: {
    "If-Match": "some-etag"
  },
  range: {
    offset: 100,
    length: 50
  }
};
```

```javascript
const options = {
  range: {
    suffix: 1024
  }
};
```

--------------------------------

### Create R2 Queue with Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

This command utilizes the Wrangler CLI to create a new queue named 'example-event-notification-queue'. This queue will be used to receive event notifications from an R2 bucket.

```sh
npx wrangler queues create example-event-notification-queue
```

--------------------------------

### Generate Presigned URLs with AWS SDK for JS v3

Source: https://developers.cloudflare.com/r2/llms-full

This snippet shows how to generate presigned URLs for Cloudflare R2 using the AWS SDK for JavaScript v3. It covers common HTTP methods like GET and PUT.

```JavaScript
import { S3Client, GetObjectCommand, PutObjectCommand } from "@aws-sdk/client-s3";
import { getSignedUrl } from "@aws-sdk/s3-request-presigner";

const r2Endpoint = "https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com";
const accessKeyId = "YOUR_ACCESS_KEY_ID";
const secretAccessKey = "YOUR_SECRET_ACCESS_KEY";
const bucketName = "your-bucket-name";
const objectKey = "your-object-key";

const s3Client = new S3Client({
  region: "auto", // Region is often ignored for R2, but required by SDK
  endpoint: r2Endpoint,
  credentials: {
    accessKeyId: accessKeyId,
    secretAccessKey: secretAccessKey,
  },
});

async function generatePresignedUrls() {
  // Generate presigned URL for GET (download)
  const getCommand = new GetObjectCommand({
    Bucket: bucketName,
    Key: objectKey,
  });
  const signedUrlGet = await getSignedUrl(s3Client, getCommand, {
    expiresIn: 60 * 60, // URL expires in 1 hour
  });
  console.log("Presigned URL for GET:", signedUrlGet);

  // Generate presigned URL for PUT (upload)
  const putCommand = new PutObjectCommand({
    Bucket: bucketName,
    Key: objectKey,
  });
  const signedUrlPut = await getSignedUrl(s3Client, putCommand, {
    expiresIn: 60 * 60, // URL expires in 1 hour
  });
  console.log("Presigned URL for PUT:", signedUrlPut);
}

generatePresignedUrls().catch(console.error);

```

--------------------------------

### SBT Build Configuration for Spark Application

Source: https://developers.cloudflare.com/r2/data-catalog/config-examples/spark-scala

This build.sbt file configures an SBT project to build a fat JAR for the Spark application. It specifies the Scala version, Spark dependencies, and the sbt-assembly plugin for packaging.

```SBT
name := "R2DataCatalogDemo"

version := "1.0"

scalaVersion := "2.12.15"

libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "3.5.3",
  "org.apache.spark" %% "spark-sql" % "3.5.3"
)

assembly / assemblyMergeStrategy := { 
  case PathList("META-INF", xs @ _*)
    => MergeStrategy.discard 
  case x => MergeStrategy.first 
}
```

--------------------------------

### Generate Presigned URLs for Read Access (PHP)

Source: https://developers.cloudflare.com/r2/llms-full

Generates a presigned URL for temporary read access to an object in a Cloudflare R2 bucket. The URL's validity period can be specified. This example uses the AWS SDK for PHP.

```php
$cmd = $s3_client->getCommand('GetObject', [
    'Bucket' => $bucket_name,
    'Key' => 'ferriswasm.png'
]);


// The second parameter allows you to determine how long the presigned link is valid.
$request = $s3_client->createPresignedRequest($cmd, '+1 hour');


print_r((string)$request->getUri())
```

--------------------------------

### R2 Bucket DELETE Operation Example

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Illustrates how to delete one or more objects from an R2 bucket using the `delete` method. Supports deleting up to 1000 keys per call.

```javascript
/**
 * @param {string | string[]} keys
 * @returns {Promise<void>}
 */
async function deleteObjects(keys) {
  // Assuming MY_BUCKET is bound in wrangler.toml
  await MY_BUCKET.delete(keys);
}
```

--------------------------------

### API: Get Custom Domain Endpoint

Source: https://developers.cloudflare.com/r2/platform/release-notes

Introduces a new API endpoint for retrieving details about custom domains configured for R2 buckets. This allows programmatic access to custom domain information.

```bash
GET /buckets/:bucket/domains/:domain
```

--------------------------------

### List Buckets and Objects with AWS SDK for JavaScript

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to list buckets and objects in Cloudflare R2 using the AWS SDK for JavaScript (v2). Requires R2 configuration with access keys and endpoint.

```javascript
import S3 from "aws-sdk/clients/s3.js";

const s3 = new S3({
  endpoint: `https://${accountid}.r2.cloudflarestorage.com`,
  accessKeyId: `${access_key_id}`,
  secretAccessKey: `${access_key_secret}`,
  signatureVersion: "v4",
});

console.log(await s3.listBuckets().promise());
//=> {
//=>   Buckets: [
//=>     { Name: 'user-uploads', CreationDate: 2022-04-13T21:23:47.102Z },
//=>     { Name: 'my-bucket-name', CreationDate: 2022-05-07T02:46:49.218Z }
//=>   ],
//=>   Owner: {
//=>     DisplayName: 'னமாக',
//=>     ID: '...' 
//=>   }
//=> }

console.log(await s3.listObjects({ Bucket: "my-bucket-name" }).promise());
//=> {
//=>   IsTruncated: false,
//=>   Name: 'my-bucket-name',
//=>   CommonPrefixes: [],
//=>   MaxKeys: 1000,
//=>   Contents: [
//=>     {
//=>       Key: 'cat.png',
//=>       LastModified: 2022-05-07T02:50:45.616Z,
//=>       ETag: '"c4da329b38467509049e615c11b0c48a"',
//=>       ChecksumAlgorithm: [],
//=>       Size: 751832,
//=>       Owner: [Object]
//=>     },
//=>     {
//=>       Key: 'todos.txt',
//=>       LastModified: 2022-05-07T21:37:17.150Z,
//=>       ETag: '"29d911f495d1ba7cb3a4d7d15e63236a"',
//=>       ChecksumAlgorithm: [],
//=>       Size: 279,
//=>       Owner: [Object]
//=>     }
//=>   ]
//=> }
```

--------------------------------

### API: Sippy Endpoint Changes

Source: https://developers.cloudflare.com/r2/platform/release-notes

Highlights changes to the Sippy API, including a modified response shape for `GET /buckets/:bucket/sippy`, the exposure of the `/buckets/:bucket/sippy/validate` endpoint over APIGW, and a changed configuration object shape.

```json
{
  "responseShape": "updated"
}
```

```json
{
  "configuration": "changed"
}
```

--------------------------------

### Connect Custom Domain to R2 Bucket

Source: https://developers.cloudflare.com/r2/tutorials/cloudflare-access

This example shows how to connect a custom domain to an R2 bucket using the Cloudflare dashboard. It involves navigating to R2 settings, adding a custom domain, and confirming the DNS record.

```json
// Conceptual JSON representation of a custom domain connection request
// Actual interaction is via Cloudflare Dashboard UI or API.
{
  "bucketName": "my-r2-bucket",
  "customDomain": "r2-access.your-cloudflare-domain.com",
  "dnsRecord": {
    "type": "CNAME",
    "name": "r2-access.your-cloudflare-domain.com",
    "content": "your-bucket-id.r2.cloudflarestorage.com"
  }
}
```

--------------------------------

### Generate Presigned URLs for Write Access (PHP)

Source: https://developers.cloudflare.com/r2/llms-full

Generates a presigned URL for temporary write access to a specific object in a Cloudflare R2 bucket. The URL's validity period can be specified. This example uses the AWS SDK for PHP.

```php
$cmd = $s3_client->getCommand('PutObject', [
    'Bucket' => $bucket_name,
    'Key' => 'ferriswasm.png'
]);


$request = $s3_client->createPresignedRequest($cmd, '+1 hour');


print_r((string)$request->getUri())
```

--------------------------------

### HTTP Metadata for R2 Objects

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

HTTP metadata fields for R2 objects, such as contentType, contentLanguage, contentDisposition, contentEncoding, cacheControl, and cacheExpiry. These can be overridden during GET requests.

```javascript
{
  contentType?: string,
  contentLanguage?: string,
  contentDisposition?: string,
  contentEncoding?: string,
  cacheControl?: string,
  cacheExpiry?: Date
}
```

--------------------------------

### PySpark: Connect to R2 Data Catalog

Source: https://developers.cloudflare.com/r2/llms-full

Shows how to initialize a SparkSession and set up connection details for R2 Data Catalog in PySpark.

```Python
from pyspark.sql import SparkSession


# Define catalog connection details (replace variables)
WAREHOUSE = "<WAREHOUSE>"
TOKEN = "<TOKEN>"
CATALOG_URI = "<CATALOG_URI>"

# Initialize SparkSession (example, may need further configuration for R2)
spark = SparkSession.builder \
    .appName("R2DataCatalogSpark") \
    .getOrCreate()

# Further configuration to connect to R2 Data Catalog would go here,
# potentially involving setting Spark configurations for Iceberg and REST catalog.
```

--------------------------------

### Access R2 with Cloudflare Workers

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to interact with Cloudflare R2 from within a Cloudflare Worker using the Workers API. This involves configuring a binding to your R2 bucket and performing operations like putting and getting objects.

```javascript
export default {
	async fetch(request, env, ctx) {
		// Assuming you have a binding named MY_BUCKET to your R2 bucket
		const url = new URL(request.url);
		const key = url.pathname.substring(1);

		// Put an object
		await env.MY_BUCKET.put(key, "Hello World!");

		// Get an object
		const object = await env.MY_BUCKET.get(key);

		// Return the object value
		return new Response(object.body);
	},
}
```

--------------------------------

### Upload Object using cURL with Presigned URL

Source: https://developers.cloudflare.com/r2/buckets/cors

This example shows how to test a presigned URL by uploading an object to Cloudflare R2 using `curl`. It demonstrates uploading plain text content with a specified `Content-Type`.

```bash
curl -X PUT "YOUR_PRESIGNED_URL" \
     --header "Content-Type: text/plain" \
     --upload-file <(echo "This is the content of the file.")
```

--------------------------------

### List Cloudflare R2 Buckets using Postman

Source: https://developers.cloudflare.com/r2/llms-full

Execute the 'GET ListBuckets' request within the Postman collection to retrieve a list of existing R2 buckets. This operation uses AWS SigV4 authentication for the handshake.

```Postman
Navigate to Cloudflare R2 folder > Buckets folder > GET ListBuckets.
Select Send.
Expected response: 200 OK with a list of buckets.
```

--------------------------------

### R2 ListMultipartUploads Beta Support

Source: https://developers.cloudflare.com/r2/llms-full

Beta support for the `ListMultipartUploads` operation has been added, allowing users to manage multipart uploads in R2.

```javascript
// No direct code example, this is a feature announcement.
```

--------------------------------

### SBT Assembly Plugin Configuration

Source: https://developers.cloudflare.com/r2/data-catalog/config-examples/spark-scala

This assembly.sbt file enables the sbt-assembly plugin, which is crucial for creating a fat JAR that bundles all project dependencies. This is necessary for deploying the Spark application.

```SBT
addSbtPlugin("com.github.sbt" % "sbt-assembly" % "2.0.0")
```

--------------------------------

### AWS CLI: Transition Object to Infrequent Access

Source: https://developers.cloudflare.com/r2/buckets/storage-classes

This example demonstrates how to use the AWS Command Line Interface (CLI) to change the storage class of an object from 'STANDARD' to 'STANDARD_IA' (Infrequent Access). This operation is useful for optimizing storage costs for less frequently accessed data.

```bash
aws s3 cp s3://your-bucket/your-object s3://your-bucket/your-object --storage-class STANDARD_IA
```

--------------------------------

### Initialize AWS SDK v3 Client for Cloudflare R2

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to instantiate the S3Client from the AWS SDK for JavaScript v3, configuring it to connect to Cloudflare R2 with provided credentials and endpoint.

```typescript
import {
  S3Client,
  ListBucketsCommand,
  ListObjectsV2Command,
  GetObjectCommand,
  PutObjectCommand,
} from "@aws-sdk/client-s3";


const S3 = new S3Client({
  region: "auto",
  endpoint: `https://${ACCOUNT_ID}.r2.cloudflarestorage.com`,
  credentials: {
    accessKeyId: ACCESS_KEY_ID,
    secretAccessKey: SECRET_ACCESS_KEY,
  },
});
```

--------------------------------

### Upload Objects via Rclone

Source: https://developers.cloudflare.com/r2/objects/upload-objects

Upload multiple objects concurrently to Cloudflare R2 using the rclone command-line tool. Ensure rclone is installed and configured for R2. Use `rclone copy` to upload files and `rclone ls` to verify.

```bash
rclone copy /path/to/local/files your-r2-bucket:your-bucket-name
rclone ls your-r2-bucket:your-bucket-name
```

--------------------------------

### Upload an Object to R2 using Postman

Source: https://developers.cloudflare.com/r2/tutorials/postman

Upload an object to your R2 bucket using the `PUT` PutObject request in Postman. This example uses multipart upload and requires setting the `r2-object` variable and attaching the binary file.

```Postman
Set the `r2-object` variable to the desired object name (e.g., `cat-pic.jpg`).
Navigate to **Cloudflare R2** > **Objects** > **Multipart** > **`PUT`PutObject**.
In the **Body** tab, choose **binary** and attach your file.
Select **Send**.
```

--------------------------------

### Add AWS SDK for Go v2

Source: https://developers.cloudflare.com/r2/llms-full

Provides the command to add the necessary AWS SDK for Go v2 packages to a Go project, enabling interaction with services like Cloudflare R2.

```sh
go get github.com/aws/aws-sdk-go-v2
```

--------------------------------

### Generate Presigned URLs for R2 using Rust

Source: https://developers.cloudflare.com/r2/llms-full

This Rust code shows how to generate presigned URLs for temporary read (GET) or write (PUT) access to objects in an R2 bucket. It utilizes the aws_sdk_s3 crate and requires an S3 client, bucket name, object key, and a duration for the URL's validity. The generated URLs can be used with any HTTP client.

```rust
use aws_sdk_s3::presigning::PresigningConfig;
use std::time::Duration;


async fn generate_get_presigned_url(
    client: &s3::Client,
    bucket: &str,
    key: &str,
    expires_in: Duration,
) -> Result<String, s3::Error> {
    let presigning_config = PresigningConfig::expires_in(expires_in)?;


    // Generate a presigned URL for GET (download)
    let presigned_get_request = client
        .get_object()
        .bucket(bucket)
        .key(key)
        .presigned(presigning_config)
        .await?;


    Ok(presigned_get_request.uri().to_string())
}


async fn generate_upload_presigned_url(
    client: &s3::Client,
    bucket: &str,
    key: &str,
    expires_in: Duration,
) -> Result<String, s3::Error> {
    let presigning_config = PresigningConfig::expires_in(expires_in)?;


    // Generate a presigned URL for PUT (upload)
    let presigned_put_request = client
        .put_object()
        .bucket(bucket)
        .key(key)
        .presigned(presigning_config)
        .await?;


    Ok(presigned_put_request.uri().to_string())
}
```

--------------------------------

### Get Bucket Encryption - S3 API Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

Describes the 'GetBucketEncryption' operation for the S3 API in Cloudflare R2. Features related to 'Bucket Owner' and 'x-amz-expected-bucket-owner' are not implemented.

```S3 API
GetBucketEncryption
```

--------------------------------

### Get Bucket Location - S3 API Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

Outlines the 'GetBucketLocation' operation for the S3 API in Cloudflare R2. Features related to 'Bucket Owner' and 'x-amz-expected-bucket-owner' are not implemented.

```S3 API
GetBucketLocation
```

--------------------------------

### Get Bucket Lifecycle Configuration - S3 API Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

Details the 'GetBucketLifecycleConfiguration' operation for the S3 API in Cloudflare R2. Features related to 'Bucket Owner' and 'x-amz-expected-bucket-owner' are not implemented.

```S3 API
GetBucketLifecycleConfiguration
```

--------------------------------

### Get Bucket Cors - S3 API Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

Describes the 'GetBucketCors' operation for the S3 API in Cloudflare R2. Features related to 'Bucket Owner' and 'x-amz-expected-bucket-owner' are not implemented.

```S3 API
GetBucketCors
```

--------------------------------

### Cloudflare R2 Worker Bindings: Conditional Uploads

Source: https://developers.cloudflare.com/r2/platform/release-notes

Demonstrates how to perform a conditional upload using the R2 worker bindings with the `onlyIf` field, similar to the `get()` functionality. This allows for atomic operations based on the state of the object.

```javascript
import { getAssetFromKV } from '@cloudflare/kv-asset-handler';

// Example of using onlyIf for conditional upload
await env.MY_BUCKET.put('my-object', data, {
  onlyIf: {
    etagDoesNotMatch: 'some-etag-value'
  }
});

```

--------------------------------

### R2 Event Notification Message Format

Source: https://developers.cloudflare.com/r2/llms-full

Example JSON structure of a message received by a consumer Worker when an R2 event notification is triggered. It includes account details, action, bucket information, object key, size, etag, event time, and copy source details if applicable.

```json
{
  "account": "3f4b7e3dcab231cbfdaa90a6a28bd548",
  "action": "CopyObject",
  "bucket": "my-bucket",
  "object": {
    "key": "my-new-object",
    "size": 65536,
    "eTag": "c846ff7a18f28c2e262116d6e8719ef0"
  },
  "eventTime": "2024-05-24T19:36:44.379Z",
  "copySource": {
    "bucket": "my-bucket",
    "object": "my-original-object"
  }
}
```

--------------------------------

### Generate Presigned URL for R2 Upload (JavaScript/TypeScript)

Source: https://developers.cloudflare.com/r2/llms-full

This snippet demonstrates how to generate a presigned URL for uploading objects to a Cloudflare R2 bucket. It uses the `aws4fetch` library to sign the request, allowing authenticated uploads to a specific path within the bucket. The example is designed to run on Cloudflare Workers and includes placeholders for access keys, bucket name, and account ID.

```typescript
import { AwsClient } from "aws4fetch";


// Create a new client
// Replace with your own access key ID and secret access key
// Make sure to store these securely and not expose them
const client = new AwsClient({
  accessKeyId: "",
  secretAccessKey: "",
});


export default {
  async fetch(req): Promise<Response> {
    // This is just an example to demonstrating using aws4fetch to generate a presigned URL.
    // This Worker should not be used as-is as it does not authenticate the request, meaning
    // that anyone can upload to your bucket.
    //
    // Consider implementing authorization, such as a preshared secret in a request header.
    const requestPath = new URL(req.url).pathname;


    // Cannot upload to the root of a bucket
    if (requestPath === "/") {
      return new Response("Missing a filepath", { status: 400 });
    }


    // Replace with your bucket name and account ID
    const bucketName = "";
    const accountId = "";


    const url = new URL(
      `https://${bucketName}.${accountId}.r2.cloudflarestorage.com`,
    );


    // preserve the original path
    url.pathname = requestPath;


    // Specify a custom expiry for the presigned URL, in seconds
    url.searchParams.set("X-Amz-Expires", "3600");


    const signed = await client.sign(
      new Request(url, {
        method: "PUT",
      }),
      {
        aws: { signQuery: true },
      },
    );


    // Caller can now use this URL to upload to that object.
    return new Response(signed.url, { status: 200 });
  },


  // ... handle other kinds of requests
} satisfies ExportedHandler;
```

--------------------------------

### Authenticate R2 API with S3 API using JavaScript

Source: https://developers.cloudflare.com/r2/examples/authenticate-r2-auth-tokens

This snippet shows how to authenticate against Cloudflare R2 using the S3 API and an API token in JavaScript. Ensure the AWS SDK is installed and environmental variables are set.

```javascript
const { S3Client, GetObjectCommand } = require("@aws-sdk/client-s3");

// Ensure AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_REGION are set as environment variables
// For R2, the region should be the name of your R2 bucket.
const s3Client = new S3Client({
  region: "YOUR_R2_BUCKET_NAME",
  endpoint: `https://${process.env.CLOUDFLARE_ACCOUNT_ID}.r2.cloudflarestorage.com`,
  credentials: {
    accessKeyId: process.env.ACCESS_KEY_ID,
    secretAccessKey: process.env.SECRET_ACCESS_KEY,
  },
});

async function getObject(bucketName, objectKey) {
  const command = new GetObjectCommand({
    Bucket: bucketName,
    Key: objectKey,
  });

  try {
    const response = await s3Client.send(command);
    const stream = response.Body;
    // Process the stream data
    console.log("Successfully retrieved object.");
    // Example: Convert stream to text
    // const chunks = [];
    // for await (const chunk of stream) {
    //   chunks.push(chunk);
    // }
    // const fileContent = Buffer.concat(chunks).toString('utf-8');
    // console.log(fileContent);
  } catch (error) {
    console.error("Error retrieving object:", error);
  }
}

// Example usage:
// const bucketName = "YOUR_R2_BUCKET_NAME";
// const objectKey = "your-object-key";
// getObject(bucketName, objectKey);

```

--------------------------------

### Sign Additional Headers for Presigned URL

Source: https://developers.cloudflare.com/r2/api/s3/presigned-urls

This example shows how to modify a presigned URL generation process to include and sign additional headers, such as 'If-Unmodified-Since'. The caller must provide the exact same headers when using the generated URL.

```JavaScript
import { AwsClient } from "aws4fetch";

const client = new AwsClient({
  accessKeyId: "YOUR_ACCESS_KEY_ID",
  secretAccessKey: "YOUR_SECRET_ACCESS_KEY",
  service: "s3",
  region: "us-east-1",
});

const url = await client.getSignedUrl("PUT", "https://your-r2-bucket.your-account-id.r2.cloudflarestorage.com/uploads/dog.png", {
  headers: {
    "x-amz-acl": "public-read",
    "If-Unmodified-Since": "Mon, 27 Jul 2023 10:00:00 GMT", // Example date
  },
});

console.log(url);
```

--------------------------------

### R2 Binding: Conditional Upload with `uploadedBefore`

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates uploading an object to an R2 bucket using an R2 binding with a conditional upload check. It ensures that an object is only uploaded if it was not previously uploaded before a specific date, preventing overwrites. This example uses TypeScript and assumes the existence of an `env.R2_BUCKET` binding.

```typescript
const existingObject = await env.R2_BUCKET.put(key, request.body, {
    onlyIf: {
      // No objects will have been uploaded before September 28th, 2021 which
      // is the initial R2 announcement.
      uploadedBefore: new Date(1632844800000),
    },
  });
  if (existingObject?.etag !== request.headers.get("etag")) {
    return new Response("attempt to overwrite object", { status: 400 });
  }
```

--------------------------------

### S3 Bucket Configuration Dummy Implementations

Source: https://developers.cloudflare.com/r2/platform/release-notes

Added dummy implementations for several S3 bucket configuration operations (GetBucketVersioning, GetBucketLifecycleConfiguration, GetBucketReplication, GetBucketTagging, GetObjectLockConfiguration) to mimic initial bucket responses.

```text
Added dummy implementations of the following operations that mimic the response that a basic AWS S3 bucket will return when first created:
    * `GetBucketVersioning`
    * `GetBucketLifecycleConfiguration`
    * `GetBucketReplication`
    * `GetBucketTagging`
    * `GetObjectLockConfiguration`
```

--------------------------------

### Build Spark Session with Iceberg Configurations (Python)

Source: https://developers.cloudflare.com/r2/llms-full

This code snippet demonstrates how to build a Spark session with necessary Iceberg configurations to connect to Cloudflare R2 Data Catalog. It includes setting up the application name, required packages, Spark SQL extensions, catalog details, and default catalog.

```Python
spark = SparkSession.builder \
  .appName("R2DataCatalogExample") \
  .config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1,org.apache.iceberg:iceberg-aws-bundle:1.6.1') \
  .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
  .config("spark.sql.catalog.my_catalog", "org.apache.iceberg.spark.SparkCatalog") \
  .config("spark.sql.catalog.my_catalog.type", "rest") \
  .config("spark.sql.catalog.my_catalog.uri", CATALOG_URI) \
  .config("spark.sql.catalog.my_catalog.warehouse", WAREHOUSE) \
  .config("spark.sql.catalog.my_catalog.token", TOKEN) \
  .config("spark.sql.catalog.my_catalog.header.X-Iceberg-Access-Delegation", "vended-credentials") \
  .config("spark.sql.catalog.my_catalog.s3.remote-signing-enabled", "false") \
  .config("spark.sql.defaultCatalog", "my_catalog") \
  .getOrCreate()
spark.sql("USE my_catalog")
```

--------------------------------

### Set Custom Header Per PutObject Request with aws-sdk-js-v3

Source: https://developers.cloudflare.com/r2/examples/aws/custom-header

This JavaScript example demonstrates how to apply a custom header to a specific `PutObject` request using `aws-sdk-js-v3`. It involves creating middleware that is attached directly to the command, allowing for per-request header customization.

```javascript
import { S3Client, PutObjectCommand } from "@aws-sdk/client-s3";

// Configure the S3 client for R2
const client = new S3Client({
  region: "auto",
  endpoint: "https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com",
  credentials: {
    accessKeyId: "YOUR_ACCESS_KEY_ID",
    secretAccessKey: "YOUR_SECRET_ACCESS_KEY",
  },
});

// Define the middleware to add a custom header to a specific request
const addRequestSpecificHeaderMiddleware = (headerName, headerValue) => (next) => async (args) => {
  args.request.headers[headerName] = headerValue;
  return next(args);
};

// Example of making a request with a custom header:
// const command = new PutObjectCommand({
//   Bucket: "my-bucket",
//   Key: "my-object",
//   Body: "Hello, R2!",
// });

// Add the middleware directly to the command
// command.middlewareStack.add(
//   addRequestSpecificHeaderMiddleware('x-if-match', 'some-etag-value'),
//   { step: "build" }
// );

// try {
//   const response = await client.send(command);
//   console.log("Object uploaded successfully with custom header:", response);
// } catch (error) {
//   console.error("Error uploading object:", error);
// }

```

--------------------------------

### R2 Conditional Upload with put()

Source: https://developers.cloudflare.com/r2/llms-full

The R2 `put()` binding now supports conditional uploads using the `onlyIf` field, similar to the `get()` binding. This allows for uploads to be performed only if specific conditions are met, preventing unintended overwrites or ensuring data integrity.

```javascript
await env.BUCKET.put(key, value, {
  onlyIf: {
    // Example: only upload if the object does not exist
    // exists: false
    // Example: only upload if the object's ETag matches a specific value
    // etagMatch: "some-etag-value"
  }
});
```

--------------------------------

### Cloudflare R2 Worker Bindings: Ranged GET Malformed Request

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixes an internal error returned for malformed ranged `.get` requests in the Workers bindings. This improves the robustness of handling invalid range parameters.

```javascript
const response = await env.MY_BUCKET.get('my-object', {
  range: {
    offset: 'invalid-offset',
    length: 1024
  }
});

```

--------------------------------

### Authenticate Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Logs in Wrangler to enable deployments to Cloudflare. This command will open a browser for authentication.

```txt
wrangler login
```

--------------------------------

### Create AWS IAM Policy for R2 Migration

Source: https://developers.cloudflare.com/r2/data-migration/super-slurper

This policy grants necessary permissions for Super Slurper to access a specific S3 bucket for migration. It allows listing and getting objects within the bucket.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket",
        "s3:GetObject"
      ],
      "Resource": [
        "arn:aws:s3:::<BUCKET_NAME>",
        "arn:aws:s3:::<BUCKET_NAME>/*"
      ]
    }
  ]
}
```

--------------------------------

### Generate Presigned URL for PUT Object

Source: https://developers.cloudflare.com/r2/api/s3/presigned-urls

This example shows how to create a presigned URL for uploading an object to an R2 bucket. It includes the necessary parameters like account ID, bucket name, object key, credentials, and expiration time, tailored for a PUT request.

```Python
import boto3
from botocore.exceptions import ClientError
import datetime

def generate_presigned_put_url(bucket_name, object_name, expiration=3600):
    """Generate a presigned URL for PUT requests

    :param bucket_name: bucket to put object in
    :param object_name: name of object to put
    :param expiration: Time in seconds for the presigned URL to remain valid
    :return: Presigned URL or None if error occurred
    """
    # Note: R2 does not require region_name if using a custom endpoint URL
    # For R2, you would typically use your account ID and the R2 endpoint.
    # Example: endpoint_url=f"https://{ACCOUNT_ID}.r2.cloudflarestorage.com"
    # However, boto3's S3 client is used here for compatibility with the signing process.
    # You'll need to configure your AWS credentials (e.g., via environment variables or ~/.aws/credentials)
    # and potentially set the endpoint_url if not using the default AWS S3 endpoint.

    # For Cloudflare R2, you might need to configure the client like this:
    # s3_client = boto3.client('s3',
    #                          endpoint_url=f"https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com",
    #                          aws_access_key_id='YOUR_ACCESS_KEY_ID',
    #                          aws_secret_access_key='YOUR_SECRET_ACCESS_KEY')

    s3_client = boto3.client('s3') # Assumes credentials and endpoint are configured elsewhere or via env vars

    try:
        response = s3_client.generate_presigned_url('put_object',
                                                    Params={'Bucket': bucket_name,
                                                            'Key': object_name},
                                                    ExpiresIn=expiration,
                                                    HttpMethod='PUT')
    except ClientError as e:
        print(e)
        return None
    return response

# Example Usage:
# BUCKET_NAME = 'your-r2-bucket-name'
# OBJECT_NAME = 'path/to/your/upload.txt'
#
# presigned_url = generate_presigned_put_url(BUCKET_NAME, OBJECT_NAME)
#
# if presigned_url:
#     print(f"Presigned URL for PUT: {presigned_url}")
# else:
#     print("Failed to generate presigned URL")

```

--------------------------------

### Spark Submit Script for R2 Data Catalog Demo

Source: https://developers.cloudflare.com/r2/data-catalog/config-examples/spark-scala

This shell script demonstrates how to submit the compiled Spark application JAR to a Spark cluster. It includes essential Java compatibility flags for running Spark with Java 17 and specifies the main class and JAR file.

```Shell
#!/bin/bash

export SPARK_MAJOR_VERSION=3

spark-submit \
  --class com.example.R2DataCatalogDemo \
  --master local[*] \
  --conf spark.driver.extraJavaOptions="-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.security=ALL-UNNAMED" \
  --conf spark.executor.extraJavaOptions="-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.security=ALL-UNNAMED" \
  target/scala-2.12/R2DataCatalogDemo-assembly-1.0.jar
```

--------------------------------

### S3 GetBucketAcl Dummy Implementation

Source: https://developers.cloudflare.com/r2/platform/release-notes

Added a dummy implementation for the GetBucketAcl operation, mimicking the response of a basic AWS S3 bucket upon creation.

```text
Added dummy implementation of the following operation that mimics the response that a basic AWS S3 bucket will return when first created: `GetBucketAcl`.
```

--------------------------------

### Cloudflare R2 Worker Bindings: User-Specified Checksums

Source: https://developers.cloudflare.com/r2/platform/release-notes

Illustrates how to provide user-specified checksums (SHA-1, SHA-256, SHA-384, SHA-512) during an upload to R2 using worker bindings. It also mentions that these checksums are available in `get()` and `head()` responses, with MD5 included by default for non-multipart uploads.

```javascript
import crypto from 'crypto';

const data = new Uint8Array([1, 2, 3]);
const sha256Hash = crypto.createHash('sha256').update(data).digest('hex');

await env.MY_BUCKET.put('my-object', data, {
  checksums: {
    'sha-256': sha256Hash
  }
});

// Retrieving checksums
const { checksums } = await env.MY_BUCKET.head('my-object');
console.log(checksums['sha-256']);

```

--------------------------------

### Implement Bucket Authorization Logic in Worker (JavaScript)

Source: https://developers.cloudflare.com/r2/llms-full

This JavaScript code snippet defines functions to authorize requests to an R2 bucket. It uses a pre-shared secret from environment variables for PUT and DELETE operations and checks an allow list for GET requests. The `authorizeRequest` function centralizes this logic, which is then used in the Worker's `fetch` handler to return a 403 Forbidden response for unauthorized requests.

```javascript
const ALLOW_LIST = ["cat-pic.jpg"];


// Check requests for a pre-shared secret
const hasValidHeader = (request, env) => {
  return request.headers.get("X-Custom-Auth-Key") === env.AUTH_KEY_SECRET;
};


function authorizeRequest(request, env, key) {
  switch (request.method) {
    case "PUT":
    case "DELETE":
      return hasValidHeader(request, env);
    case "GET":
      return ALLOW_LIST.includes(key);
    default:
      return false;
  }
}


export default {
  async fetch(request, env, ctx) {
    const url = new URL(request.url);
    const key = url.pathname.slice(1);


    if (!authorizeRequest(request, env, key)) {
      return new Response("Forbidden", { status: 403 });
    }


    // ...
  },
};
```

--------------------------------

### List Buckets using AWS SDK for .NET

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to list all buckets associated with a Cloudflare R2 account using the `ListBucketsAsync` method of the AWS SDK for .NET. It iterates through the response and prints each bucket name.

```csharp
static async Task ListBuckets()
{
  var response = await s3Client.ListBucketsAsync();


  foreach (var s3Bucket in response.Buckets)
  {
    Console.WriteLine("{0}", s3Bucket.BucketName);
  }
}
// sdk-example
// my-bucket-name
```

--------------------------------

### Create and List R2 Buckets

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-usage

This snippet shows the commands to create a new R2 bucket and then list all existing buckets to verify the creation. These commands are executed via the Wrangler CLI.

```bash
npx wrangler r2 bucket create <YOUR_BUCKET_NAME>
npx wrangler r2 bucket list

```

--------------------------------

### Set Custom Header on All PutObject Requests with aws-sdk-js-v3

Source: https://developers.cloudflare.com/r2/examples/aws/custom-header

This example shows how to add a custom header to every `PutObject` request using the JavaScript `aws-sdk-js-v3`. It utilizes the middleware stack feature of the SDK to inject the header into the request pipeline before it's sent.

```javascript
import { S3Client, PutObjectCommand } from "@aws-sdk/client-s3";

// Configure the S3 client for R2
const client = new S3Client({
  region: "auto",
  endpoint: "https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com",
  credentials: {
    accessKeyId: "YOUR_ACCESS_KEY_ID",
    secretAccessKey: "YOUR_SECRET_ACCESS_KEY",
  },
});

// Define the middleware to add the custom header
const addCustomHeaderMiddleware = (next) => async (args) => {
  args.request.headers['x-custom-header'] = 'MyValue';
  return next(args);
};

// Add the middleware to the client's command middleware stack
client.middlewareStack.add(addCustomHeaderMiddleware, {
  step: "build", // 'build' step is typically before sending the request
});

// Now, any PutObject command executed with this client will include the custom header
// Example:
// const command = new PutObjectCommand({
//   Bucket: "my-bucket",
//   Key: "my-object",
//   Body: "Hello, R2!",
// });

// try {
//   const response = await client.send(command);
//   console.log("Object uploaded successfully:", response);
// } catch (error) {
//   console.error("Error uploading object:", error);
// }

```

--------------------------------

### List Buckets and Objects with AWS SDK for .NET

Source: https://developers.cloudflare.com/r2/examples/aws/aws-sdk-net

Shows how to list all buckets associated with your R2 account and the contents of a specific bucket using the `ListBucketsAsync` and `ListObjectsAsync` methods from the AWS SDK for .NET.

```csharp
using Amazon.S3.Model;

// List buckets
var listBucketsResponse = await s3Client.ListBucketsAsync();
foreach (var bucket in listBucketsResponse.Buckets)
{
    Console.WriteLine($"Bucket: {bucket.BucketName}");
}

// List objects in a bucket
var listObjectsRequest = new ListObjectsV2Request
{
    BucketName = "your-bucket-name"
};
var listObjectsResponse = await s3Client.ListObjectsV2Async(listObjectsRequest);
foreach (var obj in listObjectsResponse.S3Objects)
{
    Console.WriteLine($"Object: {obj.Key}");
}
```

--------------------------------

### R2 Bucket Operations in Cloudflare Worker

Source: https://developers.cloudflare.com/r2/llms-full

This JavaScript code snippet demonstrates how to perform GET, PUT, and DELETE operations on a Cloudflare R2 bucket from within a Worker. It handles request methods, object retrieval, metadata, and error responses. The code assumes the bucket is available as `env.MY_BUCKET`.

```javascript
export default {
  async fetch(request, env) {
    const url = new URL(request.url);
    const key = url.pathname.slice(1);


    switch (request.method) {
      case "PUT":
        await env.MY_BUCKET.put(key, request.body);
        return new Response(`Put ${key} successfully!`);
      case "GET":
        const object = await env.MY_BUCKET.get(key);


        if (object === null) {
          return new Response("Object Not Found", { status: 404 });
        }


        const headers = new Headers();
        object.writeHttpMetadata(headers);
        headers.set("etag", object.httpEtag);


        return new Response(object.body, {
          headers,
        });
      case "DELETE":
        await env.MY_BUCKET.delete(key);
        return new Response("Deleted!");


      default:
        return new Response("Method Not Allowed", {
          status: 405,
          headers: {
            Allow: "PUT, GET, DELETE",
          },
        });
    }
  },
};
```

--------------------------------

### List Buckets and Objects with Rclone

Source: https://developers.cloudflare.com/r2/examples/rclone

Demonstrates how to use the `rclone tree` command to list the contents of a Cloudflare R2 remote, providing visibility into stored data.

```bash
rclone tree <r2_remote_name>:
```

--------------------------------

### Cloudflare R2 S3 API: ListObjects StartAfter Bug

Source: https://developers.cloudflare.com/r2/platform/release-notes

Corrects a bug in `ListObjects` where the `startAfter` parameter would incorrectly skip objects whose keys had numbers immediately following the specified prefix. This ensures accurate pagination.

```http
GET /?startAfter=prefix123&prefix=prefix HTTP/1.1
Host: your-bucket.s3.amazonaws.com


```

--------------------------------

### Enable R2 Data Catalog using Wrangler CLI

Source: https://developers.cloudflare.com/r2/llms-full

Enables the R2 Data Catalog on a specified bucket using the Wrangler CLI. After execution, it provides the Catalog URI and Warehouse name required by Iceberg clients.

```bash
npx wrangler r2 bucket catalog enable <BUCKET_NAME>
```

--------------------------------

### Generate Presigned URL for GET Object

Source: https://developers.cloudflare.com/r2/api/s3/presigned-urls

This snippet demonstrates how to generate a presigned URL for retrieving an object from an R2 bucket. It requires your account ID, bucket name, object key, access key ID, secret access key, and the desired expiration time.

```JavaScript
import { R2 } from '@cloudflare/workers-types/r2';

async function generatePresignedGetUrl(accountId, bucketName, objectKey, accessKeyId, secretAccessKey, expiresInSeconds) {
  const url = new URL(`https://${accountId}.r2.cloudflarestorage.com/${bucketName}/${objectKey}`);
  const expires = Math.floor((Date.now() + expiresInSeconds * 1000) / 1000);

  const headers = {
    'x-amz-content-sha256': 'UNSIGNED-PAYLOAD',
    'x-amz-date': new Date().toISOString().replace(/[:-]/g, '').split('.')[0] + 'Z',
    'host': `${accountId}.r2.cloudflarestorage.com`
  };

  const canonicalRequest = `GET\n/${objectKey}\n\nhost:${headers.host}\nx-amz-content-sha256:${headers['x-amz-content-sha256']}\nx-amz-date:${headers['x-amz-date']}\n\nhost;x-amz-content-sha256;x-amz-date\n${crypto.subtle.digest('SHA-256', new TextEncoder().encode(url.pathname.substring(1)))}`

  const dateStamp = headers['x-amz-date'].substring(0, 8);
  const region = ''; // R2 does not use regions for signing
  const service = 's3';

  const stringToSign = `AWS4-HMAC-SHA256\n${headers['x-amz-date']}\n${dateStamp}/${region}/${service}/aws4_request\n${crypto.subtle.hash('SHA-256', new TextEncoder().encode(canonicalRequest))}`

  const kSecret = `AWS4${secretAccessKey}`;
  const kDate = crypto.subtle.sign('HMAC', kSecret, dateStamp);
  const kRegion = crypto.subtle.sign('HMAC', kDate, region);
  const kService = crypto.subtle.sign('HMAC', kRegion, service);
  const kSigning = crypto.subtle.sign('HMAC', kService, 'aws4_request');

  const signature = crypto.subtle.sign('HMAC', kSigning, stringToSign);

  const signedUrl = new URL(url);
  signedUrl.searchParams.append('X-Amz-Algorithm', 'AWS4-HMAC-SHA256');
  signedUrl.searchParams.append('X-Amz-Credential', `${accessKeyId}/${dateStamp}/${region}/${service}/aws4_request`);
  signedUrl.searchParams.append('X-Amz-Date', headers['x-amz-date']);
  signedUrl.searchParams.append('X-Amz-Expires', expiresInSeconds.toString());
  signedUrl.searchParams.append('X-Amz-SignedHeaders', 'host');
  signedUrl.searchParams.append('X-Amz-Signature', signature);

  return signedUrl.toString();
}

// Example Usage:
// const accountId = 'YOUR_ACCOUNT_ID';
// const bucketName = 'YOUR_BUCKET_NAME';
// const objectKey = 'path/to/your/object.txt';
// const accessKeyId = 'YOUR_ACCESS_KEY_ID';
// const secretAccessKey = 'YOUR_SECRET_ACCESS_KEY';
// const expiresInSeconds = 3600; // 1 hour

// generatePresignedGetUrl(accountId, bucketName, objectKey, accessKeyId, secretAccessKey, expiresInSeconds)
//   .then(url => console.log('Presigned URL:', url))
//   .catch(err => console.error('Error generating URL:', err));

```

--------------------------------

### Fix startAfter and continuationToken in List Operations

Source: https://developers.cloudflare.com/r2/platform/release-notes

Corrects a bug where `startAfter` and `continuationToken` parameters were not functioning correctly in S3 API list operations.

```S3 API
startAfter parameter
continuationToken parameter
```

--------------------------------

### S3 ListObjects Response Rendering

Source: https://developers.cloudflare.com/r2/platform/release-notes

Ensures the S3 API ListObjects operation correctly renders `Prefix`, `Delimiter`, `StartAfter`, and `MaxKeys` parameters in the response.

```S3 API
ListObjects response rendering (Prefix, Delimiter, StartAfter, MaxKeys)
```

--------------------------------

### Initialize S3 Client for R2

Source: https://developers.cloudflare.com/r2/llms-full

Initializes an S3 client using the AWS SDK for JavaScript to interact with Cloudflare R2. Requires account ID, access key ID, and secret access key.

```javascript
const client = new S3({
  endpoint: "https://<account_id>.r2.cloudflarestorage.com",
  credentials: {
    accessKeyId: "<access_key_id>",
    secretAccessKey: "<access_key_secret>",
  },
  region: "auto",
});
```

--------------------------------

### Snowflake: Connect to R2 Data Catalog and Query Iceberg Table

Source: https://developers.cloudflare.com/r2/llms-full

Provides SQL commands for Snowflake to create an external volume for R2, set up a catalog integration for R2 Data Catalog, create an Iceberg table, and query it.

```SQL
-- Create a database (if you don't already have one) to organize your external data
CREATE DATABASE IF NOT EXISTS r2_example_db;


-- Create an external volume pointing to your R2 bucket
CREATE OR REPLACE EXTERNAL VOLUME ext_vol_r2
    STORAGE_LOCATIONS = (
        ( 
            NAME = 'my_r2_storage_location'
            STORAGE_PROVIDER = 'S3COMPAT'
            STORAGE_BASE_URL = 's3compat://<bucket-name>'
            CREDENTIALS = (
                AWS_KEY_ID = '<access_key>'
                AWS_SECRET_KEY = '<secret_access_key>'
            )
            STORAGE_ENDPOINT = '<account_id>.r2.cloudflarestorage.com'
        )
    )
    ALLOW_WRITES = FALSE;


-- Create a catalog integration for R2 Data Catalog (read-only)
CREATE OR REPLACE CATALOG INTEGRATION r2_data_catalog
    CATALOG_SOURCE = ICEBERG_REST
    TABLE_FORMAT = ICEBERG
    CATALOG_NAMESPACE = 'default'
    REST_CONFIG = (
        CATALOG_URI = '<catalog_uri>'
        CATALOG_NAME = '<warehouse_name>'
    )
    REST_AUTHENTICATION = (
        TYPE = BEARER
        BEARER_TOKEN = '<token>'
    )
    ENABLED = TRUE;


-- Create an Apache Iceberg table in your selected Snowflake database
CREATE ICEBERG TABLE my_iceberg_table
    CATALOG = 'r2_data_catalog'
    EXTERNAL_VOLUME = 'ext_vol_r2'
    CATALOG_TABLE_NAME = 'my_table';  -- Name of existing table in your R2 data catalog


-- Query your Iceberg table
SELECT * FROM my_iceberg_table;
```

--------------------------------

### sbt-assembly Plugin Configuration (Plaintext)

Source: https://developers.cloudflare.com/r2/llms-full

This plaintext file configures the sbt-assembly plugin, which is used to build fat JARs. It specifies the plugin and its version.

```plaintext
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "1.2.0")
```

--------------------------------

### Deploy Worker to Cloudflare (Bash)

Source: https://developers.cloudflare.com/r2/llms-full

This bash command initiates the deployment of your Cloudflare Worker to the global network. After setting up your R2 bucket and Worker code, running `npx wrangler deploy` uploads your application, making it accessible via a Cloudflare endpoint. This step is essential for testing your authorization logic in a live environment.

```bash
npx wrangler deploy
```

--------------------------------

### List Objects in R2 Bucket (Ruby)

Source: https://developers.cloudflare.com/r2/llms-full

Lists the first 20 objects in a specified Cloudflare R2 bucket using the R2 client. Requires the R2 client to be initialized.

```ruby
puts @r2.list_objects(bucket:"your-bucket", max_keys:20)
```

--------------------------------

### Marimo Notebook for R2 Data Catalog Interaction

Source: https://developers.cloudflare.com/r2/llms-full

A Python script using marimo to create and interact with an Iceberg table in Cloudflare R2. It includes steps for connecting to the catalog, creating namespaces and tables, appending data, and scanning table contents. Replace placeholder variables with actual credentials.

```python
import marimo


__generated_with = "0.11.31"
app = marimo.App(width="medium")



@app.cell
def _():
    import marimo as mo
    return (mo,)



@app.cell
def _():
    import pandas
    import pyarrow as pa
    import pyarrow.compute as pc
    import pyarrow.parquet as pq


    from pyiceberg.catalog.rest import RestCatalog


    # Define catalog connection details (replace variables)
    WAREHOUSE = "<WAREHOUSE>"
    TOKEN = "<TOKEN>"
    CATALOG_URI = "<CATALOG_URI>"


    # Connect to R2 Data Catalog
    catalog = RestCatalog(
        name="my_catalog",
        warehouse=WAREHOUSE,
        uri=CATALOG_URI,
        token=TOKEN,
    )
    return (
        CATALOG_URI,
        RestCatalog,
        TOKEN,
        WAREHOUSE,
        catalog,
        pa,
        pandas,
        pc,
        pq,
    )



@app.cell
def _(catalog):
    # Create default namespace if needed
    catalog.create_namespace_if_not_exists("default")
    return



@app.cell
def _(pa):
    # Create simple PyArrow table
    df = pa.table({
        "id": [1, 2, 3],
        "name": ["Alice", "Bob", "Charlie"],
        "score": [80.0, 92.5, 88.0],
    })
    return (df,)



@app.cell
def _(catalog, df):
    # Create or load Iceberg table
    test_table = ("default", "people")
    if not catalog.table_exists(test_table):
        print(f"Creating table: {test_table}")
        table = catalog.create_table(
            test_table,
            schema=df.schema,
        )
    else:
        table = catalog.load_table(test_table)
    return table, test_table



@app.cell
def _(df, table):
    # Append data
    table.append(df)
    return



@app.cell
def _(table):
    print("Table contents:")
    scanned = table.scan().to_arrow()
    print(scanned.to_pandas())
    return (scanned,)



@app.cell
def _():
    # Optional cleanup. To run uncomment and run cell
    # print(f"Deleting table: {test_table}")
    # catalog.drop_table(test_table)
    # print("Table dropped.")
    return



if __name__ == "__main__":
    app.run()

```

--------------------------------

### Spark Application for R2 Data Catalog Demo (Java)

Source: https://developers.cloudflare.com/r2/llms-full

This Java code defines a Spark application that connects to Cloudflare R2 using Iceberg. It creates a namespace, writes data to a table, and queries it. It requires environment variables for catalog URI, warehouse, and token.

```java
package com.example


import org.apache.spark.sql.SparkSession


object R2DataCatalogDemo {
    def main(args: Array[String]): Unit = {


        val uri = sys.env("CATALOG_URI")
        val warehouse = sys.env("WAREHOUSE")
        val token = sys.env("TOKEN")


        val spark = SparkSession.builder()
            .appName("My R2 Data Catalog Demo")
            .master("local[*]")
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
            .config("spark.sql.catalog.mydemo", "org.apache.iceberg.spark.SparkCatalog")
            .config("spark.sql.catalog.mydemo.type", "rest")
            .config("spark.sql.catalog.mydemo.uri", uri)
            .config("spark.sql.catalog.mydemo.warehouse", warehouse)
            .config("spark.sql.catalog.mydemo.token", token)
            .getOrCreate()


        import spark.implicits._


        val data = Seq(
            (1, "Alice", 25),
            (2, "Bob", 30),
            (3, "Charlie", 35),
            (4, "Diana", 40)
        ).toDF("id", "name", "age")


        spark.sql("USE mydemo")


        spark.sql("CREATE NAMESPACE IF NOT EXISTS demoNamespace")


        data.writeTo("demoNamespace.demotable").createOrReplace()


        val readResult = spark.sql("SELECT * FROM demoNamespace.demotable WHERE age > 30")
        println("Records with age > 30:")
        readResult.show()
    }
}
```

--------------------------------

### Enable R2 Bucket Event Notifications

Source: https://developers.cloudflare.com/r2/tutorials/upload-logs-event-notifications

Command to enable event notifications for a specific R2 bucket (`example-upload-bucket`), directing events to the configured queue.

```bash
wrangler r2 bucket notification create example-upload-bucket --queue your-queue-name --event put
```

--------------------------------

### ListBuckets with Search Parameters

Source: https://developers.cloudflare.com/r2/api/s3/extensions

Demonstrates how to use search parameters like `prefix`, `start-after`, `continuation-token`, and `max-keys` for the ListBuckets operation in Cloudflare R2. These parameters can be sent via HTTP headers if not directly supported by the client library.

```bash
GET /?prefix=my-prefix&max-keys=100 HTTP/1.1
Host: example.com
cf-prefix: my-prefix
cf-max-keys: 100
```

--------------------------------

### Configure R2 Bucket with Wrangler CLI

Source: https://developers.cloudflare.com/r2/llms-full

Shows how to configure your R2 bucket using the Wrangler CLI, a command-line tool for Cloudflare Workers. This typically involves setting up the bucket name and region in your `wrangler.toml` configuration file.

```bash
# wrangler.toml

name = "my-worker"
main = "src/index.js"

[[r2_buckets]]
binding = "MY_BUCKET"
tag = "my-bucket-tag"

# Optional: specify a specific bucket location
# location = "us-east-1"
```

--------------------------------

### Navigate to Application Directory

Source: https://developers.cloudflare.com/r2/llms-full

Command to change the current directory to the newly created R2 worker application folder.

```sh
cd r2-worker
```

--------------------------------

### Optimize Multipart Uploads with Rclone

Source: https://developers.cloudflare.com/r2/llms-full

Shows how to configure multipart upload part sizes and cutoffs using `rclone copy` with `--s3-upload-cutoff` and `--s3-chunk-size` flags to manage Class A operations and potential costs.

```sh
rclone copy long-video.mp4 r2:user-uploads/ --s3-upload-cutoff=100M --s3-chunk-size=100M
```

--------------------------------

### Create R2 Bucket with Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Creates a new R2 bucket using the Wrangler command-line tool. Bucket names have specific naming constraints regarding characters, length, and placement of hyphens.

```sh
wrangler r2 bucket create your-bucket-name
```

--------------------------------

### Generate Presigned URLs with Rclone Link

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to generate a presigned URL for temporary public access to an object in an R2 bucket using the `rclone link` command with an expiration time.

```sh
# You can pass the --expire flag to determine how long the presigned link is valid. The --unlink flag isn't supported by R2.
rclone link r2:my-bucket-name/cat.png --expire 3600
# https://<accountid>.r2.cloudflarestorage.com/my-bucket-name/cat.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=<credential>&X-Amz-Date=<timestamp>&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=<signature>
```

--------------------------------

### Upload Object via Rclone

Source: https://developers.cloudflare.com/r2/llms-full

Shows how to upload files and folders to a Cloudflare R2 bucket using the rclone command-line tool. It covers copying a single file and an entire directory, and how to list objects in the bucket.

```sh
# Upload a single file
rclone copy /path/to/local/file.txt r2:bucket_name


# Upload everything in a directory
rclone copy /path/to/local/folder r2:bucket_name
```

```sh
rclone ls r2:bucket_name
```

--------------------------------

### Download Object from R2 with Rust SDK

Source: https://developers.cloudflare.com/r2/examples/aws/aws-sdk-rust

Shows how to download an object from a Cloudflare R2 bucket using the AWS SDK for Rust. Requires R2 configuration credentials.

```rust
use aws_sdk_s3::Client;
use std::io::Write;

// Assume access_key_id and access_key_secret are set as environment variables or provided.
// let config = aws_config::load_from_env().await;
// let s3_client = Client::new(&config);

// Placeholder for actual client instantiation with R2 configuration
// let s3_client = Client::with_config(config, aws_sdk_s3::config::Region::new("auto"));

// async fn download_object(s3_client: &Client, bucket_name: &str, object_key: &str) {
//     let mut file = File::create("downloaded_file.txt").expect("Failed to create file");
//     let resp = s3_client
//         .get_object()
//         .bucket(bucket_name)
//         .key(object_key)
//         .send()
//         .await
//         .expect("Failed to get object");
//
//     let body = resp.body.collect().await.expect("Failed to read object body");
//     file.write_all(&body).expect("Failed to write to file");
// }

```

--------------------------------

### R2MultipartOptions: Configure multipart uploads with metadata and storage class

Source: https://developers.cloudflare.com/r2/llms-full

Define options for R2 multipart uploads, allowing specification of HTTP and custom metadata, storage class, and server-side encryption keys.

```javascript
const options = {
  httpMetadata: {
    contentType: "image/jpeg"
  },
  customMetadata: {
    "x-file-id": "12345"
  },
  storageClass: "Standard"
};
```

--------------------------------

### Create Cloudflare Queue

Source: https://developers.cloudflare.com/r2/llms-full

This command creates a new Cloudflare Queue named 'pdf-summarizer' to receive event notifications from R2 bucket changes. A paid plan is required for this functionality.

```sh
npx wrangler queues create pdf-summarizer
```

--------------------------------

### Create Iceberg Namespace and Table (Python)

Source: https://developers.cloudflare.com/r2/llms-full

This code snippet shows how to create a namespace and an Iceberg table within the configured Spark session. It first ensures the namespace exists and then defines the schema for the new table.

```Python
# Create namespace if it does not exist
spark.sql("CREATE NAMESPACE IF NOT EXISTS default")


# Create a table in the namespace using Iceberg
spark.sql("""
    CREATE TABLE IF NOT EXISTS default.my_table (
        id BIGINT,
        name STRING
    )
    USING iceberg
""")
```

--------------------------------

### Create R2 Bucket Event Notification

Source: https://developers.cloudflare.com/r2/llms-full

Configures an event notification for an R2 bucket to trigger a queue ('pdf-summarizer') when a new object with the '.pdf' suffix is created.

```sh
npx wrangler r2 bucket notification create <R2_BUCKET_NAME> --event-type object-create --queue pdf-summarizer --suffix "pdf"
```

--------------------------------

### sbt Build Configuration for Spark and Iceberg (Scala)

Source: https://developers.cloudflare.com/r2/llms-full

This Scala build file (build.sbt) configures an sbt project for a Spark application using Iceberg. It specifies Spark and Iceberg versions, Scala version, and includes dependencies for core Spark, SQL, and Iceberg components. It also configures the sbt-assembly plugin for creating a fat JAR.

```scala
name := "R2DataCatalogDemo"


version := "1.0"


val sparkVersion = "3.5.3"
val icebergVersion = "1.8.1"


// You need to use binaries of Spark compiled with either 2.12 or 2.13; and 2.12 is more common.
// If you download Spark 3.5.3 with sdkman, then it comes with 2.12.18
scalaVersion := "2.12.18"


libraryDependencies ++= Seq(
    "org.apache.spark" %% "spark-core" % sparkVersion,
    "org.apache.spark" %% "spark-sql" % sparkVersion,
    "org.apache.iceberg" % "iceberg-core" % icebergVersion,
    "org.apache.iceberg" % "iceberg-spark-runtime-3.5_2.12" % icebergVersion,
    "org.apache.iceberg" % "iceberg-aws-bundle" % icebergVersion,
)


// build a fat JAR with all dependencies
assembly / assemblyMergeStrategy := {
    case PathList("META-INF", "services", xs @ _*) => MergeStrategy.concat
    case PathList("META-INF", xs @ _*) => MergeStrategy.discard
    case "reference.conf" => MergeStrategy.concat
    case "application.conf" => MergeStrategy.concat
    case x if x.endsWith(".properties") => MergeStrategy.first
    case x => MergeStrategy.first
}


// For Java  17 Compatability
Compile / javacOptions ++= Seq("--release", "17")
```

--------------------------------

### Configure Wrangler for Static Assets (JSONC)

Source: https://developers.cloudflare.com/r2/llms-full

This snippet shows how to configure the `wrangler.jsonc` file to specify the directory for static assets. This tells Wrangler where to find your front-end files.

```jsonc
{
  "assets": {
    "directory": "public"
  }
}
```

--------------------------------

### Upload Summary to R2 Bucket

Source: https://developers.cloudflare.com/r2/llms-full

Stores the generated summary as a text file in the R2 bucket, with a filename appended with '-summary.txt', setting the content type to 'text/plain'.

```typescript
async queue(batch, env) {
  for(let message of batch.messages) {
    // Extract the textual content from the PDF
    // ...
    // Use Workers AI to summarize the content
    // ...


    // Add the summary to the R2 bucket
    const upload = await env.MY_BUCKET.put(`${message.body.object.key}-summary.txt`, summary, {
          httpMetadata: {
            contentType: 'text/plain',
          },
    });
    console.log(`Summary added to the R2 bucket: ${upload.key}`);
  }
}
```

--------------------------------

### Retrieve Objects from Cloudflare R2 with Rclone

Source: https://developers.cloudflare.com/r2/examples/rclone

Illustrates how to use the `rclone copy` command to download files from a Cloudflare R2 bucket to a local destination.

```bash
rclone copy <r2_remote_name>:<bucket_name>/<object_path> <destination_path>
```

--------------------------------

### Create R2 Buckets with Wrangler

Source: https://developers.cloudflare.com/r2/tutorials/upload-logs-event-notifications

Commands to create two R2 buckets: one for uploads and another to store logs. These buckets are essential for the event notification system.

```bash
wrangler r2 bucket create example-upload-bucket
wrangler r2 bucket create example-log-sink-bucket
```

--------------------------------

### Migrate Files with rclone

Source: https://developers.cloudflare.com/r2/llms-full

This section describes how to use the rclone program to upload newly created media files to Cloudflare R2 during a migration. It highlights that re-running the sync program will check existing files using Class B operations.

```bash
rclone
```

--------------------------------

### R2ListOptions Configuration

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Explains the parameters for `R2ListOptions` used in listing objects, such as `limit`, `prefix`, `cursor`, `delimiter`, and `include` for metadata. It also notes compatibility requirements for the `include` option.

```javascript
{
  limit: number,
  prefix: string,
  cursor: string,
  delimiter: string,
  include: Array<string>
}
```

--------------------------------

### Enable R2 Data Catalog via Wrangler CLI

Source: https://developers.cloudflare.com/r2/data-catalog/manage-catalogs

Enables the R2 Data Catalog on a specified bucket using the Wrangler CLI. After execution, it returns the Catalog URI and Warehouse name required by Iceberg clients.

```bash
r2 bucket catalog enable command
```

--------------------------------

### Cloudflare R2 S3 API: ListObjectsV1 S3 Compatibility

Source: https://developers.cloudflare.com/r2/platform/release-notes

Enhances S3 compatibility for `ListObjectsV1` by ensuring the `nextMarker` is only set when the listing is truncated. This aligns R2's behavior with standard S3 practices.

```http
GET /?marker=some-marker&max-keys=1000&versions HTTP/1.1
Host: your-bucket.s3.amazonaws.com


```

--------------------------------

### Upload Image to R2 with Next.js

Source: https://developers.cloudflare.com/r2/demos

This demo showcases how to upload images to Cloudflare R2 from a Next.js application. It demonstrates a practical integration pattern for handling user-generated content.

```javascript
// Example of uploading an image to Cloudflare R2 from a Next.js application.
// This would typically involve a frontend component to select the file
// and a backend API route or a Cloudflare Worker to handle the upload.

// Frontend (e.g., React component in Next.js):
/*
async function handleImageUpload(event) {
  const file = event.target.files[0];
  const formData = new FormData();
  formData.append('image', file);

  const response = await fetch('/api/upload-to-r2', { // Assuming an API route
    method: 'POST',
    body: formData
  });

  if (response.ok) {
    console.log('Image uploaded successfully!');
  } else {
    console.error('Image upload failed.');
  }
}
*/

// Backend API Route (e.g., /pages/api/upload-to-r2.js) or Cloudflare Worker:
/*
// Using Cloudflare Workers KV/R2 bindings (example):
// Assumes you have configured R2 bindings in your wrangler.toml

export default {
  async fetch(request) {
    if (request.method === 'POST') {
      const formData = await request.formData();
      const imageFile = formData.get('image');

      if (imageFile) {
        const objectName = imageFile.name;
        await env.YOUR_R2_BUCKET.put(objectName, imageFile.stream());
        return new Response(`Uploaded ${objectName} to R2 successfully!`, { status: 200 });
      } else {
        return new Response('No image file provided.', { status: 400 });
      }
    }
    return new Response('Method not allowed', { status: 405 });
  }
}
*/

```

--------------------------------

### Access R2 Bucket Objects (Go SDK)

Source: https://developers.cloudflare.com/r2/tutorials/cloudflare-access

This Go snippet demonstrates how to interact with Cloudflare R2 using the Go SDK. It shows how to list objects in a bucket, assuming authentication is handled.

```go
package main

import (
	"context"
	"fmt"
	"github.com/cloudflare/cloudflare-go"
)

func main() {
	// Initialize the Cloudflare client
	// Replace with your actual API token or credentials
	client, err := cloudflare.NewWithAPIToken("YOUR_CLOUDFLARE_API_TOKEN")
	if err != nil {
		fmt.Println("Error creating Cloudflare client:", err)
		return
	}

	bucketName := "my-r2-bucket"

	// List objects in the bucket
	objects, err := client.ListR2Objects(context.Background(), "YOUR_ACCOUNT_ID", bucketName)
	if err != nil {
		fmt.Printf("Error listing objects in bucket %s: %v\n", bucketName, err)
		return
	}

	fmt.Printf("Objects in bucket %s:\n", bucketName)
	for _, obj := range objects {
		fmt.Printf("- Key: %s, Size: %d bytes\n", obj.Key, obj.Size)
	}
}
```

--------------------------------

### Configure Mastodon .env.production for R2

Source: https://developers.cloudflare.com/r2/llms-full

This snippet shows the necessary configuration settings within the Mastodon `.env.production` file to enable and connect to Cloudflare R2 for file storage. It includes placeholders for the custom hostname, bucket name, S3 API endpoint, and AWS credentials.

```plaintext
S3_ENABLED=true
S3_ALIAS_HOST={{mastodon-files.example.com}}                  # Change to the hostname determined in step 1
S3_BUCKET={{your-bucket-name}}                                # Change to the bucket name set in step 2
S3_ENDPOINT=https://{{unique-id}}.r2.cloudflarestorage.com/   # Change the {{unique-id}} to the part of S3 API retrieved in step 2
AWS_ACCESS_KEY_ID={{your-access-key-id}}                      # Change to the Access Key ID retrieved in step 2
AWS_SECRET_ACCESS_KEY={{your-secret-access-key}}              # Change to the Secret Access Key retrieved in step 2
S3_PROTOCOL=https
S3_PERMISSION=private
```

--------------------------------

### Add R2 Binding to Wrangler (wrangler.toml)

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Adds an R2 bucket binding to the Wrangler configuration in TOML format. This allows the Worker to interact with a specified R2 bucket, replacing `<R2_BUCKET_NAME>` with the actual bucket name.

```toml
[env.dev]
[env.prod]
name = "pdf-summarizer"
main = "src/index.ts"
compatibility_date = "2023-08-03"
[site]
 bucket = "public"
 entry-point = "/"

r2_buckets = [
  {
    "binding" = "MY_BUCKET",
    "id" = "<R2_BUCKET_NAME>"
  }
]

```

--------------------------------

### Set Lifecycle Configuration with Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Sets the entire lifecycle configuration for an R2 bucket from a JSON file using the Wrangler CLI. The JSON file must adhere to the specified API format.

```sh
npx wrangler r2 bucket lifecycle set <BUCKET_NAME> --file <FILE_PATH>
```

--------------------------------

### List R2 Buckets with Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Lists all R2 buckets within the current Cloudflare account using the Wrangler command-line tool.

```sh
wrangler r2 bucket list
```

--------------------------------

### Configure Wrangler for Static Assets (TOML)

Source: https://developers.cloudflare.com/r2/llms-full

This snippet shows how to configure the `wrangler.toml` file to specify the directory for static assets. This tells Wrangler where to find your front-end files.

```toml
[assets]
directory = "public"
```

--------------------------------

### Configure AWS SDK for PHP with Cloudflare R2

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to configure the AWS SDK for PHP (v3) to interact with Cloudflare R2. This involves setting up credentials and the R2 endpoint when creating the S3 client. It shows how to list objects and buckets.

```php
<?php
require 'vendor/aws/aws-autoloader.php';


$bucket_name        = "sdk-example";
$account_id         = "<accountid>";
$access_key_id      = "<access_key_id>";
$access_key_secret  = "<access_key_secret>";


$credentials = new AwsCredentialsCredentials($access_key_id, $access_key_secret);


$options = [
    'region' => 'auto',
    'endpoint' => "https://$account_id.r2.cloudflarestorage.com",
    'version' => 'latest',
    'credentials' => $credentials
];


$s3_client = new AwsS3S3Client($options);


$contents = $s3_client->listObjectsV2([
    'Bucket' => $bucket_name
]);


var_dump($contents['Contents']);


// array(1) {
//   [0]=>
//   array(5) {
//     ["Key"]=>
//     string(14) "ferriswasm.png"
//     ["LastModified"]=>
//     object(AwsApiDateTimeResult)#187 (3) {
//       ["date"]=>
//       string(26) "2022-05-18 17:20:21.670000"
//       ["timezone_type"]=>
//       int(2)
//       ["timezone"]=>
//       string(1) "Z"
//     }
//     ["ETag"]=>
//     string(34) ""eb2b891dc67b81755d2b726d9110af16""
//     ["Size"]=>
//     string(5) "87671"
//     ["StorageClass"]=>
//     string(8) "STANDARD"
//   }
// }


$buckets = $s3_client->listBuckets();


var_dump($buckets['Buckets']);


// array(1) {
//   [0]=>
//   array(2) {
//     ["Name"]=>
//     string(11) "sdk-example"
//     ["CreationDate"]=>
//     object(AwsApiDateTimeResult)#212 (3) {
//       ["date"]=>
//       string(26) "2022-05-18 17:19:59.645000"
//       ["timezone_type"]=>
//       int(2)
//       ["timezone"]=>
//       string(1) "Z"
//     }
//   }
// }


?>

```

--------------------------------

### Add S3 Virtual-Hosted Style Paths and GetBucketLocation

Source: https://developers.cloudflare.com/r2/platform/release-notes

Introduces support for S3 virtual-hosted style paths (e.g., <BUCKET>.<ACCOUNT_ID>.r2.cloudflarestorage.com) and implements the GetBucketLocation operation for compatibility with external tools, returning 'auto' as the LocationConstraint.

```S3 API
S3 virtual-hosted style paths
GetBucketLocation operation
LocationConstraint: auto
```

--------------------------------

### Configure AWS SDK for .NET with R2 Credentials

Source: https://developers.cloudflare.com/r2/examples/aws/aws-sdk-net

Demonstrates how to set up the AWS SDK for .NET client by explicitly providing R2 configuration credentials. This is essential for authenticating requests to Cloudflare R2 storage.

```csharp
using Amazon.S3;

// Replace with your actual Access Key ID and Secret Access Key
var accessKeyId = "YOUR_ACCESS_KEY_ID";
var accessKeySecret = "YOUR_ACCESS_KEY_SECRET";

var s3Client = new AmazonS3Client(accessKeyId, accessKeySecret, new AmazonS3Config
{
    ServiceURL = "https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com"
});

// Now you can use s3Client to interact with R2
```

--------------------------------

### Upload Object to R2 with Rust SDK

Source: https://developers.cloudflare.com/r2/examples/aws/aws-sdk-rust

Demonstrates how to upload an object to a Cloudflare R2 bucket using the AWS SDK for Rust. Requires R2 configuration credentials.

```rust
use aws_sdk_s3::Client;
use std::fs::File;

// Assume access_key_id and access_key_secret are set as environment variables or provided.
// let config = aws_config::load_from_env().await;
// let s3_client = Client::new(&config);

// Placeholder for actual client instantiation with R2 configuration
// let s3_client = Client::with_config(config, aws_sdk_s3::config::Region::new("auto"));

// async fn upload_object(s3_client: &Client, bucket_name: &str, object_key: &str, file_path: &str) {
//     let file = File::open(file_path).expect("Failed to open file");
//     let _ = s3_client
//         .put_object()
//         .bucket(bucket_name)
//         .key(object_key)
//         .body(file.into())
//         .send()
//         .await;
// }

```

--------------------------------

### Configure Wrangler for Static Assets (wrangler.toml)

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Configuration for Wrangler to enable Static Assets, allowing the serving of front-end files from the Worker. This TOML file defines the assets directory and the file to serve as the index.

```toml
[env.dev]
[env.prod]
name = "pdf-summarizer"
main = "src/index.ts"
compatibility_date = "2023-08-03"
[site]
 bucket = "public"
 entry-point = "/"

```

--------------------------------

### Configure Wrangler for Static Assets (wrangler.jsonc)

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Configuration for Wrangler to enable Static Assets, allowing the serving of front-end files from the Worker. This JSONC file defines the assets directory and the file to serve as the index.

```json
{
  "$schema": "https://json.schemastore.org/wrangler",
  "compatibilityDate": "2023-08-03",
  "main": "src/index.ts",
  "server": {
    "assetsDir": "public"
  },
  "routes": [
    {
      "pattern": "/api/upload",
      "methods": ["POST", "GET", "PUT", "DELETE", "OPTIONS", "PATCH", "HEAD", "POST"]
    }
  ]
}

```

--------------------------------

### List Lifecycle Rules with Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Retrieves the list of lifecycle rules associated with an R2 bucket using the Wrangler CLI. Requires the bucket name as an argument.

```sh
npx wrangler r2 bucket lifecycle list <BUCKET_NAME>
```

--------------------------------

### Generate Presigned URL for PutObject in Go

Source: https://developers.cloudflare.com/r2/llms-full

This Go code snippet demonstrates how to generate a presigned URL for temporarily sharing public write access to an object in Cloudflare R2 using the AWS SDK for Go. It initializes a presign client and calls `PresignPutObject` with the bucket name and object key.

```go
presignClient := s3.NewPresignClient(client)


presignResult, err := presignClient.PresignPutObject(context.TODO(), &s3.PutObjectInput{
    Bucket: aws.String(bucketName),
    Key:    aws.String("example.txt"),
})


if err != nil {
    panic("Couldn't get presigned URL for PutObject")
}


fmt.Printf("Presigned URL For object: %s\n", presignResult.URL)
```

--------------------------------

### Create R2 Bucket using Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Command to create a new R2 bucket using the Wrangler CLI. Replace `<YOUR_BUCKET_NAME>` with the desired name for your bucket.

```sh
npx wrangler r2 bucket create <YOUR_BUCKET_NAME>
```

--------------------------------

### Connect Custom Domain to R2 Bucket

Source: https://developers.cloudflare.com/r2/llms-full

Steps to connect a custom domain to an R2 bucket, including DNS configuration and status verification. This process involves interacting with the Cloudflare dashboard.

```bash
Step 1: Go to R2 and select your bucket.
Step 2: On the bucket page, select Settings.
Step 3: Under Custom Domains, select Add.
Step 4: Enter the domain name and select Continue.
Step 5: Review the DNS record and select Connect Domain.
```

--------------------------------

### S3 API: PutBucketCors Method

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates the use of the `PutBucketCors` method in the S3 compatible API for R2, which is used to set the CORS configuration for a bucket.

```Shell
# Example using AWS CLI (replace with actual bucket name and region)
# aws s3api put-bucket-cors --bucket my-r2-bucket --cors-configuration file://cors-config.json --endpoint-url https://<account_id>.r2.cloudflarestorage.com
```

--------------------------------

### Cloudflare R2 S3 API: Location Hints

Source: https://developers.cloudflare.com/r2/platform/release-notes

Introduces the ability to set location hints when creating buckets, both via the S3 API and the Cloudflare dashboard. This can help optimize data locality.

```http
PUT /my-new-bucket HTTP/1.1
Host: your-bucket.s3.amazonaws.com
X-Amz-Bucket-Location: us-east-1


```

--------------------------------

### API: Configure R2 Bucket Custom Domains

Source: https://developers.cloudflare.com/r2/platform/release-notes

Details the API support for configuring R2 bucket custom domains. This enables programmatic management of custom domains, complementing the Wrangler CLI functionality.

```bash
POST /buckets/:bucket/domains
```

--------------------------------

### HTML Structure for PDF Upload Front-end

Source: https://developers.cloudflare.com/r2/llms-full

This HTML code defines the structure of the front-end for a PDF summarizer application. It includes a file input, an upload button, and basic styling for the page layout and elements. It also includes links to relevant Cloudflare documentation and a GitHub repository.

```html
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>PDF Summarizer</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        display: flex;
        flex-direction: column;
        min-height: 100vh;
        margin: 0;
        background-color: #fefefe;
      }
      .content {
        flex: 1;
        display: flex;
        justify-content: center;
        align-items: center;
      }
      .upload-container {
        background-color: #f0f0f0;
        padding: 20px;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
      }
      .upload-button {
        background-color: #4caf50;
        color: white;
        padding: 10px 15px;
        border: none;
        border-radius: 4px;
        cursor: pointer;
        font-size: 16px;
      }
      .upload-button:hover {
        background-color: #45a049;
      }
      footer {
        background-color: #f0f0f0;
        color: white;
        text-align: center;
        padding: 10px;
        width: 100%;
      }
      footer a {
        color: #333;
        text-decoration: none;
        margin: 0 10px;
      }
      footer a:hover {
        text-decoration: underline;
      }
    </style>
  </head>
  <body>
    <div class="content">
      <div class="upload-container">
        <h2>Upload PDF File</h2>
        <form id="uploadForm" onsubmit="return handleSubmit(event)">
          <input
            type="file"
            id="pdfFile"
            name="pdfFile"
            accept=".pdf"
            required
          />
          <button type="submit" id="uploadButton" class="upload-button">
            Upload
          </button>
        </form>
      </div>
    </div>


    <footer>
      <a
        href="https://developers.cloudflare.com/r2/buckets/event-notifications/"
        target="_blank"
        >R2 Event Notification</a
      >
      <a
        href="https://developers.cloudflare.com/queues/get-started/#3-create-a-queue"
        target="_blank"
        >Cloudflare Queues</a
      >
      <a href="https://developers.cloudflare.com/workers-ai/" target="_blank"
        >Workers AI</a
      >
      <a
        href="https://github.com/harshil1712/pdf-summarizer-r2-event-notification"
        target="_blank"
        >GitHub Repo</a
      >
    </footer>


    <script>
      handleSubmit = async (event) => {
        event.preventDefault();


        // Disable the upload button and show a loading message
        const uploadButton = document.getElementById("uploadButton");
        uploadButton.disabled = true;
        uploadButton.textContent = "Uploading...";


        // get form data
        const formData = new FormData(event.target);
        const file = formData.get("pdfFile");


        if (file) {
          // call /api/upload endpoint and send the file
          await fetch("/api/upload", {
            method: "POST",
            body: formData,
          });


          event.target.reset();
        } else {
          console.log("No file selected");
        }
        uploadButton.disabled = false;
        uploadButton.textContent = "Upload";
      };
    </script>
  </body>
</html>
```

--------------------------------

### Configure Rclone for Cloudflare R2

Source: https://developers.cloudflare.com/r2/examples/rclone

Steps to configure Rclone to use Cloudflare R2 as a storage provider. This involves running `rclone config` and providing specific details for the R2 service.

```bash
rclone config
```

--------------------------------

### Manage R2 Bucket Custom Domains with Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Control custom domains for R2 buckets using the Wrangler CLI. Supports listing, adding, removing, and updating custom domain configurations for public buckets.

```bash
wrangler r2 bucket domain list <bucket-name>
wrangler r2 bucket domain add <bucket-name> <domain>
wrangler r2 bucket domain remove <bucket-name> <domain>
wrangler r2 bucket domain update <bucket-name> <domain>
```

--------------------------------

### Cloudflare R2 S3 API: ListParts Implementation

Source: https://developers.cloudflare.com/r2/platform/release-notes

Announces the implementation and availability of the `ListParts` API, which is used in conjunction with multipart uploads to list the parts of an uploaded object.

```http
GET /my-object?uploadId=EXAMPLEID&list-parts HTTP/1.1
Host: your-bucket.s3.amazonaws.com


```

--------------------------------

### Deploy Worker

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Command to deploy the Cloudflare Worker using the Wrangler CLI. This makes the Worker accessible via a URL.

```bash
wrangler deploy
```

--------------------------------

### Use Location Hints for Cloudflare R2 Bucket Creation

Source: https://developers.cloudflare.com/r2/index

Demonstrates how to use Location Hints when creating a Cloudflare R2 bucket. Location Hints are optional parameters that specify the primary geographical location for data access, influencing data placement for optimized performance.

```JavaScript
async function createBucketWithLocationHint(accountId, bucketName, locationHint) {
  const url = `https://api.cloudflare.com/client/v4/accounts/${accountId}/r2/buckets`;
  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer YOUR_API_TOKEN`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      name: bucketName,
      locationHint: locationHint
    })
  });
  const data = await response.json();
  console.log(data);
}
```

--------------------------------

### Generate Presigned URLs with Rclone

Source: https://developers.cloudflare.com/r2/examples/rclone

Demonstrates the use of the `rclone link` command to generate temporary, publicly accessible URLs for files stored in Cloudflare R2.

```bash
rclone link <r2_remote_name>:<bucket_name>/<object_path>
```

--------------------------------

### Migrate Mastodon Media Files using rclone

Source: https://developers.cloudflare.com/r2/tutorials/mastodon

This command demonstrates how to use `rclone` to migrate local Mastodon media files to a Cloudflare R2 bucket. It assumes you have configured `rclone` with your R2 credentials and bucket details.

```bash
rclone sync /path/to/local/mastodon/media/files/ r2-remote:your-r2-bucket-name/media/
```

--------------------------------

### S3 ListMultipartUploads Beta Support

Source: https://developers.cloudflare.com/r2/platform/release-notes

Beta support for the S3 ListMultipartUploads operation has been added.

```text
Beta `ListMultipartUploads` support.
```

--------------------------------

### Configure aws-sdk-php for R2

Source: https://developers.cloudflare.com/r2/examples/aws/aws-sdk-php

This snippet shows how to configure the aws-sdk-php client to use Cloudflare R2. It requires the R2 configuration credentials to be passed during the S3 service client instantiation.

```php
require 'vendor/autoload.php';

use Aws\S3\S3Client;

$access_key_id = 'YOUR_ACCESS_KEY_ID';
$secret_access_key = 'YOUR_SECRET_ACCESS_KEY';
$account_id = 'YOUR_ACCOUNT_ID';
$bucket_name = 'YOUR_BUCKET_NAME';

$s3Client = new S3Client([
    'version' => 'latest',
    'region' => 'auto',
    'endpoint' => "https://{$account_id}.r2.cloudflarestorage.com",
    'credentials' => [
        'key'    => $access_key_id,
        'secret' => $secret_access_key,
    ],
]);

// Example usage: List buckets
try {
    $result = $s3Client->listBuckets();
    foreach ($result['Buckets'] as $bucket) {
        echo $bucket['Name'] . "\n";
    }
} catch (Aws\Exception\AwsException $e) {
    // Display error message
    echo $e->getMessage() . "\n";
}
```

--------------------------------

### S3 Multipart Upload XML Compatibility

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixed S3 compatibility with MinIO client spec non-compliant XML for publishing multipart uploads. Leading/trailing quotes in CompleteMultipartUpload are now optional and ignored.

```text
Fix S3 compatibility with MinIO client spec non-compliant XML for publishing multipart uploads. Any leading and trailing quotes in `CompleteMultipartUpload` are now optional and ignored as it seems to be the actual non-standard behavior AWS implements.
```

--------------------------------

### Configure Wrangler for Workers AI

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Configuration snippet for the Wrangler file (wrangler.jsonc or wrangler.toml) to include the Workers AI binding, enabling the use of AI models within the Worker.

```json
{
  "name": "pdf-summarizer",
  "compatibility_date": "2023-10-01",
  "bindings": [
    {
      "name": "AI",
      "type": "ai"
    },
    {
      "name": "R2_BUCKET",
      "type": "r2",
      "bucket_name": "your-r2-bucket-name"
    }
  ]
}
```

--------------------------------

### R2MultipartOptions Configuration

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Outlines the options available for `R2MultipartOptions`, which include HTTP metadata, custom metadata, storage class, and SSE-C keys for multipart uploads.

```javascript
{
  httpMetadata: R2HTTPMetadata | Headers,
  customMetadata: Record<string, string>,
  storageClass: string,
  ssecKey: ArrayBuffer | string
}
```

--------------------------------

### Add R2 Binding to Wrangler (wrangler.jsonc)

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Adds an R2 bucket binding to the Wrangler configuration in JSONC format. This allows the Worker to interact with a specified R2 bucket, replacing `<R2_BUCKET_NAME>` with the actual bucket name.

```json
{
  "$schema": "https://json.schemastore.org/wrangler",
  "compatibilityDate": "2023-08-03",
  "main": "src/index.ts",
  "server": {
    "assetsDir": "public"
  },
  "routes": [
    {
      "pattern": "/api/upload",
      "methods": ["POST", "GET", "PUT", "DELETE", "OPTIONS", "PATCH", "HEAD", "POST"]
    }
  ],
  "vars": {
    "R2_BUCKET_NAME": "<R2_BUCKET_NAME>"
  },
  "kv_namespaces": [],
  "durable_objects": {},
  "queues_beta": {},
  "r2_buckets": [
    {
      "binding": "MY_BUCKET",
      "id": "<R2_BUCKET_NAME>"
    }
  ]
}

```

--------------------------------

### Instantiate S3 Client with R2 Configuration (JavaScript/TypeScript)

Source: https://developers.cloudflare.com/r2/examples/aws/aws-sdk-js

Demonstrates how to configure and instantiate the S3 service client from the AWS SDK for JavaScript (or TypeScript) to interact with Cloudflare R2. This involves passing the R2-specific credentials during client instantiation.

```javascript
import S3 from 'aws-sdk/clients/s3';

const s3 = new S3({
  endpoint: 'YOUR_R2_BUCKET_ENDPOINT',
  accessKeyId: 'YOUR_ACCESS_KEY_ID',
  secretAccessKey: 'YOUR_SECRET_ACCESS_KEY',
  signatureVersion: 'v4'
});
```

--------------------------------

### Wrangler: Manage R2 Bucket Custom Domains

Source: https://developers.cloudflare.com/r2/platform/release-notes

Covers the `bucket domain` command in Wrangler for managing custom domains associated with R2 buckets. Functionality includes listing, adding, removing, and updating these custom domains.

```bash
wrangler r2 bucket domain add <bucket-name> <domain-name>
```

```bash
wrangler r2 bucket domain list <bucket-name>
```

```bash
wrangler r2 bucket domain remove <bucket-name> <domain-name>
```

--------------------------------

### S3 Multipart Upload XML Compatibility

Source: https://developers.cloudflare.com/r2/llms-full

Fixed S3 compatibility with MinIO client spec non-compliant XML for publishing multipart uploads. Leading and trailing quotes in `CompleteMultipartUpload` are now optional and ignored.

```xml
<CompleteMultipartUpload>
  <Part>
    <PartNumber>1</PartNumber>
    <ETag>"etag-value"</ETag>
  </Part>
  <Part>
    <PartNumber>2</PartNumber>
    <ETag>etag-value</ETag>
  </Part>
</CompleteMultipartUpload>
```

--------------------------------

### Download Object via Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to download a single object from a Cloudflare R2 bucket using the Wrangler CLI. The object is downloaded to the current working directory, but can be renamed or piped to stdout using flags.

```sh
wrangler r2 object get test-bucket/file.bin
```

--------------------------------

### Create Cloudflare R2 Bucket Implicitly with `cf-create-bucket-if-missing`

Source: https://developers.cloudflare.com/r2/llms-full

When uploading to a potentially non-existent Cloudflare R2 bucket, you can set the `cf-create-bucket-if-missing` header to `true`. This header ensures that if the bucket does not exist, it will be implicitly created, preventing `NoSuchBucket` errors and allowing the upload to proceed without interruption. This is particularly useful for on-demand bucket creation scenarios.

```TXT
PUT / HTTP/1.1
Host: bucket.account.r2.cloudflarestorage.com
<CreateBucketConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
   <LocationConstraint>auto</LocationConstraint>
</CreateBucketConfiguration>
```

--------------------------------

### R2 Create Bucket If Missing

Source: https://developers.cloudflare.com/r2/llms-full

The `cf-create-bucket-if-missing` option can be set on `PutObject` or `CreateMultipartUpload` requests to implicitly create the bucket if it does not already exist.

```javascript
await env.BUCKET.put(key, value, {
  customHeaders: {
    'cf-create-bucket-if-missing': 'true'
  }
});
```

--------------------------------

### List R2 Buckets using Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Command to list all existing R2 buckets associated with your Cloudflare account using the Wrangler CLI.

```sh
npx wrangler r2 bucket list
```

--------------------------------

### API: Create Temporary Access Tokens

Source: https://developers.cloudflare.com/r2/platform/release-notes

Announces the addition of an API endpoint for creating temporary access tokens. These tokens can be used for time-limited access to R2 resources.

```bash
POST /accounts/:account_id/r2/temporary-tokens
```

--------------------------------

### Implement ListObjects and ListObjectsV2 with Query Parameters

Source: https://developers.cloudflare.com/r2/llms-full

Cloudflare R2's ListObjects and ListObjectsV2 operations support various query parameters for filtering and pagination. These include delimiter, encoding-type, marker, max-keys, prefix, continuation-token, fetch-owner, and start-after.

```S3 API
ListObjects:
  Query Parameters:
    delimiter: ✅
    encoding-type: ✅
    marker: ✅
    max-keys: ✅
    prefix: ✅
ListObjectsV2:
  Query Parameters:
    list-type: ✅
    continuation-token: ✅
    delimiter: ✅
    encoding-type: ✅
    fetch-owner: ✅
    max-keys: ✅
    prefix: ✅
    start-after: ✅
```

--------------------------------

### Add AWS SDK for Ruby Dependency to Gemfile

Source: https://developers.cloudflare.com/r2/examples/aws/aws-sdk-ruby

This snippet shows how to add the necessary AWS SDK for Ruby dependency to your project's Gemfile for R2 bucket operations.

```ruby
gem "aws-sdk-ruby"
```

--------------------------------

### Cloudflare R2 Client with AWS SDK for Java v2

Source: https://developers.cloudflare.com/r2/llms-full

This Java code demonstrates a client for interacting with Cloudflare R2 storage using the AWS SDK for Java v2. It includes a `CloudflareR2Client` class with methods to list buckets and objects, and a nested `S3Config` class for managing R2 credentials and endpoint configuration. The `buildS3Client` method configures the S3 client with R2-specific settings like endpoint override and path-style access.

```java
import software.amazon.awssdk.auth.credentials.AwsBasicCredentials;
import software.amazon.awssdk.auth.credentials.StaticCredentialsProvider;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.services.s3.model.*;
import software.amazon.awssdk.services.s3.S3Configuration;
import java.net.URI;
import java.util.List;


/**
 * Client for interacting with Cloudflare R2 Storage using AWS SDK S3 compatibility
 */
public class CloudflareR2Client {
    private final S3Client s3Client;


    /**
     * Creates a new CloudflareR2Client with the provided configuration
     */
    public CloudflareR2Client(S3Config config) {
        this.s3Client = buildS3Client(config);
    }


    /**
     * Configuration class for R2 credentials and endpoint
     */
    public static class S3Config {
        private final String accountId;
        private final String accessKey;
        private final String secretKey;
        private final String endpoint;


        public S3Config(String accountId, String accessKey, String secretKey) {
            this.accountId = accountId;
            this.accessKey = accessKey;
            this.secretKey = secretKey;
            this.endpoint = String.format("https://%s.r2.cloudflarestorage.com", accountId);
        }


        public String getAccessKey() { return accessKey; }
        public String getSecretKey() { return secretKey; }
        public String getEndpoint() { return endpoint; }
    }


    /**
     * Builds and configures the S3 client with R2-specific settings
     */
    private static S3Client buildS3Client(S3Config config) {
        AwsBasicCredentials credentials = AwsBasicCredentials.create(
            config.getAccessKey(),
            config.getSecretKey()
        );


        S3Configuration serviceConfiguration = S3Configuration.builder()
            .pathStyleAccessEnabled(true)
            .build();


        return S3Client.builder()
            .endpointOverride(URI.create(config.getEndpoint()))
            .credentialsProvider(StaticCredentialsProvider.create(credentials))
            .region(Region.of("auto"))
            .serviceConfiguration(serviceConfiguration)
            .build();
    }


    /**
     * Lists all buckets in the R2 storage
     */
    public List<Bucket> listBuckets() {
        try {
            return s3Client.listBuckets().buckets();
        } catch (S3Exception e) {
            throw new RuntimeException("Failed to list buckets: " + e.getMessage(), e);
        }
    }


    /**
     * Lists all objects in the specified bucket
     */
    public List<S3Object> listObjects(String bucketName) {
        try {
            ListObjectsV2Request request = ListObjectsV2Request.builder()
                .bucket(bucketName)
                .build();


            return s3Client.listObjectsV2(request).contents();
        } catch (S3Exception e) {
            throw new RuntimeException("Failed to list objects in bucket " + bucketName + ": " + e.getMessage(), e);
        }
    }


    public static void main(String[] args) {
        S3Config config = new S3Config(
            "your_account_id",
            "your_access_key",
            "your_secret_key"
        );


        CloudflareR2Client r2Client = new CloudflareR2Client(config);


        // List buckets
        System.out.println("Available buckets:");
        r2Client.listBuckets().forEach(bucket ->

```

--------------------------------

### Enable R2 Bucket Event Notifications

Source: https://developers.cloudflare.com/r2/llms-full

This command enables event notifications for an R2 bucket, directing 'object-create' events to a specified queue. This links the R2 bucket to the queue consumer Worker.

```sh
npx wrangler r2 bucket notification create example-upload-bucket --event-type object-create --queue example-event-notification-queue
```

--------------------------------

### Enable R2 Data Catalog with Wrangler CLI

Source: https://developers.cloudflare.com/r2/llms-full

Enables the R2 Data Catalog feature for a specified R2 bucket using the Wrangler CLI. This command is necessary to make data in the bucket queryable by external engines and requires noting the 'Warehouse' and 'Catalog URI' for future use.

```plaintext
npx wrangler r2 bucket catalog enable r2-data-catalog-tutorial
```

--------------------------------

### Configure AWS CLI for R2

Source: https://developers.cloudflare.com/r2/llms-full

Configure your AWS CLI profile with your R2 credentials. This involves setting the Access Key ID, Secret Access Key, default region, and output format.

```shell
aws configure
AWS Access Key ID [None]: <access_key_id>
AWS Secret Access Key [None]: <access_key_secret>
Default region name [None]: auto
Default output format [None]: json
```

--------------------------------

### Implement CreateMultipartUpload with System Metadata, Storage Class, and SSE-C

Source: https://developers.cloudflare.com/r2/llms-full

The CreateMultipartUpload operation in Cloudflare R2 allows setting system metadata, specifying storage classes (STANDARD, STANDARD_IA), and utilizing SSE-C for encryption with corresponding headers. It does not support website redirect, various SSE configurations, Request Payer, tagging, object locking, or ACLs.

```S3 API
CreateMultipartUpload:
  System Metadata:
    Content-Type: ✅
    Cache-Control: ✅
    Content-Disposition: ✅
    Content-Encoding: ✅
    Content-Language: ✅
    Expires: ✅
    Content-MD5: ✅
  Storage Class:
    x-amz-storage-class: ✅
    STANDARD: ✅
    STANDARD_IA: ✅
  SSE-C:
    x-amz-server-side-encryption-customer-algorithm: ✅
    x-amz-server-side-encryption-customer-key: ✅
    x-amz-server-side-encryption-customer-key-MD5: ✅
```

--------------------------------

### S3 ListBuckets Headers and Parameters

Source: https://developers.cloudflare.com/r2/platform/release-notes

Updates the S3 API ListBuckets operation to accept `cf-max-keys`, `cf-start-after`, and `cf-continuation-token` headers, making them behave consistently with their respective URL parameters.

```S3 API
ListBuckets operation headers (cf-max-keys, cf-start-after, cf-continuation-token)
```

--------------------------------

### Download Object from R2 using Rust

Source: https://developers.cloudflare.com/r2/llms-full

This Rust code snippet demonstrates how to download an object from a Cloudflare R2 bucket using the s3 SDK. It requires an S3 client, bucket name, object key, and an output path. The function asynchronously fetches the object, collects its body into bytes, and writes it to a local file.

```rust
use std::fs;
use std::io::Write;


async fn download_object(
    client: &s3::Client,
    bucket: &str,
    key: &str,
    output_path: &str,
) -> Result<(), Box<dyn std::error::Error>> {
    let resp = client
        .get_object()
        .bucket(bucket)
        .key(key)
        .send()
        .await?;


    let data = resp.body.collect().await?;
    let bytes = data.into_bytes();


    let mut file = fs::File::create(output_path)?;
    file.write_all(&bytes)?;


    println!("Downloaded {}/{}" to {}", bucket, key, output_path);
    Ok(())
}
```

--------------------------------

### Deploy Worker with Wrangler

Source: https://developers.cloudflare.com/r2/tutorials/upload-logs-event-notifications

Command to deploy the configured Worker to the Cloudflare platform using the Wrangler CLI.

```bash
wrangler deploy
```

--------------------------------

### S3 List Operation Unsupported Parameters

Source: https://developers.cloudflare.com/r2/platform/release-notes

Unsupported search parameters to ListObjects/ListObjectsV2 are now rejected with a 501 Not Implemented error.

```text
Unsupported search parameters to `ListObjects`/`ListObjectsV2` are now rejected with `501 Not Implemented`.
```

--------------------------------

### List Cloudflare R2 Buckets with Wrangler

Source: https://developers.cloudflare.com/r2/buckets/create-buckets

Lists all buckets associated with your current Cloudflare account using the `r2 bucket list` command. This helps in managing your R2 storage.

```bash
r2 bucket list
```

--------------------------------

### Create R2 Bucket with Wrangler CLI

Source: https://developers.cloudflare.com/r2/llms-full

Creates a new R2 bucket using the Wrangler CLI. This command requires a unique bucket name and is a fundamental step for using R2 Object Storage.

```plaintext
npx wrangler r2 bucket create r2-data-catalog-tutorial
```

--------------------------------

### List Objects in a Bucket with AWS SDK v3

Source: https://developers.cloudflare.com/r2/llms-full

Illustrates how to list objects within a specified bucket in Cloudflare R2 using the `ListObjectsV2Command` with the AWS SDK for JavaScript v3.

```typescript
console.log(
  await S3.send(new ListObjectsV2Command({ Bucket: "my-bucket-name" })),
);
```

--------------------------------

### Cloudflare R2 S3 API: Wildcard CORS Rules and Presigned URLs

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixes issues related to wildcard CORS rules and presigned URLs, ensuring that CORS configurations are correctly applied even with complex wildcard patterns and when using presigned URLs.

```xml
<CORSConfiguration>
  <CORSRule>
    <AllowedOrigin>*</AllowedOrigin>
    <AllowedMethod>GET</AllowedMethod>
    <AllowedMethod>PUT</AllowedMethod>
    <AllowedHeader>*</AllowedHeader>
  </CORSRule>
</CORSConfiguration>

```

--------------------------------

### Generate Presigned URLs with AWS SDK for .NET

Source: https://developers.cloudflare.com/r2/examples/aws/aws-sdk-net

Demonstrates how to generate a presigned URL for a `PutObject` operation using the `GetPreSignedURL` method in the AWS SDK for .NET. This allows temporary, signed access to an object.

```csharp
using Amazon.S3;

var url = s3Client.GetPreSignedURL(new GetPreSignedUrlRequest
{
    BucketName = "your-bucket-name",
    Verb = HttpVerb.PUT,
    Key = "sdk-example/file.txt",
    Expires = DateTime.UtcNow.AddMinutes(5) // URL expires in 5 minutes
});

Console.WriteLine($"Presigned URL: {url}");
```

--------------------------------

### Wrangler: Manage R2 Bucket Lifecycle Rules

Source: https://developers.cloudflare.com/r2/platform/release-notes

Introduces the `bucket lifecycle` command in Wrangler for managing object lifecycle rules on R2 buckets. This includes functionality to list, add, and remove these rules.

```bash
wrangler r2 bucket lifecycle --help
```

--------------------------------

### R2 Batch Deletion with delete()

Source: https://developers.cloudflare.com/r2/llms-full

The R2 `delete()` binding has been updated to support deleting multiple keys at once. This enhances efficiency for bulk data management operations.

```javascript
await env.BUCKET.delete(['key1', 'key2', 'key3']);
```

--------------------------------

### S3 Virtual-Hosted Style Paths

Source: https://developers.cloudflare.com/r2/llms-full

Supports S3 virtual-hosted style paths, such as `<BUCKET>.<ACCOUNT_ID>.r2.cloudflarestorage.com`, for accessing buckets.

```bash
curl http://<bucket-name>.<account-id>.r2.cloudflarestorage.com/<object-key>
```

--------------------------------

### Navigate to Project Directory

Source: https://developers.cloudflare.com/r2/llms-full

This command changes the current directory to the newly created Cloudflare Worker project folder, allowing you to access and modify project files.

```sh
cd pdf-summarizer
```

--------------------------------

### Add AI Type Definition

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Command to execute for adding the AI type definition, likely to enhance TypeScript support and autocompletion when working with Workers AI.

```bash
npx wrangler generate --type-definitions ai
```

--------------------------------

### Operate on R2 Buckets with AWS SDK for Ruby

Source: https://developers.cloudflare.com/r2/examples/aws/aws-sdk-ruby

Demonstrates how to use Ruby with the AWS SDK to perform operations on Cloudflare R2 buckets. Ensure you have generated your Access Key ID and Secret Access Key.

```ruby
# Initialize the R2 client with your credentials
r2 = Aws::S3::Client.new(
  endpoint: "https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com",
  access_key_id: ENV["ACCESS_KEY_ID"],
  secret_access_key: ENV["SECRET_ACCESS_KEY"],
  region: "us-east-1" # Or your desired region
)

# Example: List buckets (Note: R2 does not support listing buckets directly via S3 API)
# You would typically interact with a specific bucket.

# Example: Upload a file to a bucket
# File.open("path/to/your/local/file.txt", "rb") do |file|
#   r2.put_object(
#     bucket: "your-bucket-name",
#     key: "your-object-key.txt",
#     body: file
#   )
# end

# Example: Download a file from a bucket
# object = r2.get_object(
#   bucket: "your-bucket-name",
#   key: "your-object-key.txt"
# )
# File.open("path/to/save/downloaded_file.txt", "wb") do |file|
#   file.write(object.body.read)
# end
```

--------------------------------

### Add Queue Binding to Wrangler (wrangler.toml)

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Adds a queue binding named 'MY_QUEUE' to the Wrangler configuration in TOML format, allowing the Worker to interact with a Cloudflare Queue for event notifications.

```toml
[env.dev]
[env.prod]
name = "pdf-summarizer"
main = "src/index.ts"
compatibility_date = "2023-08-03"
[site]
 bucket = "public"
 entry-point = "/"

queues_beta = [
  {
    "queue" = "pdf-summarize",
    "binding" = "MY_QUEUE"
  }
]

r2_buckets = [
  {
    "binding" = "MY_BUCKET",
    "id" = "<R2_BUCKET_NAME>"
  }
]

```

--------------------------------

### S3 XML Declaration Handling

Source: https://developers.cloudflare.com/r2/llms-full

S3 XML documents sent to R2 with an XML declaration are no longer rejected with `400 Bad Request / MalformedXML`. Minor compatibility fixes for Arq Backup on Windows have also been applied.

```xml
<?xml version="1.0" encoding="UTF-8"?>
<ListBucketResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
  ...
</ListBucketResult>
```

--------------------------------

### List lifecycle rules using Wrangler

Source: https://developers.cloudflare.com/r2/buckets/object-lifecycles

This command retrieves the list of lifecycle rules associated with your R2 bucket using the Wrangler CLI.

```bash
wrangler r2 bucket lifecycle list <bucket-name>
```

--------------------------------

### Bind R2 Bucket to Worker (wrangler.toml)

Source: https://developers.cloudflare.com/r2/llms-full

Configuration in `wrangler.toml` to bind an R2 bucket to a Worker. Specifies the binding name (a JavaScript variable) and the bucket name.

```toml
[[r2_buckets]]
binding = 'MY_BUCKET' # <~ valid JavaScript variable name
bucket_name = '<YOUR_BUCKET_NAME>'
```

--------------------------------

### Cloudflare R2 Create Bucket If Missing

Source: https://developers.cloudflare.com/r2/platform/release-notes

The `cf-create-bucket-if-missing` flag can be set on PutObject/CreateMultipartUpload requests to implicitly create the bucket if it does not exist.

```text
`cf-create-bucket-if-missing` can be set on a `PutObject`/`CreateMultipartUpload` request to implicitly create the bucket if it does not exist.
```

--------------------------------

### Create a Cloudflare R2 Bucket using Postman

Source: https://developers.cloudflare.com/r2/llms-full

Create a new R2 bucket by setting the 'r2-bucket' variable with the desired bucket name and sending the 'PUT CreateBucket' request in Postman. The creation is confirmed by a 200 OK response.

```Postman
Set 'r2-bucket' variable to your desired bucket name.
Navigate to Cloudflare R2 folder > Buckets folder > PUT CreateBucket.
Select Send.
Expected response: 200 OK.
```

--------------------------------

### Generate Presigned URLs for Cloudflare R2

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to generate presigned URLs for temporary read ('getObject') and write ('putObject') access to objects in a Cloudflare R2 bucket using the AWS SDK for JavaScript.

```javascript
console.log(
  await s3.getSignedUrlPromise("getObject", {
    Bucket: "my-bucket-name",
    Key: "dog.png",
    Expires: 3600,
  }),
);
// https://my-bucket-name.<accountid>.r2.cloudflarestorage.com/dog.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=<credential>&X-Amz-Date=<timestamp>&X-Amz-Expires=3600&X-Amz-Signature=<signature>&X-Amz-SignedHeaders=host


// You can also create links for operations such as putObject to allow temporary write access to a specific key.
console.log(
  await s3.getSignedUrlPromise("putObject", {
    Bucket: "my-bucket-name",
    Key: "dog.png",
    Expires: 3600,
  }),
);
```

--------------------------------

### Authenticate and Fetch Object with R2 API (Python)

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to authenticate against the Cloudflare R2 API using the `boto3` library for Python. It configures the S3 client with R2-specific endpoints and credentials, then fetches an object from a specified bucket.

```python
import os
import hashlib
import boto3
from botocore.client import Config


ACCOUNT_ID = os.environ.get('R2_ACCOUNT_ID')
ACCESS_KEY_ID = os.environ.get('R2_ACCESS_KEY_ID')
SECRET_ACCESS_KEY = os.environ.get('R2_SECRET_ACCESS_KEY')
BUCKET_NAME = os.environ.get('R2_BUCKET_NAME')


# Hash the secret access key using SHA-256
hashed_secret_key = hashlib.sha256(SECRET_ACCESS_KEY.encode()).hexdigest()


# Configure the S3 client for Cloudflare R2
s3_client = boto3.client('s3',
    endpoint_url=f'https://{ACCOUNT_ID}.r2.cloudflarestorage.com',
    aws_access_key_id=ACCESS_KEY_ID,
    aws_secret_access_key=hashed_secret_key,
    config=Config(signature_version='s3v4')
)


# Specify the object key
object_key = '2024/08/02/ingested_0001.parquet'


try:
    # Fetch the object
    response = s3_client.get_object(Bucket=BUCKET_NAME, Key=object_key)


    print('Successfully fetched the object')


    # Process the response content as needed
    # For example, to read the content:
    # object_content = response['Body'].read()


    # Or to save the file:
    # with open('ingested_0001.parquet', 'wb') as f:
    #     f.write(response['Body'].read())


except Exception as e:
    print(f'Failed to fetch the object. Error: {str(e)}')

```

--------------------------------

### Deploy Cloudflare Worker

Source: https://developers.cloudflare.com/r2/llms-full

Deploys the Cloudflare Worker application using the 'wrangler deploy' command, providing the URL of the deployed service.

```sh
npx wrangler deploy
```

--------------------------------

### Verify Authorization Logic

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-usage

These commands show how to test the authorization logic of your deployed Worker by making requests to your Worker's endpoint. This helps ensure that access controls are functioning as expected.

```bash
curl -X PUT -T "./file.txt" https://<YOUR_WORKER_SUBDOMAIN>.<YOUR_ZONE>.workers.dev/<YOUR_BUCKET_NAME>/file.txt \
  --header "Authorization: Bearer <YOUR_AUTH_KEY>"

curl -X DELETE https://<YOUR_WORKER_SUBDOMAIN>.<YOUR_ZONE>.workers.dev/<YOUR_BUCKET_NAME>/file.txt \
  --header "Authorization: Bearer <YOUR_AUTH_KEY>"

curl https://<YOUR_WORKER_SUBDOMAIN>.<YOUR_ZONE>.workers.dev/<YOUR_BUCKET_NAME>/file.txt

```

--------------------------------

### Interact with Cloudflare R2 using AWS SDK (TypeScript)

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to use the AWS SDK for JavaScript (v3) to interact with Cloudflare R2's S3-compatible API. This includes uploading single-part and multi-part objects, completing multipart uploads, and retrieving object metadata and content. It requires R2 credentials and endpoint to be configured via environment variables.

```typescript
import {
  UploadPartCommand,
  PutObjectCommand,
  S3Client,
  CompleteMultipartUploadCommand,
  CreateMultipartUploadCommand,
  type UploadPartCommandOutput,
  HeadObjectCommand,
  GetObjectCommand
} from "@aws-sdk/client-s3";


const s3 = new S3Client({
  endpoint: process.env.R2_ENDPOINT,
  credentials: {
    accessKeyId: process.env.R2_ACCESS_KEY_ID,
    secretAccessKey: process.env.R2_SECRET_ACCESS_KEY,
  },
});


const SSECustomerAlgorithm = "AES256";
const SSECustomerKey = process.env.R2_SSEC_KEY;
const SSECustomerKeyMD5 = process.env.R2_SSEC_KEY_MD5;


await s3.send(
  new PutObjectCommand({
    Bucket: "your-bucket",
    Key: "single-part",
    Body: "BeepBoop",
    SSECustomerAlgorithm,
    SSECustomerKey,
    SSECustomerKeyMD5,
  }),
);


const multi = await s3.send(
  new CreateMultipartUploadCommand({
    Bucket: "your-bucket",
    Key: "multi-part",
    SSECustomerAlgorithm,
    SSECustomerKey,
    SSECustomerKeyMD5,
  }),
);
const UploadId = multi.UploadId;


const parts: UploadPartCommandOutput[] = [];


parts.push(
  await s3.send(
    new UploadPartCommand({
      Bucket: "your-bucket",
      Key: "multi-part",
      UploadId,
      //   filledBuf() generates some random data.
      // Replace with a function/body of your choice.
      Body: filledBuf(),
      PartNumber: 1,
      SSECustomerAlgorithm,
      SSECustomerKey,
      SSECustomerKeyMD5,
    }),
  ),
);
parts.push(
  await s3.send(
    new UploadPartCommand({
      Bucket: "your-bucket",
      Key: "multi-part",
      UploadId,
      //   filledBuf() generates some random data.
      // Replace with a function/body of your choice.
      Body: filledBuf(),
      PartNumber: 2,
      SSECustomerAlgorithm,
      SSECustomerKey,
      SSECustomerKeyMD5,
    }),
  ),
);
await s3.send(
  new CompleteMultipartUploadCommand({
    Bucket: "your-bucket",
    Key: "multi-part",
    UploadId,
    MultipartUpload: {
      Parts: parts.map(({ ETag }, PartNumber) => ({
        ETag,
        PartNumber: PartNumber + 1,
      })),
    },
    SSECustomerAlgorithm,
    SSECustomerKey,
    SSECustomerKeyMD5,
  }),
);


const HeadObjectOutput = await s3.send(
  new HeadObjectCommand({
    Bucket: "your-bucket",
    Key: "multi-part",
    SSECustomerAlgorithm,
    SSECustomerKey,
    SSECustomerKeyMD5,
  }),
);


const GetObjectOutput = await s3.send(
  new GetObjectCommand({
    Bucket: "your-bucket",
    Key: "single-part",
    SSECustomerAlgorithm,
    SSECustomerKey,
    SSECustomerKeyMD5,
  }),
);

```

--------------------------------

### S3 Pre-signed URLs Support

Source: https://developers.cloudflare.com/r2/platform/release-notes

Added support for S3 pre-signed URLs, enabling time-limited access to resources.

```text
Added support for S3 pre-signed URLs.
```

--------------------------------

### Add Queue Binding to Wrangler (wrangler.jsonc)

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Adds a queue binding named 'MY_QUEUE' to the Wrangler configuration in JSONC format, allowing the Worker to interact with a Cloudflare Queue for event notifications.

```json
{
  "$schema": "https://json.schemastore.org/wrangler",
  "compatibilityDate": "2023-08-03",
  "main": "src/index.ts",
  "server": {
    "assetsDir": "public"
  },
  "routes": [
    {
      "pattern": "/api/upload",
      "methods": ["POST", "GET", "PUT", "DELETE", "OPTIONS", "PATCH", "HEAD", "POST"]
    }
  ],
  "vars": {
    "R2_BUCKET_NAME": "<R2_BUCKET_NAME>"
  },
  "kv_namespaces": [],
  "durable_objects": {},
  "queues_beta": [
    {
      "queue": "pdf-summarize",
      "binding": "MY_QUEUE"
    }
  ],
  "r2_buckets": [
    {
      "binding": "MY_BUCKET",
      "id": "<R2_BUCKET_NAME>"
    }
  ]
}

```

--------------------------------

### Cloudflare R2 S3 API: GetBucket Error Handling

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixes a bug where calling `GetBucket` on a non-existent bucket would return a 500 Internal Server Error instead of the expected 404 Not Found. This improves error reporting for bucket operations.

```http
GET /non-existent-bucket HTTP/1.1
Host: s3.amazonaws.com


```

--------------------------------

### Set R2 Environment Variables

Source: https://developers.cloudflare.com/r2/llms-full

Sets the necessary environment variables for authenticating with Cloudflare R2. These include the account ID, access key ID, secret access key, and bucket name.

```sh
export R2_ACCOUNT_ID=your_account_id
export R2_ACCESS_KEY_ID=your_access_key_id
export R2_SECRET_ACCESS_KEY=your_secret_access_key
export R2_BUCKET_NAME=your_bucket_name
```

--------------------------------

### Download Object with Wrangler CLI

Source: https://developers.cloudflare.com/r2/objects/download-objects

Download a specific object from an R2 bucket using the Wrangler CLI. The object is downloaded to the current directory. Options are available to specify a new filename or pipe the output to stdout.

```bash
wrangler r2 object get file.bin --bucket test-bucket
```

```bash
wrangler r2 object get file.bin --bucket test-bucket --file new-name.bin
```

```bash
wrangler r2 object get file.bin --bucket test-bucket --pipe
```

--------------------------------

### R2 List Compatibility Flag

Source: https://developers.cloudflare.com/r2/llms-full

Support for the `r2_list_honor_include` compat flag has been added. Without this flag or date, `list` implicitly functions as `include: ['httpMetadata', 'customMetadata']`.

```javascript
await env.BUCKET.list({ include: ['customMetadata'] }); // Behavior depends on compat flag/date.
```

--------------------------------

### Handle Non-Existent Keys/Buckets in S3 Copy Operations

Source: https://developers.cloudflare.com/r2/platform/release-notes

Ensures that copying from non-existent keys or buckets in the S3 API now returns the proper NoSuchKey or NoSuchBucket response codes, replacing generic 'internal error'.

```S3 API
Copying from non-existent key/bucket
NoSuchKey / NoSuchBucket response
```

--------------------------------

### Cloudflare R2 Worker Bindings: Multipart Uploads

Source: https://developers.cloudflare.com/r2/platform/release-notes

Details the addition of worker bindings support for multipart uploads, enabling more robust and efficient handling of large file uploads to R2. This includes `UploadPartCopy` functionality.

```javascript
// Example for initiating a multipart upload (conceptual)
const uploadId = await env.MY_BUCKET.createMultipartUpload('my-large-file.txt');

// Example for uploading a part (conceptual)
await env.MY_BUCKET.uploadPart(uploadId, 1, partData);

// Example for completing a multipart upload (conceptual)
await env.MY_BUCKET.completeMultipartUpload(uploadId, ['part1-etag', 'part2-etag']);

// Example for re-enabling UploadPartCopy (conceptual)
await env.MY_BUCKET.copyPart('source-object', 'destination-object', uploadId, 1);

```

--------------------------------

### API: Ignore x-id Query Param

Source: https://developers.cloudflare.com/r2/platform/release-notes

States that the `x-id` query parameter is now ignored for all S3-compatible actions on R2, including `ListBuckets`. This change standardizes behavior and prevents unexpected filtering.

```bash
GET /?x-id=ignored
```

--------------------------------

### R2GetOptions: Server-Side Encryption Key (SSE-C)

Source: https://developers.cloudflare.com/r2/llms-full

Provide a Server-Side Encryption Customer Key (SSE-C) for secure object retrieval in R2. The key must be 32 bytes and can be provided as an ArrayBuffer or a hex-encoded string.

```javascript
const options = {
  ssecKey: new Uint8Array(32).fill(0) // Example: 32-byte ArrayBuffer
};
```

```javascript
const options = {
  ssecKey: "0123456789abcdef0123456789abcdef"
};
```

--------------------------------

### Create R2 Bucket with Location Constraint (S3 API)

Source: https://developers.cloudflare.com/r2/llms-full

This snippet demonstrates how to create an R2 bucket and specify a location hint using the `CreateBucketCommand` from the S3 API. It utilizes the `LocationConstraint` parameter to define the desired geographical region for the bucket.

```javascript
await S3.send(
  new CreateBucketCommand({
    Bucket: "YOUR_BUCKET_NAME",
    CreateBucketConfiguration: {
      LocationConstraint: "WNAM",
    },
  }),
);
```

--------------------------------

### Handle PUT Object Request with R2 Bucket

Source: https://developers.cloudflare.com/r2/llms-full

This snippet demonstrates how to handle a PUT request to store an object in an R2 bucket using the `env.MY_BUCKET.put()` method. It extracts the key from the URL and uses the request body as the object's value. The code also includes handling for unsupported HTTP methods.

```js
export default {
  async fetch(request, env) {
    const url = new URL(request.url);
    const key = url.pathname.slice(1);


    switch (request.method) {
      case "PUT":
        await env.MY_BUCKET.put(key, request.body);
        return new Response(`Put ${key} successfully!`);


      default:
        return new Response(`${request.method} is not allowed.`, {
          status: 405,
          headers: {
            Allow: "PUT",
          },
        });
    }
  },
};
```

--------------------------------

### Access R2 Bucket Objects (Python SDK)

Source: https://developers.cloudflare.com/r2/tutorials/cloudflare-access

This Python snippet demonstrates how to interact with Cloudflare R2 using the official Python SDK. It shows how to list objects in a bucket, assuming authentication is handled.

```python
from cloudflare.r2 import R2Client

# Assuming client is initialized with appropriate credentials
r2_client = R2Client("YOUR_ACCOUNT_ID", "YOUR_ACCESS_KEY_ID", "YOUR_SECRET_ACCESS_KEY")

bucket_name = "my-r2-bucket"

try:
    objects = r2_client.list_objects(bucket_name)
    for obj in objects['Contents']:
        print(f"Object: {obj['Key']}, Size: {obj['Size']}")
except Exception as e:
    print(f"An error occurred: {e}")
```

--------------------------------

### S3 Conditional Headers Support

Source: https://developers.cloudflare.com/r2/platform/release-notes

S3 conditional headers If-Match / If-None-Match now support arrays of ETags, Weak ETags, and wildcards (*), aligning with HTTP standards and AWS S3 behavior.

```text
`If-Match` / `If-None-Match` headers now support arrays of ETags, Weak ETags and wildcard (`*`) as per the HTTP standard and undocumented AWS S3 behavior.
```

--------------------------------

### Upload Image to R2 with Next.js

Source: https://developers.cloudflare.com/r2/llms-full

This snippet demonstrates how to upload images to Cloudflare R2 from a Next.js application. It serves as a starter project for integrating R2 into a web application.

```javascript
// Example usage within a Next.js application
// Assumes you have a form with a file input and a submit button

async function uploadImageToR2(file) {
  const formData = new FormData();
  formData.append('image', file);

  try {
    const response = await fetch('/api/upload-to-r2', { // Assuming an API route for upload
      method: 'POST',
      body: formData
    });

    if (response.ok) {
      const result = await response.json();
      console.log('Image uploaded successfully:', result.url);
      return result.url;
    } else {
      console.error('Image upload failed:', response.statusText);
      return null;
    }
  } catch (error) {
    console.error('Error uploading image:', error);
    return null;
  }
}

// In your component:
// const handleFileChange = (event) => {
//   const file = event.target.files[0];
//   uploadImageToR2(file);
// };
```

--------------------------------

### Cloudflare R2 S3 API: ListObjects Delimited Listing

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixes an issue with `ListObjects` where delimited listing of unicode-normalized keys could time out. This improves the reliability of listing objects with complex key names.

```http
GET /?delimiter=/&prefix=my-folder/ HTTP/1.1
Host: your-bucket.s3.amazonaws.com


```

--------------------------------

### API: Update GetBucket Endpoint

Source: https://developers.cloudflare.com/r2/platform/release-notes

Notes a change in the `GetBucket` API endpoint, which now fetches buckets by `bucket_name` instead of `bucket_id`. This simplifies bucket identification in API requests.

```bash
GET /buckets/:bucket_name
```

--------------------------------

### R2 Metadata Mapping to Workers Bindings

Source: https://developers.cloudflare.com/r2/llms-full

This table illustrates the mapping between HTTP headers and the corresponding `httpMetadata` property names within R2 bindings in Cloudflare Workers, facilitating programmatic access to metadata.

```JavaScript
httpMetadata.contentEncoding
httpMetadata.contentType
httpMetadata.contentLanguage
httpMetadata.contentDisposition
httpMetadata.cacheControl
httpMetadata.expires

```

--------------------------------

### Generate Presigned URLs with aws-sdk-php for R2

Source: https://developers.cloudflare.com/r2/examples/aws/aws-sdk-php

This snippet demonstrates how to generate presigned URLs using aws-sdk-php for R2. These URLs provide temporary read or write access to a bucket, with a specified expiration time.

```php
require 'vendor/autoload.php';

use Aws\S3\S3Client;
use Aws\S3\Exception\S3Exception;

$access_key_id = 'YOUR_ACCESS_KEY_ID';
$secret_access_key = 'YOUR_SECRET_ACCESS_KEY';
$account_id = 'YOUR_ACCOUNT_ID';
$bucket_name = 'YOUR_BUCKET_NAME';
$object_key = 'your-object-key'; // The key of the object to upload/download

$s3Client = new S3Client([
    'version' => 'latest',
    'region' => 'auto',
    'endpoint' => "https://{$account_id}.r2.cloudflarestorage.com",
    'credentials' => [
        'key'    => $access_key_id,
        'secret' => $secret_access_key,
    ],
]);

// Generate a presigned URL for PUT (upload)
try {
    $cmd_put = $s3Client->getCommand('PutObject', [
        'Bucket' => $bucket_name,
        'Key'    => $object_key
    ]);

    $request_put = $s3Client->createPresignedRequest($cmd_put, '+15 minutes');
    $presignedUrlPut = (string) $request_put->getUri();
    echo 'Presigned URL for PUT: ' . $presignedUrlPut . "\n";

} catch (S3Exception $e) {
    echo $e->getMessage();
}

// Generate a presigned URL for GET (download)
try {
    $cmd_get = $s3Client->getCommand('GetObject', [
        'Bucket' => $bucket_name,
        'Key'    => $object_key
    ]);

    $request_get = $s3Client->createPresignedRequest($cmd_get, '+15 minutes');
    $presignedUrlGet = (string) $request_get->getUri();
    echo 'Presigned URL for GET: ' . $presignedUrlGet . "\n";

} catch (S3Exception $e) {
    echo $e->getMessage();
}
```

--------------------------------

### Allow 0 Per-Page in S3 List Endpoints

Source: https://developers.cloudflare.com/r2/platform/release-notes

Enables the S3 API ListBuckets and ListObjects endpoints to accept a `per_page` value of 0, providing flexibility in pagination.

```S3 API
ListBuckets endpoint per_page=0
ListObjects endpoint per_page=0
```

--------------------------------

### S3 Presigned URL Support for List Operations

Source: https://developers.cloudflare.com/r2/llms-full

Fixed presigned URL support for S3 `ListBuckets` and `ListObjects` operations. This ensures that these listing operations can be correctly performed using presigned URLs.

```javascript
// No direct code example, this is a behavior change in the S3 API implementation.
```

--------------------------------

### Configure Wrangler for Queues and R2

Source: https://developers.cloudflare.com/r2/llms-full

This snippet shows how to configure your Cloudflare Worker's Wrangler settings to consume from a queue and bind to an R2 bucket. It includes the necessary JSONC and TOML configurations for queue consumers and R2 bucket bindings.

```jsonc
{
  "name": "event-notification-writer",
  "main": "src/index.ts",
  "compatibility_date": "2024-03-29",
  "compatibility_flags": [
    "nodejs_compat"
  ],
  "queues": {
    "consumers": [
      {
        "queue": "example-event-notification-queue",
        "max_batch_size": 100,
        "max_batch_timeout": 5
      }
    ]
  },
  "r2_buckets": [
    {
      "binding": "LOG_SINK",
      "bucket_name": "example-log-sink-bucket"
    }
  ]
}
```

```toml
name = "event-notification-writer"
main = "src/index.ts"
compatibility_date = "2024-03-29"
compatibility_flags = ["nodejs_compat"]


[[queues.consumers]]
queue = "example-event-notification-queue"
max_batch_size = 100
max_batch_timeout = 5


[[r2_buckets]]
binding = "LOG_SINK"
bucket_name = "example-log-sink-bucket"
```

--------------------------------

### Enable Public Development URL for R2 Bucket

Source: https://developers.cloudflare.com/r2/llms-full

Steps to enable a Cloudflare-managed r2.dev subdomain for public access to an R2 bucket's contents. This is intended for non-production traffic and requires confirmation.

```bash
Step 1: In R2, select the bucket.
Step 2: Go to Settings.
Step 3: Under Public Development URL, select Enable.
Step 4: Type 'allow' in Allow Public Access? and select Allow.
Step 5: Verify Public URL Access shows Allowed.
```

--------------------------------

### S3 Presigned URL Support for List Operations

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixed presigned URL support for S3 ListBuckets and ListObjects operations, enabling secure access to bucket and object listings via presigned URLs.

```text
Fixed presigned URL support for the S3 `ListBuckets` and `ListObjects` operations.
```

--------------------------------

### Authenticate and Fetch Object with R2 API (JavaScript)

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to authenticate against the Cloudflare R2 API using the AWS SDK for JavaScript. It configures the S3 client with R2-specific endpoints and credentials, then fetches an object from a specified bucket.

```javascript
const AWS = require('aws-sdk');
const crypto = require('crypto');


const ACCOUNT_ID = process.env.R2_ACCOUNT_ID;
const ACCESS_KEY_ID = process.env.R2_ACCESS_KEY_ID;
const SECRET_ACCESS_KEY = process.env.R2_SECRET_ACCESS_KEY;
const BUCKET_NAME = process.env.R2_BUCKET_NAME;


// Hash the secret access key
const hashedSecretKey = crypto.createHash('sha256').update(SECRET_ACCESS_KEY).digest('hex');


// Configure the S3 client for Cloudflare R2
const s3Client = new AWS.S3({
    endpoint: `https://${ACCOUNT_ID}.r2.cloudflarestorage.com`,
    accessKeyId: ACCESS_KEY_ID,
    secretAccessKey: hashedSecretKey,
    signatureVersion: 'v4',
    region: 'auto' // Cloudflare R2 doesn't use regions, but this is required by the SDK
});


// Specify the object key
const objectKey = '2024/08/02/ingested_0001.parquet';


// Function to fetch the object
async function fetchObject() {
    try {
        const params = {
            Bucket: BUCKET_NAME,
            Key: objectKey
        };


        const data = await s3Client.getObject(params).promise();
        console.log('Successfully fetched the object');


        // Process the data as needed
        // For example, to get the content as a Buffer:
        // const content = data.Body;


        // Or to save the file (requires 'fs' module):
        // const fs = require('fs').promises;
        // await fs.writeFile('ingested_0001.parquet', data.Body);


    } catch (error) {
        console.error('Failed to fetch the object:', error);
    }
}


fetchObject();
```

--------------------------------

### S3 XML Compatibility Fix for Arq Backup

Source: https://developers.cloudflare.com/r2/platform/release-notes

Minor S3 XML compatibility fix for Arq Backup on Windows, ensuring response contains XML declaration tag prefix and xmlns attribute on all top-level tags.

```text
Minor S3 XML compatibility fix impacting Arq Backup on Windows only (not the Mac version). Response now contains XML declaration tag prefix and the xmlns attribute is present on all top-level tags in the response.
```

--------------------------------

### Wrangler: Manage R2 Bucket r2.dev URL

Source: https://developers.cloudflare.com/r2/platform/release-notes

Explains the `bucket dev-url` command in Wrangler for managing the public access URL for R2 buckets (`r2.dev`). It supports enabling, disabling, and checking the status of this feature.

```bash
wrangler r2 bucket dev-url enable <bucket-name>
```

```bash
wrangler r2 bucket dev-url disable <bucket-name>
```

```bash
wrangler r2 bucket dev-url status <bucket-name>
```

--------------------------------

### R2 Worker Bindings: Multipart Upload Support

Source: https://developers.cloudflare.com/r2/llms-full

Adds support for multipart uploads within R2 worker bindings, enabling more robust handling of large file uploads through a series of smaller parts.

```JavaScript
export default {
	async fetch(request, env) {
		// Example: Uploading a part of a multipart upload
		// const uploadId = "...";
		// const partNumber = 1;
		// const partData = await request.arrayBuffer();
		// await env.MY_BUCKET.uploadPart(uploadId, partNumber, partData);

		// Example: Completing a multipart upload
		// const uploadId = "...";
		// const parts = [
		// 	{ PartNumber: 1, ETag: "..." },
		// 	{ PartNumber: 2, ETag: "..." }
		// ];
		// await env.MY_BUCKET.completeMultipartUpload(uploadId, parts);

		return new Response("Multipart upload operations handled.");
	}
}
```

--------------------------------

### Retrieve Object from Cloudflare R2 (C#)

Source: https://developers.cloudflare.com/r2/llms-full

Downloads an object from a Cloudflare R2 bucket using the AWS SDK for .NET. Requires the bucket name and the key (file name) of the object to retrieve.

```csharp
static async Task GetObject()
{
  var bucket = "sdk-example";
  var key = "file.txt"


  var response = await s3Client.GetObjectAsync(bucket, key);


  Console.WriteLine("ETag: {0}", response.ETag);
}
```

--------------------------------

### R2 List Continuation Tokens

Source: https://developers.cloudflare.com/r2/llms-full

List continuation tokens issued prior to 2022-07-01 are no longer accepted. Users must obtain new tokens by performing a new `list` operation.

```javascript
// No direct code example, this is a behavior change in the R2 API.
```

--------------------------------

### Cloudflare R2 S3 API: Public Buckets Content-Encoding

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixes a bug where public buckets would not return the `Content-Encoding` header for gzip files when `Accept-Encoding: gzip` was used. This ensures correct content negotiation.

```http
GET /my-gzipped-file.txt HTTP/1.1
Host: your-bucket.s3.amazonaws.com
Accept-Encoding: gzip

HTTP/1.1 200 OK
Content-Encoding: gzip
...
```

--------------------------------

### Configure Boto3 for Cloudflare R2

Source: https://developers.cloudflare.com/r2/examples/aws/boto3

This snippet shows how to configure the Boto3 SDK to connect to Cloudflare R2. It emphasizes setting the `endpoint_url` and optionally using environment variables for credentials.

```Python
import boto3

r2_client = boto3.client(
    "s3",
    endpoint_url="https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com",
    aws_access_key_id="YOUR_ACCESS_KEY_ID",
    aws_secret_access_key="YOUR_SECRET_ACCESS_KEY"
)
```

--------------------------------

### Generate Presigned Upload URL with Java SDK

Source: https://developers.cloudflare.com/r2/llms-full

Generates a presigned URL for uploading objects to Cloudflare R2 using the AWS SDK for Java. This allows temporary public write access to a bucket.

```java
import software.amazon.awssdk.auth.credentials.AwsBasicCredentials;
import software.amazon.awssdk.auth.credentials.StaticCredentialsProvider;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.services.s3.model.S3Configuration;
import software.amazon.awssdk.services.s3.presigner.S3Presigner;
import software.amazon.awssdk.services.s3.presigner.model.PutObjectPresignRequest;
import software.amazon.awssdk.services.s3.presigner.model.PresignedPutObjectRequest;
import java.net.URI;
import java.time.Duration;

public class CloudflareR2Client {
    private final S3Client s3Client;
    private final S3Presigner presigner;

    /**
     * Creates a new CloudflareR2Client with the provided configuration
     */
    public CloudflareR2Client(S3Config config) {
        this.s3Client = buildS3Client(config);
        this.presigner = buildS3Presigner(config);
    }

    /**
     * Builds and configures the S3 presigner with R2-specific settings
     */
    private static S3Presigner buildS3Presigner(S3Config config) {
        AwsBasicCredentials credentials = AwsBasicCredentials.create(
            config.getAccessKey(),
            config.getSecretKey());

        return S3Presigner.builder()
            .endpointOverride(URI.create(config.getEndpoint()))
            .credentialsProvider(StaticCredentialsProvider.create(credentials))
            .region(Region.of("auto"))
            .serviceConfiguration(S3Configuration.builder()
                .pathStyleAccessEnabled(true)
                .build())
            .build();
    }

    public String generatePresignedUploadUrl(String bucketName, String objectKey, Duration expiration) {
        PutObjectPresignRequest presignRequest = PutObjectPresignRequest.builder()
            .signatureDuration(expiration)
            .putObjectRequest(builder -> builder
                .bucket(bucketName)
                .key(objectKey)
                .build())
            .build();

        PresignedPutObjectRequest presignedRequest = presigner.presignPutObject(presignRequest);
        return presignedRequest.url().toString();
    }

    // Rest of the methods remains the same

    public static void main(String[] args) {
      // config the client as before

      // Generate a pre-signed upload URL valid for 15 minutes
        String uploadUrl = r2Client.generatePresignedUploadUrl(
            "demos",
            "README.md",
            Duration.ofMinutes(15)
        );
        System.out.println("Pre-signed Upload URL (valid for 15 minutes):");
        System.out.println(uploadUrl);
    }

    // Dummy S3Client and S3Config for compilation
    private S3Client buildS3Client(S3Config config) {
        return null; // Placeholder
    }

    // Dummy S3Config class
    static class S3Config {
        private String accessKey;
        private String secretKey;
        private String endpoint;

        public String getAccessKey() { return accessKey; }
        public String getSecretKey() { return secretKey; }
        public String getEndpoint() { return endpoint; }
    }

    // Dummy r2Client instance for main method
    private static CloudflareR2Client r2Client;
}
```

--------------------------------

### Implement PutObject with System Metadata, Storage Class, and SSE-C

Source: https://developers.cloudflare.com/r2/llms-full

The PutObject operation in Cloudflare R2 supports setting system metadata like Content-Type and Cache-Control, specifying storage classes (STANDARD, STANDARD_IA), and utilizing SSE-C for encryption. It also supports various SSE-C related headers.

```S3 API
PutObject:
  System Metadata:
    Content-Type: ✅
    Cache-Control: ✅
    Content-Disposition: ✅
    Content-Encoding: ✅
    Content-Language: ✅
    Expires: ✅
    Content-MD5: ✅
  Storage Class:
    x-amz-storage-class: ✅
    STANDARD: ✅
    STANDARD_IA: ✅
  SSE-C:
    x-amz-server-side-encryption-customer-algorithm: ✅
    x-amz-server-side-encryption-customer-key: ✅
    x-amz-server-side-encryption-customer-key-MD5: ✅
```

--------------------------------

### S3 Error Response XML Compatibility

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixed an S3 compatibility issue for error responses with MinIO .NET SDK, ensuring no xmlns namespace attribute on the top-level Error tag for tools expecting this.

```text
Fixed an S3 compatibility issue for error responses with MinIO .NET SDK and any other tooling that expects no `xmlns` namespace attribute on the top-level `Error` tag.
```

--------------------------------

### S3 API: PutBucketCors Rejection of Unknown Keys

Source: https://developers.cloudflare.com/r2/llms-full

Ensures that the `PutBucketCors` method in the S3 compatible API for R2 rejects requests containing unknown keys within the XML body, improving request validation.

```XML
<CORSConfiguration>
  <CORSRule>
    <AllowedOrigin>*</AllowedOrigin>
    <AllowedMethod>GET</AllowedMethod>
    <UnknownKey>InvalidValue</UnknownKey> <!-- This key should cause rejection -->
  </CORSRule>
</CORSConfiguration>
```

--------------------------------

### Fix S3 ListObjectsV2 Common Prefixes and KeyCount

Source: https://developers.cloudflare.com/r2/platform/release-notes

Addresses issues with the S3 API ListObjectsV2 operation where empty folders were not returned as common prefixes and the KeyCount parameter was inaccurate. This ensures correct representation of folder structures and object counts.

```S3 API
ListObjectsV2 operation with delimiters
KeyCount parameter
```

--------------------------------

### Cloudflare R2 Worker Bindings: Multipart Uploads Support

Source: https://developers.cloudflare.com/r2/platform/release-notes

Adds worker bindings support for multipart uploads, enabling clients to upload large objects in parts. This includes functionality for `UploadPartCopy`.

```javascript
// Conceptual example for multipart upload operations
const uploadId = await env.MY_BUCKET.createMultipartUpload('large-file.zip');
await env.MY_BUCKET.uploadPart(uploadId, 1, part1Data);
await env.MY_BUCKET.uploadPart(uploadId, 2, part2Data);
await env.MY_BUCKET.completeMultipartUpload(uploadId, ['part1-etag', 'part2-etag']);

```

--------------------------------

### Implement ListMultipartUploads with Query Parameters

Source: https://developers.cloudflare.com/r2/llms-full

The ListMultipartUploads operation in Cloudflare R2 supports several query parameters for managing multipart uploads, including delimiter, encoding-type, key-marker, max-uploads, prefix, and upload-id-marker.

```S3 API
ListMultipartUploads:
  Query Parameters:
    delimiter: ✅
    encoding-type: ✅
    key-marker: ✅
    max-uploads: ✅
    prefix: ✅
    upload-id-marker: ✅
```

--------------------------------

### Configure R2 Bucket Jurisdiction in Wrangler (TOML)

Source: https://developers.cloudflare.com/r2/llms-full

This snippet demonstrates how to configure an R2 bucket binding with a specified jurisdiction using a `wrangler.toml` file for Cloudflare Workers.

```toml
[[r2_buckets]]
bindings = [
  { binding = "MY_BUCKET", bucket_name = "<YOUR_BUCKET_NAME>", jurisdiction = "<JURISDICTION>" }
]
```

--------------------------------

### Create Temporary R2 Access Tokens API

Source: https://developers.cloudflare.com/r2/llms-full

Generate temporary credentials for accessing R2 buckets via the Cloudflare API. This is useful for time-limited access or secure sharing.

```bash
curl -X POST https://api.cloudflare.com/client/v4/accounts/:account_id/r2/temporary_credentials -d '{"name": "my-token", "duration_seconds": 3600}'
```

--------------------------------

### Add Summary to R2 Bucket

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

TypeScript code snippet showing how to upload the generated summary as a new text file to the R2 bucket.

```typescript
// ... after generating the summary ...
await env.R2_BUCKET.put(`${key}-summary.txt`, summary);
```

--------------------------------

### List R2 Bucket Lock Rules with Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

This command uses Wrangler to list the bucket lock rules configured for a specific Cloudflare R2 bucket. It requires the bucket name as an argument.

```sh
npx wrangler r2 bucket lock list <BUCKET_NAME>
```

--------------------------------

### Cloudflare R2 S3 API: GetBucket Availability

Source: https://developers.cloudflare.com/r2/platform/release-notes

Announces the availability of the `GetBucket` operation through the Cloudflare API, allowing users to retrieve information about their R2 buckets.

```http
GET /my-bucket HTTP/1.1
Host: api.cloudflare.com


```

--------------------------------

### List R2 Buckets using AWS CLI

Source: https://developers.cloudflare.com/r2/llms-full

List all buckets in your Cloudflare R2 storage using the AWS CLI. This command requires the endpoint URL specific to your R2 account.

```shell
aws s3api list-buckets --endpoint-url https://<accountid>.r2.cloudflarestorage.com
```

--------------------------------

### List R2 Bucket Custom Domains API

Source: https://developers.cloudflare.com/r2/llms-full

Retrieve a list of custom domains configured for an R2 bucket via the Cloudflare API. This includes details such as the minimum TLS version supported.

```bash
curl https://api.cloudflare.com/client/v4/accounts/:account_id/r2/buckets/:bucket_name/domains/custom
```

--------------------------------

### R2 List Limit Handling

Source: https://developers.cloudflare.com/r2/llms-full

The `list()` binding will now correctly return a smaller limit if too much data would otherwise be returned, preventing `Internal Error` responses.

```javascript
await env.BUCKET.list({ limit: 1000 }); // If 1000 is too many, a smaller limit will be returned.
```

--------------------------------

### Configure R2 Bucket Binding in Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

This snippet shows how to configure an R2 bucket binding in Wrangler, which is necessary for uploading files to Cloudflare R2. It provides configurations for both `wrangler.jsonc` and `wrangler.toml` files.

```toml
[[r2_buckets]]
binding = "MY_BUCKET"
bucket_name = "<R2_BUCKET_NAME>"
```

--------------------------------

### Return Location Header in S3 CreateBucket

Source: https://developers.cloudflare.com/r2/platform/release-notes

Ensures that the S3 API CreateBucket operation now returns the proper `location` response header, providing information about the bucket's location.

```S3 API
CreateBucket operation location response header
```

--------------------------------

### Upload to Cloudflare R2 using Presigned URL

Source: https://developers.cloudflare.com/r2/llms-full

Shows how to upload a file to a Cloudflare R2 bucket using a presigned URL generated for the 'putObject' operation. The upload is valid until the presigned link expires.

```bash
curl -X PUT https://my-bucket-name.<accountid>.r2.cloudflarestorage.com/dog.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=<credential>&X-Amz-Date=<timestamp>&X-Amz-Expires=3600&X-Amz-Signature=<signature>&X-Amz-SignedHeaders=host --data-binary @dog.png
```

--------------------------------

### ListParts - Cloudflare R2

Source: https://developers.cloudflare.com/r2/llms-full

Lists parts of a multipart upload. Supports query parameters like max-parts and part-number-marker. Does not support request payer or bucket owner.

```bash
aws s3api list-parts --bucket BUCKET --key KEY --upload-id UPLOAD_ID --max-parts 1000 --part-number-marker 5
```

--------------------------------

### Bind R2 Bucket to Worker (wrangler.jsonc)

Source: https://developers.cloudflare.com/r2/llms-full

Configuration in `wrangler.jsonc` to bind an R2 bucket to a Worker. Specifies the binding name (a JavaScript variable) and the bucket name.

```json
{
  "r2_buckets": [
    {
      "binding": "MY_BUCKET",
      "bucket_name": "<YOUR_BUCKET_NAME>"
    }
  ]
}
```

--------------------------------

### UploadPartCopy with SSE-C and Range

Source: https://developers.cloudflare.com/r2/api/s3/api

This snippet outlines the headers and parameters for the UploadPartCopy operation in Cloudflare R2, including support for SSE-C, copying source objects with SSE-C, and specifying a byte range for the copy.

```api-headers-query-parameters
x-amz-copy-source
x-amz-copy-source-server-side-encryption-customer-algorithm
x-amz-copy-source-server-side-encryption-customer-key
x-amz-copy-source-server-side-encryption-customer-key-MD5
x-amz-server-side-encryption-customer-algorithm
x-amz-server-side-encryption-customer-key
x-amz-server-side-encryption-customer-key-MD5
x-amz-copy-source-range
```

--------------------------------

### Enable/Disable R2 Bucket dev-url with Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Manage the public access URL for R2 buckets using the Wrangler CLI. This command allows enabling, disabling, and checking the status of the r2.dev public URL.

```bash
wrangler r2 bucket dev-url enable <bucket-name>
wrangler r2 bucket dev-url disable <bucket-name>
wrangler r2 bucket dev-url status <bucket-name>
```

--------------------------------

### Create R2 Binding in Wrangler

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Configure your Wrangler file (JSONC or TOML) to bind an R2 bucket to your Worker. Update the `binding` and `bucket_name` properties.

```json
{
  "version": "1",
  "name": "my-worker",
  "main": "src/index.js",
  "compatibility_flags": [
    "nodejs_compat"
  ],
  "env": {
    "production": {
      "routes": [
        {
          "pattern": "my-worker.example.com/",
          "custom_domain": true
        }
      ]
    }
  },
  "routes": [
    {
      "pattern": "workers.example.com",
      "script": "my-worker"
    }
  ],
  "triggers": {
    "crons": [
      "0 * * * *" 
    ]
  },
  "kv_namespaces": [
    {
      "binding": "MY_KV",
      "id": "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
    }
  ],
  "r2_buckets": [
    {
      "binding": "MY_BUCKET",
      "bucket_name": "my-bucket-name"
    }
  ]
}
```

```toml
[env.production]
routes = [
  {
    pattern = "my-worker.example.com/",
    custom_domain = true
  }
]

[[routes]]
pattern = "workers.example.com"
script = "my-worker"

[[triggers]]
crons = ["0 * * * *"]

[[kv_namespaces]]
binding = "MY_KV"
id = "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

[[r2_buckets]]
binding = "MY_BUCKET"
bucket_name = "my-bucket-name"

[compatibility_flags]

nodejs_compat = true

```

--------------------------------

### Create an R2 Bucket using Postman

Source: https://developers.cloudflare.com/r2/tutorials/postman

Use the `PUT` CreateBucket request in Postman to create a new bucket in Cloudflare R2. Ensure the `r2-bucket` variable is set to your desired bucket name before sending the request.

```Postman
Set the `r2-bucket` variable to your desired bucket name.
Navigate to **Cloudflare R2** > **Buckets** > **`PUT`CreateBucket** and select **Send**.
```

--------------------------------

### Upload Object using Presigned URL (Shell)

Source: https://developers.cloudflare.com/r2/llms-full

Uploads an object to a Cloudflare R2 bucket using a previously generated presigned URL for write access. This command uses `curl` and requires the presigned URL obtained from a `putObject` operation.

```sh
curl -X PUT https://sdk-example.<accountid>.r2.cloudflarestorage.com/ferriswasm.png?X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=<credential>&X-Amz-Date=<timestamp>&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Signature=<signature> --data-binary @ferriswasm.png
```

--------------------------------

### Configure Workers AI Binding in Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Sets up the Workers AI binding in the Wrangler configuration file ('wrangler.jsonc' or 'wrangler.toml') to enable AI functionalities.

```json
{
  "ai": {
    "binding": "AI"
  }
}
```

```toml
[ai]
binding = "AI"
```

--------------------------------

### Upload using Presigned URL with cURL

Source: https://developers.cloudflare.com/r2/llms-full

Shows how to use a presigned URL generated for a `putObject` operation to upload a file to a Cloudflare R2 bucket using `curl`. The URL includes all necessary parameters for authentication and expiration.

```sh
curl -X PUT https://my-bucket-name.<accountid>.r2.cloudflarestorage.com/dog.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential<credential>&X-Amz-Date=<timestamp>&X-Amz-Expires=3600&X-Amz-Signature=<signature>&X-Amz-SignedHeaders=host&x-id=PutObject -F "data=@dog.png"
```

--------------------------------

### Set lifecycle configuration from JSON using Wrangler

Source: https://developers.cloudflare.com/r2/buckets/object-lifecycles

This command sets the entire lifecycle configuration for an R2 bucket from a JSON file using the Wrangler CLI. The JSON file must match the format of the S3 API put object lifecycle configuration request body.

```bash
wrangler r2 bucket lifecycle set <bucket-name> --config-file=lifecycle.json
```

--------------------------------

### Create R2 Bucket Custom Domain API

Source: https://developers.cloudflare.com/r2/llms-full

Add a new custom domain to an R2 bucket using the Cloudflare API. This allows using your own domain name for accessing bucket contents.

```bash
curl -X POST https://api.cloudflare.com/client/v4/accounts/:account_id/r2/buckets/:bucket_name/domains/custom -d '{"domain": "your.custom.domain"}'
```

--------------------------------

### API: Conditional Multipart Publish Operations

Source: https://developers.cloudflare.com/r2/platform/release-notes

Introduces the ability to complete conditional multipart publish operations. If a condition fails during publishing, the upload is now treated as aborted and is no longer available.

```json
{
  "CompleteMultipartUpload": {
    "Parts": [
      {
        "PartNumber": 1,
        "ETag": "..."
      }
    ]
  },
  "Condition": {
    "IfMatch": "..."
  }
}
```

--------------------------------

### R2MultipartUpload Complete Upload

Source: https://developers.cloudflare.com/r2/llms-full

The `complete` method finalizes a multipart upload by providing a list of `R2UploadedPart` objects. Upon successful completion, the object becomes globally accessible.

```javascript
async function completeMultipartUpload(multipartUpload, uploadedParts) {
  const completedObject = await multipartUpload.complete(uploadedParts);
  return completedObject;
}
```

--------------------------------

### Upload Object to R2 with AWS SDK for Rust

Source: https://developers.cloudflare.com/r2/llms-full

Provides a function to upload an object to a Cloudflare R2 bucket using the AWS SDK for Rust. It takes the client, bucket name, key, and local file path as input, and streams the file content as the object body.

```rust
use aws_sdk_s3::primitives::ByteStream;
use std::path::Path;


async fn upload_object(
    client: &s3::Client,
    bucket: &str,
    key: &str,
    file_path: &str,
) -> Result<(), s3::Error> {
    let body = ByteStream::from_path(Path::new(file_path)).await.unwrap();


    client
        .put_object()
        .bucket(bucket)
        .key(key)
        .body(body)
        .send()
        .await?;


    println!("Uploaded {} to {}/{}", file_path, bucket, key);
    Ok(())
}

```

--------------------------------

### Cloudflare R2 S3 API: Ranged Reads for Multipart Objects

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixes an issue with ranged reads for multipart objects where part sizes were unaligned to 64KiB. This ensures that partial reads work correctly regardless of part alignment.

```http
GET /my-multipart-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com
Range: bytes=0-1023


```

--------------------------------

### S3 Error Response XML Compatibility

Source: https://developers.cloudflare.com/r2/llms-full

Fixed an S3 compatibility issue for error responses with the MinIO .NET SDK and other tools expecting no `xmlns` namespace attribute on the top-level `Error` tag.

```xml
<Error>
  <Code>NoSuchBucket</Code>
  <Message>The specified bucket does not exist</Message>
  <Key>my-object</Key>
</Error>
```

--------------------------------

### ListBuckets Response Headers

Source: https://developers.cloudflare.com/r2/api/s3/extensions

Explains the response headers returned by the ListBuckets operation, including `cf-is-truncated`, `cf-next-continuation-token`, and `cf-is-truncated`. These headers provide information about the completeness of the bucket list and continuation tokens for paginated results.

```json
HTTP/1.1 200 OK
Content-Type: application/xml
...
cf-is-truncated: true
cf-next-continuation-token: next-token-value
...
```

--------------------------------

### Tail Worker Logs

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Command to view real-time logs from the deployed Cloudflare Worker, useful for debugging and monitoring.

```bash
wrangler tail
```

--------------------------------

### Filter R2 Bucket Notification by Prefix and Suffix using Wrangler CLI

Source: https://developers.cloudflare.com/r2/llms-full

Command to create an R2 bucket notification rule with both prefix and suffix filtering using the Wrangler CLI. This enables notifications for objects that match both criteria.

```sh
# Filter using prefix and suffix. Both the conditions will be used for filtering
$ npx wrangler r2 bucket notification create <BUCKET_NAME> --event-type <EVENT_TYPE> --queue <QUEUE_NAME> --prefix "<PREFIX_VALUE>" --suffix "<SUFFIX_VALUE>"
```

--------------------------------

### Configure Queue Consumer Binding in Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

This snippet shows how to configure a queue consumer binding in Wrangler, specifying the queue name ('pdf-summarizer') that the worker will listen to for event notifications. Configurations are provided for both `wrangler.jsonc` and `wrangler.toml`.

```toml
[[queues.consumers]]
queue = "pdf-summarizer"
```

--------------------------------

### Create R2 Bucket Notification via Wrangler CLI

Source: https://developers.cloudflare.com/r2/llms-full

Command to create an event notification rule for an R2 bucket using the Wrangler CLI. It specifies the bucket name, event type, and the target queue.

```sh
npx wrangler r2 bucket notification create <BUCKET_NAME> --event-type <EVENT_TYPE> --queue <QUEUE_NAME>
```

--------------------------------

### R2Object writeHttpMetadata Method

Source: https://developers.cloudflare.com/r2/llms-full

The `writeHttpMetadata` method on an `R2Object` takes a `Headers` object and applies the associated HTTP metadata from the `R2Object` to it. This is useful for setting response headers based on object metadata.

```javascript
async function writeMetadata(r2Object, headers) {
  await r2Object.writeHttpMetadata(headers);
}
```

--------------------------------

### Set R2 Bucket Lock Configuration via Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Illustrates how to set the entire bucket lock configuration for an R2 bucket using a JSON file with the Wrangler CLI. The JSON file format should match the R2 API's put bucket lock configuration.

```sh
npx wrangler r2 bucket lock set <BUCKET_NAME> --file <FILE_PATH>
```

--------------------------------

### Configure Cloudflare R2 with Terraform

Source: https://developers.cloudflare.com/r2/examples/terraform

This snippet shows the basic Terraform configuration for Cloudflare R2 using the Cloudflare provider. It assumes you have `access_key_id` and `access_key_secret` variables defined, which should be populated with your generated Access Key ID and Secret Access Key.

```Terraform
provider "cloudflare" {
  account_id = "YOUR_ACCOUNT_ID"
  api_token  = var.cloudflare_api_token
}

resource "cloudflare_r2_bucket" "my_bucket" {
  account_id = "YOUR_ACCOUNT_ID"
  name       = "my-r2-bucket"
  public     = false
}
```

--------------------------------

### Login to Wrangler CLI

Source: https://developers.cloudflare.com/r2/llms-full

Logs the user into the Wrangler CLI, which is used for managing Cloudflare Workers and R2 resources. This command is a prerequisite for other Wrangler operations.

```plaintext
npx wrangler login
```

--------------------------------

### Configure R2 Bucket Binding in Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

This snippet shows how to configure an R2 bucket binding in Wrangler, which is necessary for uploading files to Cloudflare R2. It provides configurations for both `wrangler.jsonc` and `wrangler.toml` files.

```json
{
  "r2_buckets": [
    {
      "binding": "MY_BUCKET",
      "bucket_name": "<R2_BUCKET_NAME>"
    }
  ]
}
```

--------------------------------

### UploadPartCopy - Cloudflare R2

Source: https://developers.cloudflare.com/r2/llms-full

Copies a part of an object to another location. Supports range, SSE-C, and copy source SSE-C. Does not support conditional operations, request payer, or bucket owner.

```bash
aws s3api upload-part-copy --bucket DEST_BUCKET --key DEST_KEY --part-number 1 --copy-source SOURCE_BUCKET/SOURCE_KEY --copy-source-range bytes=0-1023 --sse-customer-algorithm AES256 --sse-customer-key YOUR_KEY --sse-customer-key-md5 YOUR_KEY_MD5 --copy-source-sse-customer-algorithm AES256 --copy-source-sse-customer-key SOURCE_KEY --copy-source-sse-customer-key-md5 SOURCE_KEY_MD5
```

--------------------------------

### Cloudflare R2 S3 API: CopyObject Conditional Headers

Source: https://developers.cloudflare.com/r2/platform/release-notes

Introduces support for Cloudflare-specific headers in the S3 compatible `CopyObject` API. These headers enable conditional copying based on the state of the destination object.

```http
PUT /destination-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com
CopySource: /your-bucket/source-object
If-Match: "expected-etag"


```

--------------------------------

### Upload to Cloudflare R2 using Presigned URL (Shell)

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to upload a file to a Cloudflare R2 bucket using a presigned URL generated for a PUT operation. This command uses `curl` to perform the HTTP PUT request with the file data.

```Shell
curl -X PUT "https://<accountid>.r2.cloudflarestorage.com/my-bucket-name/dog.png?X-Amz-Expires=3600&X-Amz-Date=<timestamp>&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=<credential>&X-Amz-SignedHeaders=host&X-Amz-Signature=<signature>" -F "data=@dog.png"
```

--------------------------------

### R2PutOptions Configuration

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Details the configuration options for `R2PutOptions`, including conditional uploads (`onlyIf`), HTTP metadata, custom metadata, integrity hashes (MD5, SHA-1, SHA-256, SHA-384, SHA-512), storage class, and SSE-C keys.

```javascript
{
  onlyIf: R2Conditional | Headers,
  httpMetadata: R2HTTPMetadata | Headers,
  customMetadata: Record<string, string>,
  md5: ArrayBuffer | string,
  sha1: ArrayBuffer | string,
  sha256: ArrayBuffer | string,
  sha384: ArrayBuffer | string,
  sha512: ArrayBuffer | string,
  storageClass: 'Standard' | 'InfrequentAccess',
  ssecKey: ArrayBuffer | string
}
```

--------------------------------

### Generate AI Type Definitions

Source: https://developers.cloudflare.com/r2/llms-full

Executes a command to generate TypeScript type definitions for Cloudflare Workers AI, ensuring type safety when using AI bindings.

```sh
npm run cf-typegen
```

--------------------------------

### Deploy Worker

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-usage

This command is used to deploy your Cloudflare Worker application to Cloudflare's global network. It packages your code and configuration for deployment.

```bash
npx wrangler deploy

```

--------------------------------

### Generate Presigned URLs for Cloudflare R2 (TypeScript)

Source: https://developers.cloudflare.com/r2/llms-full

Generates presigned URLs for temporary read or write access to Cloudflare R2 buckets. It utilizes the aws4fetch library to sign requests, allowing for time-limited access to specific objects. The expiration time is controlled via the `X-Amz-Expires` query parameter.

```TypeScript
import { AwsClient } from "aws4fetch";


const client = new AwsClient({
  service: "s3",
  region: "auto",
  accessKeyId: ACCESS_KEY_ID,
  secretAccessKey: SECRET_ACCESS_KEY,
});


const R2_URL = `https://${ACCOUNT_ID}.r2.cloudflarestorage.com`;


// Use the `X-Amz-Expires` query param to determine how long the presigned link is valid.
console.log(
  (
    await client.sign(
      new Request(`${R2_URL}/my-bucket-name/dog.png?X-Amz-Expires=${3600}`),
      {
        aws: { signQuery: true },
      },
    )
  ).url.toString(),
);
// https://<accountid>.r2.cloudflarestorage.com/my-bucket-name/dog.png?X-Amz-Expires=3600&X-Amz-Date=<timestamp>&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=<credential>&X-Amz-SignedHeaders=host&X-Amz-Signature=<signature>


// You can also create links for operations such as PutObject to allow temporary write access to a specific key.
console.log(
  (
    await client.sign(
      new Request(`${R2_URL}/my-bucket-name/dog.png?X-Amz-Expires=${3600}`, {
        method: "PUT",
      }),
      {
        aws: { signQuery: true },
      },
    )
  ).url.toString(),
);

```

--------------------------------

### Set R2 Temporary Credentials

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to set environment variables for AWS access keys and session tokens to use temporary R2 credentials for S3-compatible requests.

```plaintext
AWS_ACCESS_KEY_ID = <accessKeyId>
AWS_SECRET_ACCESS_KEY = <secretAccessKey>
AWS_SESSION_TOKEN = <sessionToken>
```

--------------------------------

### Enable R2 Bucket Lock via API

Source: https://developers.cloudflare.com/r2/llms-full

This snippet demonstrates how to enable a bucket lock configuration for Cloudflare R2 using the API. It includes setting up rules for object retention based on age or indefinite locking. The request requires an API token and specifies the bucket name and account ID.

```bash
curl -X PUT "https://api.cloudflare.com/client/v4/accounts/<ACCOUNT_ID>/r2/buckets/<BUCKET_NAME>/lock" \
    -H "Authorization: Bearer <API_TOKEN>" \
    -H "Content-Type: application/json" \
    -d '{
        "rules": [
            {
                "id": "lock-logs-7d",
                "enabled": true,
                "prefix": "logs/",
                "condition": {
                    "type": "Age",
                    "maxAgeSeconds": 604800
                }
            },
            {
                "id": "lock-images-indefinite",
                "enabled": true,
                "prefix": "images/",
                "condition": {
                    "type": "Indefinite"
                }
            }
        ]
    }'
```

--------------------------------

### CopyObject with Destination Conditions

Source: https://developers.cloudflare.com/r2/api/s3/extensions

Illustrates the use of R2-specific headers for conditional `CopyObject` operations based on the destination object's state. Headers like `cf-copy-destination-if-match` and `cf-copy-destination-if-none-match` allow for conditional writes, failing with a 412 error if conditions are not met.

```bash
PUT /destination-object HTTP/1.1
Host: example.com
CopySource: /source-bucket/source-object
x-amz-copy-source-if-match: source-etag
cf-copy-destination-if-none-match: destination-etag
```

--------------------------------

### API: Fix OPTIONS Request Origin Header

Source: https://developers.cloudflare.com/r2/platform/release-notes

Corrects behavior for `OPTIONS` requests to the public entrypoint. When the `origin` header is missing, the API now returns an `HTTP 400` instead of an `HTTP 401`.

```http
OPTIONS /public/object HTTP/1.1
Host: your-bucket.r2.dev

```

--------------------------------

### Authenticate R2 API with S3 API using Python

Source: https://developers.cloudflare.com/r2/examples/authenticate-r2-auth-tokens

This snippet demonstrates how to authenticate against Cloudflare R2 using the S3 API and an API token in Python. It requires the boto3 library and proper environmental variable configuration.

```python
import boto3
import os

# Ensure AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_REGION are set as environment variables
# For R2, the region should be the name of your R2 bucket.
s3_client = boto3.client(
    "s3",
    endpoint_url=f"https://{os.environ.get('CLOUDFLARE_ACCOUNT_ID')}.r2.cloudflarestorage.com",
    aws_access_key_id=os.environ.get("ACCESS_KEY_ID"),
    aws_secret_access_key=os.environ.get("SECRET_ACCESS_KEY"),
    region_name=os.environ.get("AWS_REGION", "us-east-1") # Default region if not specified
)

def get_r2_object(bucket_name, object_key):
    try:
        response = s3_client.get_object(Bucket=bucket_name, Key=object_key)
        object_data = response['Body'].read()
        print(f"Successfully retrieved object: {object_key}")
        # Process object_data (e.g., print, save to file)
        # print(object_data.decode('utf-8'))
    except Exception as e:
        print(f"Error retrieving object {object_key}: {e}")

# Example usage:
# bucket_name = "YOUR_R2_BUCKET_NAME"
# object_key = "your-object-key"
# get_r2_object(bucket_name, object_key)

```

--------------------------------

### R2PutOptions: Configure object storage with metadata and integrity checks

Source: https://developers.cloudflare.com/r2/llms-full

Set options for R2 PUT requests, including conditional storage, HTTP and custom metadata, and integrity checks using MD5, SHA-1, SHA-256, SHA-384, or SHA-512 hashes.

```javascript
const options = {
  httpMetadata: {
    contentType: "application/json"
  },
  customMetadata: {
    "x-custom-header": "value"
  },
  sha256: "your-sha256-hash"
};
```

```javascript
const options = {
  storageClass: "InfrequentAccess"
};
```

--------------------------------

### Query R2 Storage Data via GraphQL

Source: https://developers.cloudflare.com/r2/llms-full

This snippet shows how to query the `r2StorageAdaptiveGroups` dataset using GraphQL to retrieve storage information for R2 buckets. It includes filtering by account tag and datetime, and specifies fields such as `bucketName`, `payloadSize`, and `objectCount`. This helps in monitoring storage usage and costs.

```GraphQL
query (
  $accountTag: String!
) {
  r2StorageAdaptiveGroups(accountTag: $accountTag) {
    bucketName
    payloadSize
    metadataSize
    objectCount
    uploadCount
    datetime
  }
}
```

--------------------------------

### Upload object with Wrangler CLI

Source: https://developers.cloudflare.com/r2/llms-full

Uploads a single object to an R2 bucket using the Wrangler CLI. This command supports setting HTTP header metadata via optional flags. Note that Wrangler's `object put` command is limited to uploading one object at a time and files up to 315MB.

```sh
wrangler r2 object put test-bucket/dataset.csv --file=dataset.csv
```

--------------------------------

### Configure Queue Consumer Binding in Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

This snippet shows how to configure a queue consumer binding in Wrangler, specifying the queue name ('pdf-summarizer') that the worker will listen to for event notifications. Configurations are provided for both `wrangler.jsonc` and `wrangler.toml`.

```json
{
  "queues": {
    "consumers": [
      {
        "queue": "pdf-summarizer"
      }
    ]
  }
}
```

--------------------------------

### Upload Object via Wrangler

Source: https://developers.cloudflare.com/r2/objects/upload-objects

Upload a single object to Cloudflare R2 using the Wrangler CLI. The `put` command requires an object name (key) and the local file path. Optional flags can set HTTP headers like Content-Type.

```bash
wrangler r2 object put --file=<path/to/your/file> <your-object-name>
wrangler r2 object put --file=<path/to/your/file> <your-object-name> --content-type=text/plain --cache-control="max-age=3600"
```

--------------------------------

### Create R2 Bucket Event Notification

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Bash command to create an event notification for an R2 bucket. This triggers the Worker when a new file with a specific suffix (e.g., '.pdf') is uploaded.

```bash
wrangler r2 bucket notification create <R2_BUCKET_NAME> --event object-create --suffix pdf --queue pdf-summarizer
```

--------------------------------

### Return ContinuationToken in S3 ListObjectsV2

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixes a bug where the ContinuationToken field was not correctly returned in S3 API ListObjectsV2 responses when provided in the request. This ensures proper pagination functionality.

```S3 API
ContinuationToken field in S3 API ListObjectsV2 response
```

--------------------------------

### S3 GetBucketLocation Implementation

Source: https://developers.cloudflare.com/r2/llms-full

Implements `GetBucketLocation` for compatibility with external tools. This will always return a `LocationConstraint` of `auto`.

```bash
curl -X GET http://<bucket-name>.r2.cloudflarestorage.com/?location
```

--------------------------------

### API: Add Minimum TLS Version for Custom Domains

Source: https://developers.cloudflare.com/r2/platform/release-notes

Describes the API update that allows setting and updating the minimum TLS version for R2 bucket custom domains. This enhances security by controlling the TLS versions accepted for custom domain access.

```json
{
  "minTLS": "1.2"
}
```

--------------------------------

### Set Minimum TLS Version for R2 Bucket Custom Domains

Source: https://developers.cloudflare.com/r2/llms-full

Configure the minimum TLS version required for accessing an R2 bucket via its custom domain. This enhances security by enforcing modern TLS standards.

```bash
curl -X PUT https://api.cloudflare.com/client/v4/accounts/:account_id/r2/buckets/:bucket_name/domains/custom/:domain_name -d '{"minTLS": "1.2"}'
```

--------------------------------

### Implement CompleteMultipartUpload

Source: https://developers.cloudflare.com/r2/llms-full

The CompleteMultipartUpload operation in Cloudflare R2 is implemented, but it does not support Bucket Owner or Request Payer specific headers.

```S3 API
CompleteMultipartUpload:
  Unsupported Features:
    x-amz-expected-bucket-owner: ❌
    x-amz-request-payer: ❌
```

--------------------------------

### Add Lifecycle Rule with Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Adds a lifecycle rule to an R2 bucket using the Wrangler CLI. This command allows specifying rules for object expiration or transition.

```sh
npx wrangler r2 bucket lifecycle add <BUCKET_NAME> [OPTIONS]
```

--------------------------------

### Create AWS IAM Policy for R2 Access

Source: https://developers.cloudflare.com/r2/data-migration/sippy

This JSON policy grants necessary permissions for accessing a specific Cloudflare R2 bucket. Replace `<BUCKET_NAME>` with your actual bucket name. This policy is used when creating credentials for Cloudflare R2.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowR2Access",
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket",
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject"
      ],
      "Resource": [
        "arn:aws:s3:::<BUCKET_NAME>",
        "arn:aws:s3:::<BUCKET_NAME>/*"
      ]
    }
  ]
}
```

--------------------------------

### S3 Pre-signed URLs Support

Source: https://developers.cloudflare.com/r2/llms-full

Cloudflare R2 now supports S3 pre-signed URLs, enabling secure, time-limited access to objects.

```bash
aws s3 presign s3://<bucket-name>/<object-key> --expires-in 3600
```

--------------------------------

### S3 List Continuation Tokens Update

Source: https://developers.cloudflare.com/r2/platform/release-notes

List continuation tokens prior to 2022-07-01 are no longer accepted and must be obtained again through a new list operation.

```text
List continuation tokens prior to 2022-07-01 are no longer accepted and must be obtained again through a new `list` operation.
```

--------------------------------

### API: Fix PutBucketCors Valid Origins

Source: https://developers.cloudflare.com/r2/platform/release-notes

Ensures that `PutBucketCors` now only accepts valid origins, improving the security and correctness of Cross-Origin Resource Sharing configurations.

```json
{
  "CORSRules": [
    {
      "AllowedOrigins": ["https://example.com"]
    }
  ]
}
```

--------------------------------

### Allow 0-Length Parts in S3 UploadPart

Source: https://developers.cloudflare.com/r2/platform/release-notes

Enables the S3 API UploadPart operation to accept 0-length parts, providing more flexibility in handling data uploads.

```S3 API
UploadPart operation with 0-length part
```

--------------------------------

### R2 Metadata Mapping to HTTP Headers

Source: https://developers.cloudflare.com/r2/llms-full

This table shows how R2's `httpMetadata` properties in the Workers API map to standard HTTP headers, which are crucial for managing object content and behavior.

```HTTP
Content-Encoding
Content-Type
Content-Language
Content-Disposition
Cache-Control
Expires

```

--------------------------------

### Generate Presigned URL with AWS SDK for JavaScript

Source: https://developers.cloudflare.com/r2/llms-full

Generates a presigned PUT object URL using the `@aws-sdk/client-s3` package for JavaScript. This URL can be used to upload objects to an R2 bucket and is valid for a specified duration.

```javascript
import { PutObjectCommand, S3Client } from "@aws-sdk/client-s3";
import { getSignedUrl } from "@aws-sdk/s3-request-presigner";
const S3 = new S3Client({
  endpoint: "https://<account_id>.r2.cloudflarestorage.com",
  credentials: {
    accessKeyId: "<access_key_id>",
    secretAccessKey: "<access_key_secret>",
  },
  region: "auto",
});
const url = await getSignedUrl(
  S3,
  new PutObjectCommand({
    Bucket: bucket,
    Key: object,
  }),
  {
    expiresIn: 60 * 60 * 24 * 7, // 7d
  },
);
console.log(url);
```

--------------------------------

### Cloudflare R2 List Operation Compat Flag

Source: https://developers.cloudflare.com/r2/platform/release-notes

Support for the `r2_list_honor_include` compat flag in Cloudflare R2 list operations. Without this flag/date, list implicitly functions as `include: ['httpMetadata', 'customMetadata']`.

```text
Support the `r2_list_honor_include` compat flag coming up in an upcoming runtime release (default behavior as of 2022-07-14 compat date). Without that compat flag/date, list will continue to function implicitly as `include: ['httpMetadata', 'customMetadata']` regardless of what you specify.
```

--------------------------------

### S3 Listing Behavior Fixes

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixed S3 listing behavior for scenarios including exceeding the file limit, common prefixes, and objects sharing base names with folders.

```text
Fixes for Listing:
    * Fix listing behavior when the number of files within a folder exceeds the limit (you'd end up seeing a CommonPrefix for that large folder N times where N = number of children within the CommonPrefix / limit).
    * Fix corner case where listing could cause objects with sharing the base name of a "folder" to be skipped.
    * Fix listing over some files that shared a certain common prefix.
```

--------------------------------

### Summarize PDF Content using Workers AI

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

TypeScript code demonstrating how to use the Workers AI binding to summarize the extracted textual content from a PDF file.

```typescript
import { unpdf } from 'unpdf';

// ... inside the queue handler ...
const file = await env.R2_BUCKET.get(key);
const text = await unpdf(await file.arrayBuffer());

const response = await env.AI.run(
  '@cf/meta/llama-2-7b-chat-fp16',
  {
    prompt: `Summarize the following text: ${text}`
  }
);

const summary = response.response;
```

--------------------------------

### Fix Pagination Cursors with MaxKeys

Source: https://developers.cloudflare.com/r2/platform/release-notes

Corrects an issue where pagination cursors were incorrectly returned when the number of keys in a bucket matched the MaxKeys argument in S3 API list operations.

```S3 API
Pagination cursors with MaxKeys
```

--------------------------------

### S3 API: CORS Configuration

Source: https://developers.cloudflare.com/r2/llms-full

Allows configuration of Cross-Origin Resource Sharing (CORS) for R2 buckets via the S3 API, enabling browser-based access to R2 resources.

```XML
<CORSConfiguration>
  <CORSRule>
    <AllowedOrigin>*</AllowedOrigin>
    <AllowedMethod>GET</AllowedMethod>
    <AllowedMethod>PUT</AllowedMethod>
    <AllowedMethod>POST</AllowedMethod>
    <AllowedMethod>DELETE</AllowedMethod>
    <AllowedMethod>HEAD</AllowedMethod>
    <AllowedHeader>*</AllowedHeader>
    <ExposeHeader>Content-Length</ExposeHeader>
    <ExposeHeader>Content-Range</ExposeHeader>
    <MaxAgeSeconds>3000</MaxAgeSeconds>
  </CORSRule>
</CORSConfiguration>
```

--------------------------------

### R2PutOptions: Server-Side Encryption Key (SSE-C)

Source: https://developers.cloudflare.com/r2/llms-full

Configure Server-Side Encryption Customer Key (SSE-C) for R2 PUT operations. The key, which must be 32 bytes, can be supplied as an ArrayBuffer or a hex-encoded string to ensure data security during upload.

```javascript
const options = {
  ssecKey: "fedcba9876543210fedcba9876543210"
};
```

--------------------------------

### R2GetOptions Definition

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Specifies options for retrieving an object from R2. This includes conditional retrieval based on `onlyIf` or headers, specifying a byte range for partial retrieval, and providing a Server-Side Encryption Customer Key (SSE-C).

```typescript
interface R2GetOptions {
  onlyIf?: R2Conditional | Headers;
  range?: R2Range;
  ssecKey?: ArrayBuffer | string;
}
```

--------------------------------

### S3 List Binding Limit Handling

Source: https://developers.cloudflare.com/r2/platform/release-notes

The S3 list() binding now correctly returns a smaller limit if too much data would otherwise be returned, preventing Internal Errors.

```text
The `list()` binding will now correctly return a smaller limit if too much data would otherwise be returned (previously would return an `Internal Error`).
```

--------------------------------

### S3 CopyObject Response Headers

Source: https://developers.cloudflare.com/r2/llms-full

The S3 `CopyObject` operation now includes `x-amz-version-id` and `x-amz-copy-source-version-id` in the response headers. This change ensures consistency with other S3 methods.

```xml
<CopyObjectResult>
  <LastModified>2023-01-01T12:00:00.000Z</LastModified>
  <ETag>"some-etag-value"</ETag>
  <x-amz-version-id>some-version-id</x-amz-version-id>
  <x-amz-copy-source-version-id>some-copy-source-version-id</x-amz-copy-source-version-id>
</CopyObjectResult>
```

--------------------------------

### R2 All Buckets Resource Identifier

Source: https://developers.cloudflare.com/r2/llms-full

Defines the format for identifying all R2 buckets within an account in an access policy. It requires the account ID.

```json
"com.cloudflare.api.account.<ACCOUNT_ID>": {
  "com.cloudflare.edge.r2.bucket.*": "*"
}
```

--------------------------------

### HTML for File Upload Form

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

Basic HTML structure for a file upload form. It includes a file input and a submit button, allowing users to upload PDF files.

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PDF Uploader</title>
    <style>
        body { font-family: sans-serif; }
        .container { max-width: 500px; margin: 50px auto; padding: 20px; border: 1px solid #ccc; border-radius: 8px; text-align: center; }
        input[type="file"] { margin-bottom: 20px; }
        button { padding: 10px 20px; background-color: #007bff; color: white; border: none; border-radius: 4px; cursor: pointer; }
        button:hover { background-color: #0056b3; }
    </style>
</head>
<body>
    <div class="container">
        <h1>Upload a PDF File</h1>
        <form id="uploadForm">
            <input type="file" id="pdfFile" name="pdfFile" accept=".pdf" required>
            <button type="submit">Upload</button>
        </form>
    </div>

    <script>
        const uploadForm = document.getElementById('uploadForm');

        uploadForm.addEventListener('submit', async (event) => {
            event.preventDefault();

            const fileInput = document.getElementById('pdfFile');
            const file = fileInput.files[0];

            if (!file) {
                alert('Please select a file.');
                return;
            }

            const formData = new FormData();
            formData.append('pdfFile', file);

            try {
                const response = await fetch('/api/upload', {
                    method: 'POST',
                    body: formData
                });

                if (response.ok) {
                    alert('File uploaded successfully!');
                    uploadForm.reset();
                } else {
                    alert('File upload failed.');
                }
            } catch (error) {
                console.error('Error uploading file:', error);
                alert('An error occurred during upload.');
            }
        });
    </script>
</body>
</html>

```

--------------------------------

### R2 ListMultipartUpload Key Encoding

Source: https://developers.cloudflare.com/r2/llms-full

The `ListMultipartUpload` operation now correctly encodes the returned `Key` when the `encoding-type` parameter is specified.

```javascript
await env.BUCKET.list({ encodingType: 'url' });
```

--------------------------------

### S3 XML Declaration Handling

Source: https://developers.cloudflare.com/r2/platform/release-notes

S3 XML documents sent to R2 with an XML declaration are no longer rejected with a 400 Bad Request / MalformedXML error.

```text
S3 XML documents sent to R2 that have an XML declaration are not rejected with `400 Bad Request` / `MalformedXML`.
```

--------------------------------

### Generate Presigned URL for Upload (JavaScript/TypeScript)

Source: https://developers.cloudflare.com/r2/examples/aws/aws-sdk-js

Shows how to generate a presigned URL using the AWS SDK for JavaScript for uploading an object to a Cloudflare R2 bucket. The generated URL is temporary and grants write access until it expires.

```javascript
const params = {
  Bucket: 'YOUR_BUCKET_NAME',
  Key: 'YOUR_OBJECT_KEY'
};

s3.getSignedUrl('putObject', params, (err, url) => {
  if (err) {
    console.error(err);
  } else {
    console.log('Presigned URL:', url);
  }
});
```

--------------------------------

### API: Fix PutBucketCors AllowedHeaders

Source: https://developers.cloudflare.com/r2/platform/release-notes

Addresses a bug in the `PutBucketCors` API action where it incorrectly accepted empty strings for the `AllowedHeaders` property. This fix ensures valid CORS configurations.

```json
{
  "AllowedHeaders": ["*"]
}
```

--------------------------------

### List Objects in R2 Bucket using AWS CLI

Source: https://developers.cloudflare.com/r2/llms-full

List objects within a specific Cloudflare R2 bucket using the AWS CLI. You need to provide the endpoint URL and the bucket name.

```shell
aws s3api list-objects-v2 --endpoint-url https://<accountid>.r2.cloudflarestorage.com --bucket sdk-example
```

--------------------------------

### S3 CopyObject Response Headers

Source: https://developers.cloudflare.com/r2/platform/release-notes

The S3 CopyObject operation now includes x-amz-version-id and x-amz-copy-source-version-id in response headers for improved consistency with other methods.

```text
CopyObject operation now includes `x-amz-version-id` and `x-amz-copy-source-version-id` in the response headers.
```

--------------------------------

### Conditional Headers for S3 UploadObject and CreateMultipartUpload

Source: https://developers.cloudflare.com/r2/platform/release-notes

Implements a 412 Precondition Failed status code for S3 API UploadObject and CreateMultipartUpload operations when conditional headers are provided and the object exists but conditions are not met. This enhances conditional upload logic.

```S3 API
UploadObject operation
CreateMultipartUpload operation
Conditional headers
```

--------------------------------

### Create Bucket - S3 API Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

Describes the 'CreateBucket' operation for the S3 API in Cloudflare R2. Key features such as ACLs, Object Locking, and Bucket Owner details are not implemented.

```S3 API
CreateBucket
```

--------------------------------

### Explicitly Reject Unsupported S3 Operations

Source: https://developers.cloudflare.com/r2/platform/release-notes

Ensures that unsupported S3 API operations are now explicitly rejected with an 'unimplemented' error, rather than being implicitly converted to other operations.

```S3 API
Explicit rejection of unsupported operations
```

--------------------------------

### Implicitly Create Bucket on Upload (R2)

Source: https://developers.cloudflare.com/r2/api/s3/extensions

When uploading objects to Cloudflare R2, you can ensure a bucket exists by setting the `cf-create-bucket-if-missing` header to `true`. This header implicitly creates the bucket if it does not already exist, preventing `NoSuchBucket` errors and avoiding the need for separate `CreateBucket` calls, especially useful for on-demand bucket creation.

```HTTP
PUT /my-object HTTP/1.1
Host: your-bucket.your-account.r2.cloudflarestorage.com
Content-Type: application/octet-stream
Content-Length: 1234

cf-create-bucket-if-missing: true

[...object data...]
```

--------------------------------

### Query R2 Operations Data via GraphQL

Source: https://developers.cloudflare.com/r2/llms-full

This snippet demonstrates how to query the `r2OperationsAdaptiveGroups` dataset using GraphQL to retrieve operation details for R2 buckets. It includes filtering by account tag and datetime, and specifies fields like `actionType`, `bucketName`, and `responseStatusCode`. This is useful for analyzing request patterns and performance.

```GraphQL
query (
  $accountTag: String!
) {
  r2OperationsAdaptiveGroups(accountTag: $accountTag) {
    actionType
    actionStatus
    bucketName
    objectName
    responseStatusCode
    datetime
  }
}
```

--------------------------------

### S3 API: HeadBucket Response

Source: https://developers.cloudflare.com/r2/llms-full

Modifies the `HeadBucket` response in the S3 compatible API for R2 to include the `x-amz-bucket-region` header set to `auto`.

```Shell
# Example using AWS CLI to get bucket location (implicitly checks HeadBucket)
# aws s3api head-bucket --bucket my-r2-bucket --endpoint-url https://<account_id>.r2.cloudflarestorage.com
```

--------------------------------

### Handle NoSuchBucket Error in S3 CopyObject

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixes the S3 API CopyObject operation to return a NoSuchBucket error when copying to a non-existent bucket, instead of an internal error.

```S3 API
CopyObject to non-existent bucket (NoSuchBucket error)
```

--------------------------------

### UploadPart with SSE-C and System Metadata

Source: https://developers.cloudflare.com/r2/api/s3/api

This snippet details the headers required for the UploadPart operation in Cloudflare R2, specifically supporting Server-Side Encryption with Customer-Provided Keys (SSE-C) and system metadata like Content-MD5.

```api-headers
x-amz-server-side-encryption-customer-algorithm
x-amz-server-side-encryption-customer-key
x-amz-server-side-encryption-customer-key-MD5
Content-MD5
```

--------------------------------

### Cloudflare R2 Worker Bindings: Conditional Headers

Source: https://developers.cloudflare.com/r2/platform/release-notes

Describes the improved support in R2 worker bindings for parsing conditional headers with multiple ETags. These ETags can be strong, weak, or wildcards, offering more flexibility in conditional requests.

```javascript
// Example of conditional GET with multiple ETags (conceptual)
const response = await env.MY_BUCKET.get('my-object', {
  headers: {
    'If-Match': '"etag1", "etag2", *'
  }
});

```

--------------------------------

### S3 API: CopyObject with Cloudflare Headers

Source: https://developers.cloudflare.com/r2/llms-full

Supports Cloudflare-specific headers in the CopyObject operation of the S3 compatible API, enabling conditional copying based on the destination object's state.

```Shell
# Example using curl with a conditional header (e.g., If-Match)
# curl -X COPY -H "Destination: /my-destination-object" -H "If-Match: \"some-etag\"" https://<account_id>.r2.cloudflarestorage.com/my-source-object --endpoint-url https://<account_id>.r2.cloudflarestorage.com
```

--------------------------------

### Configure S3 IAM Policy for Cloudflare R2 Migration

Source: https://developers.cloudflare.com/r2/llms-full

This JSON policy grants read and list permissions to an S3 bucket, allowing Super Slurper to copy objects to Cloudflare R2. Replace `<BUCKET_NAME>` with your specific S3 bucket name.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": ["s3:Get*", "s3:List*"],
      "Resource": ["arn:aws:s3:::<BUCKET_NAME>", "arn:aws:s3:::<BUCKET_NAME>/*"]
    }
  ]
}
```

--------------------------------

### Configure R2 Bucket Jurisdiction in Wrangler (JSONC)

Source: https://developers.cloudflare.com/r2/llms-full

This snippet shows how to define an R2 bucket binding with a specific jurisdiction in a `wrangler.jsonc` file for Cloudflare Workers.

```json
{
  "r2_buckets": [
    {
      "bindings": [
        {
          "binding": "MY_BUCKET",
          "bucket_name": "<YOUR_BUCKET_NAME>",
          "jurisdiction": "<JURISDICTION>"
        }
      ]
    }
  ]
}
```

--------------------------------

### Cloudflare R2 S3 API: CORS Preflight and Headers

Source: https://developers.cloudflare.com/r2/platform/release-notes

Implements CORS preflight responses and adds CORS headers to other responses for S3 and public buckets. CORS configuration is currently managed via the S3 API.

```http
OPTIONS /my-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com
Access-Control-Request-Method: PUT
Access-Control-Request-Headers: Content-Type, Authorization
Origin: https://example.com

HTTP/1.1 200 OK
Access-Control-Allow-Origin: https://example.com
Access-Control-Allow-Methods: PUT, GET
Access-Control-Allow-Headers: Content-Type, Authorization
Access-Control-Max-Age: 3600


```

--------------------------------

### Cloudflare R2 S3 API: UploadPartCopy Re-enabled

Source: https://developers.cloudflare.com/r2/platform/release-notes

Restores the `UploadPartCopy` functionality, which allows copying individual parts of a multipart upload. This is crucial for managing and manipulating large objects efficiently.

```http
PUT /destination-object-part-1 HTTP/1.1
Host: your-bucket.s3.amazonaws.com
CopySource: /your-bucket/source-object(part-1)


```

--------------------------------

### Fix SignatureDoesNotMatch with Accept-Encoding

Source: https://developers.cloudflare.com/r2/platform/release-notes

Corrects a bug where using Accept-Encoding in SignedHeaders for S3 API requests resulted in a SignatureDoesNotMatch response. This ensures correct signature validation.

```S3 API
Accept-Encoding in SignedHeaders
SignatureDoesNotMatch response
```

--------------------------------

### Import unpdf and Extract Text in Worker

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

TypeScript code for a Cloudflare Worker's queue handler. It imports necessary modules from 'unpdf' and extracts textual content from a PDF file retrieved from an R2 bucket.

```typescript
import { unpdf } from 'unpdf';

// ... inside the queue handler ...
const file = await env.R2_BUCKET.get(key);
const text = await unpdf(await file.arrayBuffer());
console.log(text);
```

--------------------------------

### Create Bucket Scoped Tokens for Cloudflare R2

Source: https://developers.cloudflare.com/r2/index

Explains how to create bucket-scoped tokens for Cloudflare R2, providing granular control over data access. These tokens can be assigned specific permissions for individual buckets.

```JavaScript
async function createBucketScopedToken(accountId, bucketName, tokenName, permissions) {
  const url = `https://api.cloudflare.com/client/v4/accounts/${accountId}/r2/buckets/${bucketName}/tokens`;
  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer YOUR_API_TOKEN`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      name: tokenName,
      permissions: permissions // e.g., ["read", "write"]
    })
  });
  const data = await response.json();
  console.log(data);
}
```

--------------------------------

### R2 User-Specified Checksums with put()

Source: https://developers.cloudflare.com/r2/llms-full

The R2 `put()` binding now allows users to specify SHA-1, SHA-256, SHA-384, and SHA-512 checksums in the options. This provides greater control over data integrity verification during uploads.

```javascript
await env.BUCKET.put(key, value, {
  checksum: "sha256-example",
  checksumAlgorithm: "sha256"
});
```

--------------------------------

### Make Cloudflare R2 Buckets Public

Source: https://developers.cloudflare.com/r2/index

Illustrates how to make a Cloudflare R2 bucket public, allowing direct internet access to its contents. This is useful for serving static assets like images, videos, or website files.

```JavaScript
async function makeBucketPublic(accountId, bucketName) {
  const url = `https://api.cloudflare.com/client/v4/accounts/${accountId}/r2/buckets/${bucketName}/public`;
  const response = await fetch(url, {
    method: 'PUT',
    headers: {
      'Authorization': `Bearer YOUR_API_TOKEN`
    }
  });
  const data = await response.json();
  console.log(data);
}
```

--------------------------------

### S3 Empty Delimiter Regression Fix

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixed a regression for some clients using an empty delimiter in S3 operations.

```text
Fixed a regression for some clients when using an empty delimiter.
```

--------------------------------

### List Buckets - S3 API Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

Demonstrates the 'ListBuckets' operation within the S3 API for Cloudflare R2. This operation is fully implemented.

```S3 API
ListBuckets
```

--------------------------------

### View Wrangler Logs

Source: https://developers.cloudflare.com/r2/llms-full

This command allows you to view logs from your deployed Cloudflare Workers application in your terminal. It's a useful tool for debugging and monitoring.

```sh
npx wrangler tail
```

--------------------------------

### Query R2 Storage Volume (GraphQL)

Source: https://developers.cloudflare.com/r2/platform/metrics-analytics

This GraphQL query retrieves the storage details for an R2 bucket over a specified time period. It includes metrics like payload size, metadata size, object count, and pending multipart uploads. The `bucketName` can be omitted for an account-wide overview.

```GraphQL
query (
  $from: DateTime!
  $to: DateTime!
  $accountTag: String!
) {
  viewer {
    r2StorageAdaptiveGroups(
      accountTag: $accountTag
      filter: {
        datetime: { from: $from, to: $to }
      }
      # Example: Group by bucketName
      # Example: Group by bucketName, jurisdiction
      groups: [bucketName]
    ) {
      sum {
        payloadSize
        metadataSize
        objectCount
        uploadCount
      }
      dimensions {
        bucketName
      }
    }
  }
}
```

--------------------------------

### ListParts Query Parameters

Source: https://developers.cloudflare.com/r2/api/s3/api

This snippet details the query parameters available for the ListParts operation in Cloudflare R2, used for paginating through parts of a multipart upload.

```api-query-parameters
max-parts
part-number-marker
```

--------------------------------

### Fetch and Cache with Cloudflare Workers and R2

Source: https://developers.cloudflare.com/r2/llms-full

This JavaScript code snippet demonstrates how to use the Cache API with Cloudflare Workers to fetch objects from R2. It checks the cache first, and if a cache miss occurs, it retrieves the object from R2, sets appropriate headers including Cache-Control, and stores the response in the cache for future requests. It handles potential errors during the process.

```javascript
export default {
  async fetch(request, env, context) {
    try {
      const url = new URL(request.url);


      // Construct the cache key from the cache URL
      const cacheKey = new Request(url.toString(), request);
      const cache = caches.default;


      // Check whether the value is already available in the cache
      // if not, you will need to fetch it from R2, and store it in the cache
      // for future access
      let response = await cache.match(cacheKey);


      if (response) {
        console.log(`Cache hit for: ${request.url}.`);
        return response;
      }


      console.log(
        `Response for request url: ${request.url} not present in cache. Fetching and caching request.`
      );


      // If not in cache, get it from R2
      const objectKey = url.pathname.slice(1);
      const object = await env.MY_BUCKET.get(objectKey);
      if (object === null) {
        return new Response('Object Not Found', { status: 404 });
      }


      // Set the appropriate object headers
      const headers = new Headers();
      object.writeHttpMetadata(headers);
      headers.set('etag', object.httpEtag);


      // Cache API respects Cache-Control headers. Setting s-max-age to 10
      // will limit the response to be in cache for 10 seconds max
      // Any changes made to the response here will be reflected in the cached value
      headers.append('Cache-Control', 's-maxage=10');


      response = new Response(object.body, {
        headers,
      });


      // Store the fetched response as cacheKey
      // Use waitUntil so you can return the response without blocking on
      // writing to cache
      context.waitUntil(cache.put(cacheKey, response.clone()));


      return response;
    } catch (e) {
      return new Response('Error thrown ' + e.message);
    }
  },
};

```

--------------------------------

### R2MultipartUpload Upload Part

Source: https://developers.cloudflare.com/r2/llms-full

The `uploadPart` method allows you to upload a single part of a multipart upload. It requires a part number and the data for the part, which can be a ReadableStream, ArrayBuffer, string, or Blob. It returns an `R2UploadedPart` object.

```javascript
async function uploadSinglePart(multipartUpload, partNumber, partData) {
  const uploadedPart = await multipartUpload.uploadPart(partNumber, partData);
  return uploadedPart;
}
```

--------------------------------

### Cloudflare R2 S3 API: Multipart Upload Performance

Source: https://developers.cloudflare.com/r2/platform/release-notes

Addresses a performance issue where concurrent multipart part uploads were being rejected. This fix improves the throughput and reliability of uploading large files in parts.

```http
POST /my-object?uploadId=EXAMPLEID&partNumber=1&entitySize=1048576 HTTP/1.1
Host: your-bucket.s3.amazonaws.com


```

--------------------------------

### Conditional Multipart Publish Operations

Source: https://developers.cloudflare.com/r2/llms-full

Allows users to perform conditional multipart publish operations for R2. If a condition fails during publishing, the upload is treated as aborted and is no longer available.

```javascript
// Conceptual representation of a conditional multipart upload
// The actual implementation would involve specific API calls or SDK methods.
function completeMultipartUpload(uploadId, parts, conditions) {
  // ... logic to check conditions ...
  if (conditionsMet) {
    // ... complete upload ...
  } else {
    // ... abort upload ...
  }
}
```

--------------------------------

### R2 Bucket Resource Identifier

Source: https://developers.cloudflare.com/r2/llms-full

Defines the format for identifying a specific R2 bucket within an access policy. It includes account ID, jurisdiction, and bucket name.

```json
"com.cloudflare.edge.r2.bucket.<ACCOUNT_ID>_<JURISDICTION>_<BUCKET_NAME>": "*"
```

--------------------------------

### Merge Metadata with `CopyObject`

Source: https://developers.cloudflare.com/r2/llms-full

The `CopyObject` operation in Cloudflare R2 allows for flexible metadata management using the `x-amz-metadata-directive` header. In addition to `COPY` and `REPLACE`, it supports a `MERGE` value. `MERGE` copies all metadata from the source object and then replaces only the specified metadata keys with new values, without removing existing ones. For removing metadata, use the `REPLACE` directive.

--------------------------------

### Generate Presigned URL for Cloudflare R2 Object (C#)

Source: https://developers.cloudflare.com/r2/llms-full

Generates a presigned URL for a specific object in a Cloudflare R2 bucket using the AWS SDK for .NET. This allows temporary access to the object for a specified duration. Requires setting `UseSignatureVersion4` and configuring the request with bucket name, key, HTTP verb, and expiration time.

```csharp
static string? GeneratePresignedUrl()
{
  AWSConfigsS3.UseSignatureVersion4 = true;
  var presign = new GetPreSignedUrlRequest
  {
    BucketName = "sdk-example",
    Key = "file.txt",
    Verb = HttpVerb.GET,
    Expires = DateTime.Now.AddDays(7),
  };


  var presignedUrl = s3Client.GetPreSignedURL(presign);


  Console.WriteLine(presignedUrl);


  return presignedUrl;
}
```

--------------------------------

### S3 API: GetObject with Range Header

Source: https://developers.cloudflare.com/r2/llms-full

Handles ranged requests for GetObject operations in the S3 compatible API for R2, returning specific byte ranges of an object.

```Shell
# Example using curl to request a range of bytes
# curl -H "Range: bytes=0-99" https://<account_id>.r2.cloudflarestorage.com/my-object -o output.bin
```

--------------------------------

### Cloudflare R2 ListObjectsV2 Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

The ListObjectsV2 operation provides a version 2 listing of objects in a Cloudflare R2 bucket. It supports parameters like continuation-token, delimiter, encoding-type, and start-after.

```API
ListObjectsV2
```

--------------------------------

### Cloudflare R2 S3 API: CORS AllowedHeaders Case-Insensitive

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixes a CORS bug where `AllowedHeaders` in the CORS configuration were treated case-sensitively. This ensures that headers like `Authorization` are correctly matched regardless of case.

```xml
<CORSConfiguration>
  <CORSRule>
    <AllowedOrigin>*</AllowedOrigin>
    <AllowedMethod>GET</AllowedMethod>
    <AllowedHeader>AUTHORIZATION</AllowedHeader>
  </CORSRule>
</CORSConfiguration>

```

--------------------------------

### S3 Multipart Upload ETag Suffix

Source: https://developers.cloudflare.com/r2/platform/release-notes

ETags for multipart files uploaded after Open Beta now include the number of parts as a suffix, enhancing identification of multipart uploads.

```text
The `ETag` for multipart files uploaded until shortly after Open Beta uploaded now include the number of parts as a suffix.
```

--------------------------------

### Add CNAME Record for Custom Domain

Source: https://developers.cloudflare.com/r2/buckets/public-buckets

This bash command demonstrates how to add a CNAME record to your Cloudflare DNS settings, which is a necessary step when connecting a custom domain to an R2 bucket. It uses the Cloudflare API to create the DNS record.

```bash
curl -X POST "https://api.cloudflare.com/client/v4/zones/{ZONE_ID}/dns_records" \
     -H "X-Auth-Email: YOUR_EMAIL" \
     -H "X-Auth-Key: YOUR_API_KEY" \
     -H "Content-Type: application/json" \
     --data '{"type":"CNAME","name":"your.custom.domain","content":"your-bucket-name.your-account-id.r2.dev","ttl":1,"proxied":true}'
```

--------------------------------

### Fix Number-like Keys Parsing in S3 API

Source: https://developers.cloudflare.com/r2/platform/release-notes

Addresses a bug where number-like keys in S3 API requests were being parsed as numbers instead of strings. This ensures consistent handling of key names.

```S3 API
Number-like keys parsing
```

--------------------------------

### R2 Error Handling Improvements

Source: https://developers.cloudflare.com/r2/llms-full

Improvements to error handling for 500 errors. Concurrency problems are now reported as `TooMuchConcurrency` instead of `InternalError`. Internal improvements have also reduced the rate of 500 errors.

```javascript
// No direct code example, this is a behavior change in the R2 API.
```

--------------------------------

### Cloudflare R2 Authentication Tokens Scoping

Source: https://developers.cloudflare.com/r2/platform/release-notes

Updates R2 authentication tokens created via the R2 token page to be scoped to a single account by default. This enhances security by limiting token access.

--------------------------------

### S3 CreateBucket with Object Lock Disabled

Source: https://developers.cloudflare.com/r2/llms-full

Allows specifying `x-amz-bucket-object-lock-enabled` with a value of `false` in S3 `CreateBucket` requests. Requests with `true` will still be rejected as R2 does not yet support object locks.

```bash
curl -X PUT \
  -H "x-amz-bucket-object-lock-enabled: false" \
  http://<bucket-name>.r2.cloudflarestorage.com/
```

--------------------------------

### R2MultipartUpload Abort Upload

Source: https://developers.cloudflare.com/r2/llms-full

The `abort` method is used to cancel an ongoing multipart upload. It returns a Promise that resolves once the upload has been successfully aborted.

```javascript
async function abortMultipartUpload(multipartUpload) {
  await multipartUpload.abort();
}
```

--------------------------------

### Edit Rclone Configuration File

Source: https://developers.cloudflare.com/r2/examples/rclone

Instructions on how to manually edit the Rclone configuration file to add or modify a Cloudflare R2 remote. This is useful for direct configuration management.

```bash
rclone config file
```

--------------------------------

### Generate Presigned URL with AWS SDK for JavaScript

Source: https://developers.cloudflare.com/r2/api/s3/presigned-urls

This snippet demonstrates how to generate a presigned URL for uploading to a Cloudflare R2 bucket using the AWS SDK for JavaScript (v3). It requires an access key ID, secret access key, and the 'aws4fetch' package for signing.

```JavaScript
import { AwsClient } from "aws4fetch";

const client = new AwsClient({
  accessKeyId: "YOUR_ACCESS_KEY_ID",
  secretAccessKey: "YOUR_SECRET_ACCESS_KEY",
  service: "s3",
  region: "us-east-1", // Or your desired region
});

const url = await client.getSignedUrl("PUT", "https://your-r2-bucket.your-account-id.r2.cloudflarestorage.com/uploads/dog.png", {
  headers: {
    "x-amz-acl": "public-read", // Example header
  },
});

console.log(url);
```

--------------------------------

### Escape S3 CompleteMultipartUploads Requests

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixes an issue where S3 API CompleteMultipartUploads requests were not properly escaped, ensuring correct handling of multipart upload completion.

```S3 API
CompleteMultipartUploads requests escaping
```

--------------------------------

### Access R2 Bucket Operations (Module Worker)

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-usage

This code snippet shows how to interact with an R2 bucket from a Cloudflare Worker using the Module Worker syntax. It demonstrates basic operations like reading, listing, writing, and deleting objects, assuming the bucket is bound to the `MY_BUCKET` variable.

```javascript
export default {
  async fetch(request, env) {
    // Example: List objects in the bucket
    let list = await env.MY_BUCKET.list();
    let رشته = "";
    for (const object of list.objects) {
      rstring += object.key + " ";
    }
    return new Response(rstring);

    // Example: Put an object into the bucket
    // await env.MY_BUCKET.put("my-object", "Hello World!");

    // Example: Get an object from the bucket
    // let object = await env.MY_BUCKET.get("my-object");
    // return new Response(object.body);

    // Example: Delete an object from the bucket
    // await env.MY_BUCKET.delete("my-object");
  },
};

```

```typescript
interface Env {
  MY_BUCKET: R2Bucket;
}

export default {
  async fetch(request: Request, env: Env) {
    // Example: List objects in the bucket
    let list = await env.MY_BUCKET.list();
    let رشته = "";
    for (const object of list.objects) {
      rstring += object.key + " ";
    }
    return new Response(rstring);

    // Example: Put an object into the bucket
    // await env.MY_BUCKET.put("my-object", "Hello World!");

    // Example: Get an object from the bucket
    // let object = await env.MY_BUCKET.get("my-object");
    // return new Response(object.body);

    // Example: Delete an object from the bucket
    // await env.MY_BUCKET.delete("my-object");
  },
};

```

--------------------------------

### Configure R2 CORS Policy with Allowed Origins and Methods

Source: https://developers.cloudflare.com/r2/llms-full

This JSON snippet demonstrates how to configure a Cloudflare R2 CORS policy, specifying allowed origins and methods for accessing bucket resources. It's essential for enabling cross-origin requests from web applications.

```json
[
  {
    "AllowedOrigins": ["http://localhost:3000"],
    "AllowedMethods": ["GET"]
  }
]
```

--------------------------------

### Cloudflare R2 S3 API: Ranged Requests HTTP 206

Source: https://developers.cloudflare.com/r2/platform/release-notes

Ensures that ranged requests to R2 always return an HTTP 206 Partial Content status code, matching the behavior of other S3-compatible implementations. This standardizes partial content responses.

```http
GET /my-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com
Range: bytes=0-1023

HTTP/1.1 206 Partial Content
Content-Range: bytes 0-1023/51200
...
```

--------------------------------

### Upload Objects to Cloudflare R2 with Rclone

Source: https://developers.cloudflare.com/r2/examples/rclone

Shows how to use the `rclone copy` command to upload files to a Cloudflare R2 bucket. This command supports large file uploads up to R2's 5 TB limit.

```bash
rclone copy <source_path> <r2_remote_name>:<bucket_name>
```

--------------------------------

### UploadPart - Cloudflare R2

Source: https://developers.cloudflare.com/r2/llms-full

Uploads a part of an object. Supports system metadata like Content-MD5 and SSE-C. Does not support SSE or request payer.

```bash
aws s3api upload-part --bucket BUCKET --key KEY --part-number 1 --body file.txt --content-md5 MD5_HASH --sse-customer-algorithm AES256 --sse-customer-key YOUR_KEY --sse-customer-key-md5 YOUR_KEY_MD5
```

--------------------------------

### Cloudflare R2 Bucket Properties

Source: https://developers.cloudflare.com/r2/llms-full

Defines the properties associated with events in a Cloudflare R2 bucket. These properties include account ID, action type, bucket name, object details (key, size, eTag), event time, and information about copy operations.

```json
{
  "account": "String",
  "action": "String",
  "bucket": "String",
  "object": {
    "key": "String",
    "size": "Number",
    "eTag": "String"
  },
  "eventTime": "String",
  "copySource": {
    "bucket": "String",
    "object": "String"
  }
}
```

--------------------------------

### Configure R2 with Terraform AWS Provider

Source: https://developers.cloudflare.com/r2/examples/terraform-aws

This snippet demonstrates the Terraform configuration for Cloudflare R2 using the AWS provider. It includes setting the S3 endpoint, R2 API credentials, and necessary provider options to ensure correct validation behavior.

```Terraform
provider "aws" {
  access_key = var.access_key_id
  secret_key = var.access_key_secret
  region = "auto"
  skip_region_validation = true
  skip_requesting_account_id = true
  skip_credentials_validation = true

  endpoints = {
    s3 = "https://<ACCOUNT_ID>.r2.cloudflarestorage.com"
  }
}

variable "access_key_id" {
  description = "Your R2 Access Key ID"
  type        = string
}

variable "access_key_secret" {
  description = "Your R2 Secret Access Key"
  type        = string
  sensitive   = true
}
```

--------------------------------

### Summarize PDF Text using Workers AI

Source: https://developers.cloudflare.com/r2/llms-full

Utilizes the Workers AI binding to summarize the extracted text from a PDF using the '@cf/facebook/bart-large-cnn' model and logs the beginning of the summary.

```typescript
async queue(batch, env) {
  for(let message of batch.messages) {
    // Extract the textual content from the PDF
    const {text} = await extractText(document, {mergePages: true});
    console.log(`Extracted text: ${text.substring(0, 100)}...`);


    // Use Workers AI to summarize the content
    const result: AiSummarizationOutput = await env.AI.run(
    "@cf/facebook/bart-large-cnn",
      {
        input_text: text,
      }
    );
    const summary = result.summary;
    console.log(`Summary: ${summary.substring(0, 100)}...`);
  }
}
```

--------------------------------

### Configure R2 Endpoint for S3 Clients

Source: https://developers.cloudflare.com/r2/llms-full

This snippet shows how to configure the endpoint for an S3 client to interact with Cloudflare R2. It includes the general endpoint format and specific formats for jurisdictional buckets in the EU and FedRAMP.

```text
You will also need to configure the `endpoint` in your S3 client to `https://<ACCOUNT_ID>.r2.cloudflarestorage.com`.

Buckets created with jurisdictions must be accessed via jurisdiction-specific endpoints:

* European Union (EU): `https://<ACCOUNT_ID>.eu.r2.cloudflarestorage.com`
* FedRAMP: `https://<ACCOUNT_ID>.fedramp.r2.cloudflarestorage.com`
```

--------------------------------

### Verify Multipart Upload ETags

Source: https://developers.cloudflare.com/r2/llms-full

Illustrates how to calculate the ETag for a multipart uploaded object in Cloudflare R2, mimicking S3 behavior. It involves concatenating MD5 hashes of individual parts and then hashing the result.

```plaintext
echo -n $(echo -n bce6bf66aeb76c7040fdd5f4eccb78e6 | xxd -r -p -)
$(echo -n 8165449fc15bbf43d3b674595cbcc406 | xxd -r -p -) | md5sum
```

--------------------------------

### Python Script for Parallel Multipart Upload to Cloudflare Worker

Source: https://developers.cloudflare.com/r2/llms-full

This Python script facilitates uploading local files to a Cloudflare Worker using multipart uploads. It utilizes the `requests` library and `ThreadPoolExecutor` for parallel part uploads, significantly improving performance. The script handles file chunking, initiating the multipart upload, uploading individual parts with retries, and completing the upload.

```python
import math
import os
import requests
from requests.adapters import HTTPAdapter, Retry
import sys
import concurrent.futures


# Take the file to upload as an argument
filename = sys.argv[1]
# The endpoint for our worker, change this to wherever you deploy your worker
worker_endpoint = "https://myworker.myzone.workers.dev/"
# Configure the part size to be 10MB. 5MB is the minimum part size, except for the last part
partsize = 10 * 1024 * 1024



def upload_file(worker_endpoint, filename, partsize):
    url = f"{worker_endpoint}{filename}"


    # Create the multipart upload
    uploadId = requests.post(url, params={"action": "mpu-create"}).json()["uploadId"]


    part_count = math.ceil(os.stat(filename).st_size / partsize)
    # Create an executor for up to 25 concurrent uploads.
    executor = concurrent.futures.ThreadPoolExecutor(25)
    # Submit a task to the executor to upload each part
    futures = [
        executor.submit(upload_part, filename, partsize, url, uploadId, index)
        for index in range(part_count)
    ]
    concurrent.futures.wait(futures)
    # get the parts from the futures
    uploaded_parts = [future.result() for future in futures]


    # complete the multipart upload
    response = requests.post(
        url,
        params={"action": "mpu-complete", "uploadId": uploadId},
        json={"parts": uploaded_parts},
    )
    if response.status_code == 200:
        print("🎉 successfully completed multipart upload")
    else:
        print(response.text)


def upload_part(filename, partsize, url, uploadId, index):
    # Open the file in rb mode, which treats it as raw bytes rather than attempting to parse utf-8
    with open(filename, "rb") as file:
        file.seek(partsize * index)
        part = file.read(partsize)

    # Retry policy for when uploading a part fails
    s = requests.Session()
    retries = Retry(total=3, status_forcelist=[400, 500, 502, 503, 504])
    s.mount("https://", HTTPAdapter(max_retries=retries))

    return s.put(
        url,
        params={
            "action": "mpu-uploadpart",
            "uploadId": uploadId,
            "partNumber": str(index + 1),
        },
        data=part,
    ).json()


upload_file(worker_endpoint, filename, partsize)

```

--------------------------------

### Cloudflare R2 Worker Bindings: Multiple Deletions

Source: https://developers.cloudflare.com/r2/platform/release-notes

Shows how to use the R2 worker bindings to delete multiple keys from a bucket in a single operation. This is an efficient way to clean up multiple objects.

```javascript
await env.MY_BUCKET.delete(['key1', 'key2', 'key3']);

```

--------------------------------

### Read Data from Iceberg Table (Python)

Source: https://developers.cloudflare.com/r2/llms-full

This code snippet illustrates how to read data back from the Iceberg table using Spark and display the results. It specifies the 'iceberg' format for loading the data.

```Python
# Read the data back from the Iceberg table
result_df = spark.read \
    .format("iceberg") \
    .load("default.my_table")


result_df.show()
```

--------------------------------

### Enforce 'auto' in SigV4 and CreateBucket LocationConstraint

Source: https://developers.cloudflare.com/r2/platform/release-notes

Enforces the use of 'auto' for SigV4 signing and the LocationConstraint parameter in the S3 API CreateBucket operation, ensuring consistent region handling.

```S3 API
SigV4 signing 'auto' requirement
CreateBucket LocationConstraint 'auto'
```

--------------------------------

### Configure Rclone for Cloudflare R2

Source: https://developers.cloudflare.com/r2/llms-full

This TOML snippet shows how to configure a new R2 provider in the rclone.conf file. It specifies the S3 type, Cloudflare provider, access key ID, secret access key, endpoint, and ACL settings.

```toml
[r2]
type = "s3"
provider = "Cloudflare"
access_key_id = "abc123"
secret_access_key = "xyz456"
endpoint = "https://<accountid>.r2.cloudflarestorage.com"
acl = "private"
```

--------------------------------

### R2MultipartOptions: Server-Side Encryption Key (SSE-C)

Source: https://developers.cloudflare.com/r2/llms-full

Specify a Server-Side Encryption Customer Key (SSE-C) for R2 multipart uploads. The key must be 32 bytes and can be provided as an ArrayBuffer or a hex-encoded string for secure data handling.

```javascript
const options = {
  ssecKey: new Uint8Array(32).fill(1) // Example: 32-byte ArrayBuffer
};
```

--------------------------------

### Multipart Upload Etag Suffix and UploadPart Errors

Source: https://developers.cloudflare.com/r2/platform/release-notes

Updates multipart uploads to include an '-N' suffix on the etag indicating the number of parts. Also improves error messages for UploadPart and UploadPartCopy operations, replacing 'internal error' with specific codes like TooMuchConcurrency or NoSuchUpload.

```S3 API
Multipart uploads etag suffix
UploadPart errors (TooMuchConcurrency, NoSuchUpload)
```

--------------------------------

### Set Wrangler Secret for R2 Authentication (Bash)

Source: https://developers.cloudflare.com/r2/llms-full

This bash command demonstrates how to set a secret environment variable, `AUTH_KEY_SECRET`, using the Wrangler CLI. This secret is crucial for securing PUT and DELETE operations on your R2 bucket by providing a shared key for authentication. The command prompts for the secret value and confirms its successful upload.

```bash
npx wrangler secret put AUTH_KEY_SECRET
```

--------------------------------

### Retrieve Object from Cloudflare R2

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to retrieve an object from a Cloudflare R2 bucket using the GetObject request. This involves selecting the appropriate options in a tool like Postman.

```Postman
GET GetObject
```

--------------------------------

### Enable R2 Event Notifications via Wrangler CLI

Source: https://developers.cloudflare.com/r2/buckets/event-notifications

This command enables event notifications on an R2 bucket using the Wrangler CLI. You can optionally filter events by object prefix or suffix using the `--prefix` or `--suffix` flags.

```bash
wrangler login
r2 bucket notification create <bucket-name> --queue <queue-name> --event object-create --prefix "images/"
```

--------------------------------

### CopyObject with Destination Conditions

Source: https://developers.cloudflare.com/r2/llms-full

Illustrates how to use conditional headers in the CopyObject operation for Cloudflare R2. These conditions apply to the destination object, preventing the copy if the conditions are not met.

```Cloudflare CLI
wrangler r2 cp my-bucket/source-object my-bucket/destination-object --destination-if-match etag123 --destination-if-none-match etag456 --destination-if-modified-since 2023-01-01 --destination-if-unmodified-since 2023-12-31
```

--------------------------------

### Filter R2 Bucket Notification by Prefix using Wrangler CLI

Source: https://developers.cloudflare.com/r2/llms-full

Command to create an R2 bucket notification rule with prefix filtering using the Wrangler CLI. This allows notifications to be triggered only for objects matching a specific prefix.

```sh
# Filter using prefix
$ npx wrangler r2 bucket notification create <BUCKET_NAME> --event-type <EVENT_TYPE> --queue <QUEUE_NAME> --prefix "<PREFIX_VALUE>"
```

--------------------------------

### Set R2 Bucket Jurisdiction using AWS SDK for JavaScript

Source: https://developers.cloudflare.com/r2/reference/data-location

This snippet demonstrates how to create an R2 bucket within a specific jurisdiction (e.g., 'eu') using the `@aws-sdk/client-s3` package for JavaScript. It highlights the necessary configuration for interacting with jurisdiction-specific R2 endpoints.

```javascript
import { S3Client, CreateBucketCommand } from "@aws-sdk/client-s3";

const s3Client = new S3Client({
  region: "auto",
  endpoint: "https://<ACCOUNT_ID>.eu.r2.cloudflarestorage.com",
  credentials: {
    accessKeyId: "<ACCESS_KEY_ID>",
    secretAccessKey: "<SECRET_ACCESS_KEY>",
  },
});

const createBucketCommand = new CreateBucketCommand({
  Bucket: "my-bucket-in-eu",
});

try {
  const response = await s3Client.send(createBucketCommand);
  console.log("Bucket created successfully:", response);
} catch (error) {
  console.error("Error creating bucket:", error);
}
```

--------------------------------

### Cloudflare R2 S3 API: CopyObject Multipart

Source: https://developers.cloudflare.com/r2/platform/release-notes

Re-enables the `CopyObject` operation for multipart objects, allowing users to copy large objects that were uploaded in parts. This restores full functionality for object copying.

```http
PUT /destination-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com
CopySource: /your-bucket/source-multipart-object


```

--------------------------------

### Generate Presigned URLs for Cloudflare R2 Upload

Source: https://developers.cloudflare.com/r2/examples/aws/aws-sdk-js-v3

This snippet shows how to generate a presigned URL for uploading an object to a Cloudflare R2 bucket using the AWS SDK for JavaScript v3. The generated link provides temporary write access and expires after a specified duration.

```javascript
import { S3 } from "@aws-sdk/client-s3";

const s3 = new S3({
  region: "auto",
  endpoint: `https://${process.env.CLOUDFLARE_ACCOUNT_ID}.r2.cloudflarestorage.com`,
  credentials: {
    accessKeyId: process.env.CLOUDFLARE_ACCESS_KEY_ID,
    secretAccessKey: process.env.CLOUDFLARE_SECRET_ACCESS_KEY,
  },
});

async function generatePresignedPutUrl(bucketName, objectKey) {
  const command = new PutObjectCommand({
    Bucket: bucketName,
    Key: objectKey,
  });

  const url = await getSignedUrl(s3, command, {
    expiresIn: 3600, // URL expires in 1 hour
  });

  return url;
}

// Example usage:
// const uploadUrl = await generatePresignedPutUrl("my-bucket", "my-object.txt");
// console.log("Upload URL:", uploadUrl);

```

--------------------------------

### S3 ListObjectsV2 Honor encoding-type

Source: https://developers.cloudflare.com/r2/platform/release-notes

Ensures the S3 API ListObjectsV2 operation correctly honors the `encoding-type` parameter, improving response formatting.

```S3 API
ListObjectsV2 encoding-type parameter
```

--------------------------------

### Cloudflare R2 S3 API: CopyObject Multipart Re-enabled

Source: https://developers.cloudflare.com/r2/platform/release-notes

Re-enables the `CopyObject` operation for multipart objects, allowing users to copy large objects that were uploaded in parts. This restores full functionality for object copying.

```http
PUT /destination-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com
CopySource: /your-bucket/source-multipart-object


```

--------------------------------

### Create Wrangler Secret

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-usage

This command demonstrates how to create a secret using the Wrangler CLI, which can then be used as an environment variable within your Worker. The secret is entered interactively in the terminal.

```bash
npx wrangler secret put AUTH_KEY_SECRET

```

--------------------------------

### S3 DeleteObjects Key Trimming Fix

Source: https://developers.cloudflare.com/r2/platform/release-notes

The S3 DeleteObjects operation no longer trims spaces from keys before deletion, preventing issues with files having leading/trailing spaces and ensuring correct object deletion.

```text
The S3 `DeleteObjects` operation no longer trims the space from around the keys before deleting. This would result in files with leading / trailing spaces not being able to be deleted. Additionally, if there was an object with the trimmed key that existed it would be deleted instead. The S3 `DeleteObject` operation was not affected by this.
```

--------------------------------

### Implement GetObject with Conditional Operations, Range, and SSE-C

Source: https://developers.cloudflare.com/r2/llms-full

The GetObject operation in Cloudflare R2 allows for conditional retrieval using If-Match, If-Modified-Since, If-None-Match, and If-Unmodified-Since headers. It also supports the Range header for partial object retrieval and SSE-C for encrypted data.

```S3 API
GetObject:
  Conditional Operations:
    If-Match: ✅
    If-Modified-Since: ✅
    If-None-Match: ✅
    If-Unmodified-Since: ✅
  Range:
    Range: ✅
    PartNumber: ✅
  SSE-C:
    x-amz-server-side-encryption-customer-algorithm: ✅
    x-amz-server-side-encryption-customer-key: ✅
    x-amz-server-side-encryption-customer-key-MD5: ✅
```

--------------------------------

### S3 PutObject POST Request Compatibility

Source: https://developers.cloudflare.com/r2/platform/release-notes

Enables the S3 API PutObject operation to work with POST requests, enhancing compatibility with tools like `s3cmd`.

```S3 API
PutObject operation with POST requests
```

--------------------------------

### S3 Error Handling Improvements

Source: https://developers.cloudflare.com/r2/platform/release-notes

Improvements to S3 error handling convert concurrency problems to `TooMuchConcurrency` instead of `InternalError`, and internal improvements reduce the rate of 500 errors.

```text
Improvements to 500s: we now convert errors, so things that were previously concurrency problems for some operations should now be `TooMuchConcurrency` instead of `InternalError`. We've also reduced the rate of 500s through internal improvements.
```

--------------------------------

### Upload Object to Cloudflare R2 (C#)

Source: https://developers.cloudflare.com/r2/llms-full

Uploads a file to a Cloudflare R2 bucket using the AWS SDK for .NET. Requires specifying the file path, bucket name, and disabling payload signing and default checksum validation due to R2's compatibility with AWS SDK's SigV4 implementation.

```csharp
static async Task PutObject()
{
  var request = new PutObjectRequest
  {
    FilePath = @"/path/file.txt",
    BucketName = "sdk-example",
    DisablePayloadSigning = true,
    DisableDefaultChecksumValidation = true
  };


  var response = await s3Client.PutObjectAsync(request);


  Console.WriteLine("ETag: {0}", response.ETag);
}
```

--------------------------------

### Configure Mastodon File Storage with R2

Source: https://developers.cloudflare.com/r2/tutorials/mastodon

This snippet shows how to configure the file storage section in Mastodon's environment file to use Cloudflare R2 as the object storage. It requires setting specific environment variables for the R2 S3 API endpoint, access key ID, secret access key, and bucket name.

```env
POST_ARGUMENTS_DIR=public/system

# File storage
# Use local storage if not specified
# LOCAL_STORAGE_PATH=~/live/public/system

# Use R2 object storage
S3_ENABLED=true
S3_ENDPOINT=https://<ACCOUNT_ID>.r2.cloudflarestorage.com
S3_ACCESS_KEY_ID=<ACCESS_KEY_ID>
S3_SECRET_ACCESS_KEY=<SECRET_ACCESS_KEY>
S3_BUCKET=<BUCKET_NAME>
S3_REGION=auto
S3_FORCE_PATH_STYLE=false
S3_V4_SIGNATURE=true
S3_USE_ACCELERATE_ENDPOINT=false

# Optional: Use a custom hostname for R2
# OBJECT_STORAGE_HOST=https://<CUSTOM_HOSTNAME>

```

--------------------------------

### S3 GetObject Ranges

Source: https://developers.cloudflare.com/r2/llms-full

S3 `GetObject` ranges are now inclusive. Partial reads return the proper `206 Partial Content` response code.

```bash
curl -i -H "Range: bytes=0-9" http://<bucket-name>.r2.cloudflarestorage.com/<object-key>
```

--------------------------------

### Add R2 Bucket Lock Rule via Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Shows how to add a bucket lock rule to an R2 bucket using the Wrangler CLI. This command requires the bucket name and can accept additional options.

```sh
npx wrangler r2 bucket lock add <BUCKET_NAME> [OPTIONS]
```

--------------------------------

### Cloudflare R2 CreateMultipartUpload Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

The CreateMultipartUpload operation initiates a multipart upload to Cloudflare R2. It supports system metadata, storage class, and SSE-C.

```API
CreateMultipartUpload
```

--------------------------------

### Fix Errors with Prefix Object Deletion

Source: https://developers.cloudflare.com/r2/platform/release-notes

Addresses a bug where deleting an object and then another object that is a prefix of the first could lead to errors. This ensures correct object deletion behavior.

```S3 API
Deleting object with prefix object
```

--------------------------------

### S3 ListMultipartUpload Key Encoding

Source: https://developers.cloudflare.com/r2/platform/release-notes

ListMultipartUpload correctly encodes the returned Key when the encoding-type is specified, ensuring proper handling of keys with special characters.

```text
`ListMultipartUpload` correctly encodes the returned `Key` if the `encoding-type` is specified.
```

--------------------------------

### Fix Key Name Decoding in S3 API

Source: https://developers.cloudflare.com/r2/platform/release-notes

Corrects a regression where key names were not properly decoded when using the S3 API, ensuring accurate key handling.

```S3 API
Key name decoding in S3 API
```

--------------------------------

### S3 DeleteObjects Batch Size

Source: https://developers.cloudflare.com/r2/platform/release-notes

The S3 DeleteObjects operation can now handle up to 1000 objects at a time, improving efficiency for batch deletions.

```text
`DeleteObjects` can now handle 1000 objects at a time.
```

--------------------------------

### Handle File Upload in Worker (TypeScript)

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

TypeScript code for a Cloudflare Worker to handle file uploads. It checks for POST requests to '/api/upload', extracts the file from the request, and uploads it to a Cloudflare R2 bucket using the Workers API. It returns a 404 for other requests.

```typescript
import { Hono } from 'hono'
import { serveStatic } from 'hono/serve-static'
import { Bindings } from './bindings'

const app = new Hono<Bindings>()

app.get('/', serveStatic({ root: './' }))

app.post('/api/upload', async (c) => {
  const file = await c.req.parseFile('pdfFile')
  await c.env.MY_BUCKET.put(file.name, file.content)
  return c.text('File uploaded successfully!')
})

app.get('*', (c) => {
  return c.text('Not Found', 404)
})

export default {
  async fetch(request: Request, env: Env, ctx: ExecutionContext) {
    return app.fetch(request, env, ctx)
  },
}

```

--------------------------------

### Configure Multipart Upload Part Size in Rclone

Source: https://developers.cloudflare.com/r2/examples/rclone

Explains how to adjust Rclone's multipart upload behavior using CLI arguments like `--s3-chunk-size` and `--s3-upload-cutoff` to optimize performance and cost.

```bash
--s3-chunk-size <size>
```

```bash
--s3-upload-cutoff <size>
```

--------------------------------

### Handle File Upload with TypeScript Worker

Source: https://developers.cloudflare.com/r2/llms-full

This TypeScript code defines a Cloudflare Worker's fetch handler to manage file uploads. It checks for POST requests to '/api/upload', retrieves a file from the form data, and uploads it to a configured R2 bucket using `env.MY_BUCKET.put()`.

```ts
export default {
  async fetch(request, env, ctx): Promise<Response> {
    // Get the pathname from the request
    const pathname = new URL(request.url).pathname;


    if (pathname === "/api/upload" && request.method === "POST") {
      // Get the file from the request
      const formData = await request.formData();
      const file = formData.get("pdfFile") as File;


      // Upload the file to Cloudflare R2
      const upload = await env.MY_BUCKET.put(file.name, file);
      return new Response("File uploaded successfully", { status: 200 });
    }


    return new Response("incorrect route", { status: 404 });
  },
} satisfies ExportedHandler<Env>;
```

--------------------------------

### Configure R2 Credentials in Postman

Source: https://developers.cloudflare.com/r2/tutorials/postman

Set environment variables in Postman to authenticate with Cloudflare R2. This includes your account ID, R2 Access Key ID, and R2 Secret Access Key. These variables are crucial for making authenticated requests to the R2 platform.

```Postman
Set the `account-id` variable to your Cloudflare account ID.
Set the `r2-access-key-id` variable to your R2 API token's Access Key ID.
Set the `r2-secret-access-key` variable to your R2 API token's Secret Access Key.
```

--------------------------------

### DMARC Email Worker for Analytics

Source: https://developers.cloudflare.com/r2/demos

This Cloudflare worker script processes incoming DMARC reports, stores them, and generates analytics. It demonstrates how to leverage Workers for data processing and storage in R2.

```javascript
// Cloudflare Worker script to process DMARC reports.
// Assumes DMARC reports are sent to an R2 bucket or processed via a Worker route.
// This example focuses on receiving and storing the report data.

// In a real scenario, you'd likely receive reports via email or a webhook.
// This example simulates receiving a report payload.

// Assume 'env' contains your R2 bucket binding: env.YOUR_R2_BUCKET

async function processDmarcReport(reportData, filename) {
  // Parse the DMARC report (typically XML format)
  // For simplicity, we'll assume reportData is already parsed or can be directly put.
  // In a real implementation, you would parse the XML.

  try {
    // Store the raw report or parsed data in R2
    await env.YOUR_R2_BUCKET.put(filename, reportData);
    console.log(`Stored DMARC report: ${filename}`);

    // Further processing for analytics could happen here,
    // e.g., sending data to a database or another service.

    return { success: true, filename: filename };
  } catch (error) {
    console.error(`Error storing DMARC report ${filename}:`, error);
    return { success: false, error: error.message };
  }
}

// Example of how this might be called within a Worker's fetch handler:
/*
export default {
  async fetch(request, env, ctx) {
    if (request.method === 'POST') {
      const reportContent = await request.text(); // Or request.formData() if sent that way
      const reportFilename = `dmarc-reports/${Date.now()}.xml`; // Example filename

      const result = await processDmarcReport(reportContent, reportFilename);

      if (result.success) {
        return new Response(`DMARC report ${result.filename} processed.`, { status: 200 });
      } else {
        return new Response(`Failed to process DMARC report: ${result.error}`, { status: 500 });
      }
    }
    return new Response('Method not allowed', { status: 405 });
  }
}
*/

```

--------------------------------

### Fix Repeated AbortMultipartUpload Errors

Source: https://developers.cloudflare.com/r2/platform/release-notes

Resolves an issue where the S3 API AbortMultipartUpload operation would throw an error if called multiple times. This ensures reliable management of multipart uploads.

```S3 API
AbortMultipartUpload operation
```

--------------------------------

### Query R2 Bucket Storage

Source: https://developers.cloudflare.com/r2/llms-full

This GraphQL query retrieves R2 storage information, including object count and payload size, over a given time period, optionally filtered by bucket name. Results are ordered by datetime in descending order.

```graphql
query R2StorageExample(
  $accountTag: string!
  $startDate: Time
  $endDate: Time
  $bucketName: string
) {
  viewer {
    accounts(filter: { accountTag: $accountTag }) {
      r2StorageAdaptiveGroups(
        limit: 10000
        filter: {
          datetime_geq: $startDate
          datetime_leq: $endDate
          bucketName: $bucketName
        }
        orderBy: [datetime_DESC]
      ) {
        max {
          objectCount
          uploadCount
          payloadSize
          metadataSize
        }
        dimensions {
          datetime
        }
      }
    }
  }
}
```

--------------------------------

### S3 Conditional Headers Support

Source: https://developers.cloudflare.com/r2/llms-full

The `If-Match` / `If-None-Match` headers now support arrays of ETags, Weak ETags, and wildcards (`*`) as per HTTP standards and undocumented AWS S3 behavior. This enhances conditional request capabilities.

```http
PUT /my-object HTTP/1.1
Host: example.com
If-Match: "etag1", "etag2", "*"
...
```

--------------------------------

### S3 CreateBucket Object Lock Flag

Source: https://developers.cloudflare.com/r2/platform/release-notes

S3 CreateBucket requests can now specify `x-amz-bucket-object-lock-enabled` with a value of `false` without being rejected. A value of `true` is still rejected as R2 does not support object locks.

```text
S3 `CreateBucket` request can specify `x-amz-bucket-object-lock-enabled` with a value of `false` and not have the requested rejected with a `NotImplemented` error. A value of `true` will continue to be rejected as R2 does not yet support object locks.
```

--------------------------------

### Set Content-Type Header for S3 API

Source: https://developers.cloudflare.com/r2/platform/release-notes

Ensures that the S3 API returns the correct Content-Type: application/xml response header on relevant endpoints, improving response consistency.

```S3 API
Content-Type: application/xml header
```

--------------------------------

### Cloudflare R2 S3 API: Signing Headers and CORS Preflight

Source: https://developers.cloudflare.com/r2/platform/release-notes

Resolves an issue where signing additional headers could break CORS preflight requests for presigned URLs. This ensures that presigned URLs function correctly with CORS enabled.

```http
GET /my-presigned-url?X-Amz-Signature=... HTTP/1.1
Host: your-bucket.s3.amazonaws.com
Origin: https://example.com


```

--------------------------------

### Query R2 Operations Volume (GraphQL)

Source: https://developers.cloudflare.com/r2/platform/metrics-analytics

This GraphQL query retrieves the volume of each operation type performed on a specified R2 bucket within a given time period. The `bucketName` can be omitted for an account-wide overview. Additional dimensions can be added for more granular breakdowns.

```GraphQL
query (
  $ கிராம்: DateTime!
  $to: DateTime!
  $accountTag: String!
) {
  viewer {
    r2OperationsAdaptiveGroups(
      accountTag: $accountTag
      filter: {
        datetime: { from: $from, to: $to }
      }
      # Example: Group by actionType
      # Example: Group by bucketName
      # Example: Group by actionStatus
      # Example: Group by objectName
      # Example: Group by responseStatusCode
      # Example: Group by jurisdiction
      # Example: Group by bucketName, actionType
      # Example: Group by bucketName, actionStatus
      # Example: Group by bucketName, objectName
      # Example: Group by bucketName, responseStatusCode
      # Example: Group by bucketName, jurisdiction
      # Example: Group by actionType, actionStatus
      # Example: Group by actionType, objectName
      # Example: Group by actionType, responseStatusCode
      # Example: Group by actionType, jurisdiction
      # Example: Group by actionStatus, objectName
      # Example: Group by actionStatus, responseStatusCode
      # Example: Group by actionStatus, jurisdiction
      # Example: Group by objectName, responseStatusCode
      # Example: Group by objectName, jurisdiction
      # Example: Group by responseStatusCode, jurisdiction
      # Example: Group by bucketName, actionType, actionStatus
      # Example: Group by bucketName, actionType, objectName
      # Example: Group by bucketName, actionType, responseStatusCode
      # Example: Group by bucketName, actionType, jurisdiction
      # Example: Group by bucketName, actionStatus, objectName
      # Example: Group by bucketName, actionStatus, responseStatusCode
      # Example: Group by bucketName, actionStatus, jurisdiction
      # Example: Group by bucketName, objectName, responseStatusCode
      # Example: Group by bucketName, objectName, jurisdiction
      # Example: Group by bucketName, responseStatusCode, jurisdiction
      # Example: Group by actionType, actionStatus, objectName
      # Example: Group by actionType, actionStatus, responseStatusCode
      # Example: Group by actionType, actionStatus, jurisdiction
      # Example: Group by actionType, objectName, responseStatusCode
      # Example: Group by actionType, objectName, jurisdiction
      # Example: Group by actionType, responseStatusCode, jurisdiction
      # Example: Group by actionStatus, objectName, responseStatusCode
      # Example: Group by actionStatus, objectName, jurisdiction
      # Example: Group by actionStatus, responseStatusCode, jurisdiction
      # Example: Group by objectName, responseStatusCode, jurisdiction
      # Example: Group by bucketName, actionType, actionStatus, objectName
      # Example: Group by bucketName, actionType, actionStatus, responseStatusCode
      # Example: Group by bucketName, actionType, actionStatus, jurisdiction
      # Example: Group by bucketName, actionType, objectName, responseStatusCode
      # Example: Group by bucketName, actionType, objectName, jurisdiction
      # Example: Group by bucketName, actionType, responseStatusCode, jurisdiction
      # Example: Group by bucketName, actionStatus, objectName, responseStatusCode
      # Example: Group by bucketName, actionStatus, objectName, jurisdiction
      # Example: Group by bucketName, actionStatus, responseStatusCode, jurisdiction
      # Example: Group by bucketName, objectName, responseStatusCode, jurisdiction
      # Example: Group by actionType, actionStatus, objectName, responseStatusCode
      # Example: Group by actionType, actionStatus, objectName, jurisdiction
      # Example: Group by actionType, actionStatus, responseStatusCode, jurisdiction
      # Example: Group by actionType, objectName, responseStatusCode, jurisdiction
      # Example: Group by actionStatus, objectName, responseStatusCode, jurisdiction
      # Example: Group by bucketName, actionType, actionStatus, objectName, responseStatusCode
      # Example: Group by bucketName, actionType, actionStatus, objectName, jurisdiction
      # Example: Group by bucketName, actionType, actionStatus, responseStatusCode, jurisdiction
      # Example: Group by bucketName, actionType, objectName, responseStatusCode, jurisdiction
      # Example: Group by bucketName, actionStatus, objectName, responseStatusCode, jurisdiction
      # Example: Group by actionType, actionStatus, objectName, responseStatusCode, jurisdiction
      # Example: Group by bucketName, actionType, actionStatus, objectName, responseStatusCode, jurisdiction
      groups: [bucketName, actionType]
    ) {
      sum {
        count
      }
      dimensions {
        bucketName
        actionType
      }
    }
  }
}
```

--------------------------------

### Generate Presigned URLs for R2

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to generate presigned URLs for temporary read (GetObject) and write (PutObject) access to objects in a Cloudflare R2 bucket using the AWS SDK for JavaScript. The `expiresIn` property controls the URL's validity period.

```ts
import { getSignedUrl } from "@aws-sdk/s3-request-presigner";


// Use the expiresIn property to determine how long the presigned link is valid.
console.log(
  await getSignedUrl(
    S3,
    new GetObjectCommand({ Bucket: "my-bucket-name", Key: "dog.png" }),
    { expiresIn: 3600 },
  ),
);
// https://my-bucket-name.<accountid>.r2.cloudflarestorage.com/dog.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential<credential>&X-Amz-Date=<timestamp>&X-Amz-Expires=3600&X-Amz-Signature=<signature>&X-Amz-SignedHeaders=host&x-id=GetObject


// You can also create links for operations such as putObject to allow temporary write access to a specific key.
console.log(
  await getSignedUrl(
    S3,
    new PutObjectCommand({ Bucket: "my-bucket-name", Key: "dog.png" }),
    { expiresIn: 3600 },
  ),
);
```

--------------------------------

### R2 Ranged Reads

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Demonstrates how to use the `range` parameter in `R2GetOptions` to retrieve specific byte ranges of an object. Supports offset with optional length, optional offset with length, and suffix.

```javascript
{
  range: {
    offset: 100,
    length: 50
  }
}

{
  range: {
    offset: 100
  }
}

{
  range: {
    length: 50
  }
}

{
  range: {
    suffix: 50
  }
}
```

--------------------------------

### Set Custom Headers with Python (boto3)

Source: https://developers.cloudflare.com/r2/llms-full

This Python code snippet demonstrates how to add custom headers to S3 PutObject requests using the boto3 library. It registers a callback function to update request parameters with custom headers before the request is built.

```python
import boto3


# Assuming 'client' is an initialized boto3 S3 client
# client = boto3.client('s3')

def add_custom_headers(params, context, **kwargs):
    if (custom_headers := context.get("custom_headers")):
        params["headers"].update(custom_headers)


# Register the function to be called before parameter building for S3 PutObject
# event_system.register('before-parameter-build.s3.PutObject', add_custom_headers)

# Example usage:
custom_headers = {'If-Match' : '"29d911f495d1ba7cb3a4d7d15e63236a"'}
# response = client.put_object(Bucket="my_bucket", Key="my_key", Body="file_contents", custom_headers=custom_headers)
# print(response)
```

--------------------------------

### R2 Worker Bindings: Conditional Headers Parsing

Source: https://developers.cloudflare.com/r2/llms-full

Enhances the R2 worker bindings to correctly parse conditional headers that may contain multiple ETags, including strong, weak, or wildcard ETags.

```JavaScript
export default {
	async fetch(request, env) {
		// Example: Checking conditional headers like If-Match or If-None-Match
		// const ifMatchHeader = request.headers.get("if-match");
		// if (ifMatchHeader) {
		// 	// The bindings now support parsing headers with multiple ETags,
		// 	// e.g., ""etag1", "etag2", "*"
		// 	console.log("Parsed If-Match header:", ifMatchHeader);
		// }
		return new Response("Conditional headers processed.");
	}
}
```

--------------------------------

### Generate Presigned URL for R2 Object

Source: https://developers.cloudflare.com/r2/llms-full

Generate a presigned URL for a Cloudflare R2 object using the AWS CLI. This allows temporary public access to the file. The `--expires-in` flag controls the URL's validity period.

```shell
aws s3 presign --endpoint-url https://<accountid>.r2.cloudflarestorage.com  s3://sdk-example/ferriswasm.png --expires-in 3600
```

--------------------------------

### List Bucket Lock Rules via Wrangler

Source: https://developers.cloudflare.com/r2/buckets/bucket-locks

Retrieves a list of all configured bucket lock rules for an R2 bucket using the Wrangler CLI. This helps in managing and auditing existing retention policies.

```bash
r2 bucket lock list
```

--------------------------------

### S3 Encryption Support

Source: https://developers.cloudflare.com/r2/llms-full

Supports `GetBucketEncryption`, `PutBucketEncryption`, and `DeleteBucketEncryption`. The only supported encryption value is `AES256`.

```bash
curl -X PUT \
  -H "Content-Type: application/xml" \
  http://<bucket-name>.r2.cloudflarestorage.com/?encryption \
  --data-binary @encryption.xml
```

--------------------------------

### Configure R2 Credentials in Postman

Source: https://developers.cloudflare.com/r2/llms-full

Set essential variables like 'account-id', 'r2-access-key-id', and 'r2-secret-access-key' in the Postman dashboard to authenticate with Cloudflare R2. These variables are crucial for interacting with the R2 platform.

```Postman
Set 'account-id' to your Cloudflare account ID.
Set 'r2-access-key-id' to your R2 Access Key ID.
Set 'r2-secret-access-key' to your R2 Secret Access Key.
```

--------------------------------

### Alias Empty String and us-east-1 to 'auto' Region

Source: https://developers.cloudflare.com/r2/platform/release-notes

For compatibility with external tools, the S3 API now aliases an empty string and 'us-east-1' to the 'auto' region.

```S3 API
Region aliasing ('', 'us-east-1' to 'auto')
```

--------------------------------

### Require Leading Slash for S3 CopyObject Source

Source: https://developers.cloudflare.com/r2/platform/release-notes

Enforces the requirement for a leading slash in the source parameter for the S3 API CopyObject operation, ensuring correct object referencing.

```S3 API
CopyObject source parameter leading slash
```

--------------------------------

### Cloudflare R2 S3 API: PutBucketCors Rejection

Source: https://developers.cloudflare.com/r2/platform/release-notes

Ensures that the `PutBucketCors` operation in the S3 API correctly rejects requests containing unknown keys in the XML body, maintaining the integrity of the CORS configuration.

```xml
<CORSConfiguration>
  <CORSRule>
    <AllowedOrigin>*</AllowedOrigin>
    <AllowedMethod>GET</AllowedMethod>
    <ExposeHeader>Content-Length</ExposeHeader>
    <AllowedHeader>Authorization</AllowedHeader>
    <MaxAgeSeconds>3000</MaxAgeSeconds>
    <UnknownKey>InvalidValue</UnknownKey>
  </CORSRule>
</CORSConfiguration>

```

--------------------------------

### Fix MalformedXML Error for DeleteObjects

Source: https://developers.cloudflare.com/r2/platform/release-notes

Corrects the S3 API DeleteObjects request to return a MalformedXML error instead of InternalError when provided with more than 128 keys, improving error reporting for batch deletions.

```S3 API
DeleteObjects request (MalformedXML error for >128 keys)
```

--------------------------------

### Cloudflare R2 S3 API: Conditional CopyObject

Source: https://developers.cloudflare.com/r2/platform/release-notes

Explains the support for Cloudflare-specific headers in the S3 compatible API's `CopyObject` operation. These headers allow the copy operation to be conditional on the state of the destination object, enhancing data integrity.

```http
PUT /destination-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com
Date: Tue, 27 Oct 2023 10:00:00 GMT
Authorization: AWS AKIAIOSFODNN7EXAMPLE:xR4q1+q...=
CopySource: /your-bucket/source-object
If-Match: "a1b2c3d4e5f6"
X-Amz-Copy-Source-If-None-Match: "*"


```

--------------------------------

### Generate Presigned URL with AWS SDK for JavaScript

Source: https://developers.cloudflare.com/r2/buckets/cors

This snippet demonstrates how to generate a presigned `PutObject` URL using the `@aws-sdk/client-s3` package in JavaScript. This URL can be used to upload objects to an R2 bucket without exposing sensitive credentials.

```javascript
import { S3Client, PutObjectCommand } from "@aws-sdk/client-s3";

const s3Client = new S3Client({
  region: "auto",
  endpoint: `https://${process.env.ACCOUNT_ID}.r2.cloudflarestorage.com`, 
  credentials: {
    accessKeyId: process.env.ACCESS_KEY_ID,
    secretAccessKey: process.env.SECRET_ACCESS_KEY,
  },
});

const uploadFile = async (fileContent) => {
  const command = new PutObjectCommand({
    Bucket: "your-bucket-name",
    Key: "your-object-key",
    Body: fileContent,
    ContentType: "text/plain",
  });

  try {
    const url = await s3Client.send(command);
    console.log("Successfully uploaded file. URL:", url);
    return url;
  } catch (error) {
    console.error("Error uploading file:", error);
    return null;
  }
};

// Example usage:
// uploadFile("This is the content of the file.");
```

--------------------------------

### Support Unchunked Signed Payloads in S3 API

Source: https://developers.cloudflare.com/r2/platform/release-notes

Adds support for unchunked signed payloads in the S3 API, improving compatibility with various client implementations.

```S3 API
Unchunked signed payloads
```

--------------------------------

### Conditional Uploads with `PutObject`

Source: https://developers.cloudflare.com/r2/llms-full

The `PutObject` operation in Cloudflare R2 supports conditional uploads using standard HTTP headers like `If-Match`, `If-None-Match`, `If-Modified-Since`, and `If-Unmodified-Since`. These headers allow you to control uploads based on the object's current state, preventing unintended overwrites or ensuring updates only occur when specific conditions are met. If conditions are not met, the operation returns a `412 PreconditionFailed` error.

--------------------------------

### Cloudflare Worker for R2 Multipart Uploads

Source: https://developers.cloudflare.com/r2/llms-full

This Worker handles multipart uploads to a Cloudflare R2 bucket. It supports creating multipart uploads, uploading parts, completing uploads, aborting uploads, and retrieving objects. It routes requests based on HTTP method and action parameters.

```typescript
interface Env {
  MY_BUCKET: R2Bucket;
}


export default {
  async fetch(
    request,
    env,
    ctx
  ): Promise<Response> {
    const bucket = env.MY_BUCKET;


    const url = new URL(request.url);
    const key = url.pathname.slice(1);
    const action = url.searchParams.get("action");


    if (action === null) {
      return new Response("Missing action type", { status: 400 });
    }


    // Route the request based on the HTTP method and action type
    switch (request.method) {
      case "POST":
        switch (action) {
          case "mpu-create": {
            const multipartUpload = await bucket.createMultipartUpload(key);
            return new Response(
              JSON.stringify({
                key: multipartUpload.key,
                uploadId: multipartUpload.uploadId,
              })
            );
          }
          case "mpu-complete": {
            const uploadId = url.searchParams.get("uploadId");
            if (uploadId === null) {
              return new Response("Missing uploadId", { status: 400 });
            }


            const multipartUpload = env.MY_BUCKET.resumeMultipartUpload(
              key,
              uploadId
            );


            interface completeBody {
              parts: R2UploadedPart[];
            }
            const completeBody: completeBody = await request.json();
            if (completeBody === null) {
              return new Response("Missing or incomplete body", {
                status: 400,
              });
            }


            // Error handling in case the multipart upload does not exist anymore
            try {
              const object = await multipartUpload.complete(completeBody.parts);
              return new Response(null, {
                headers: {
                  etag: object.httpEtag,
                },
              });
            } catch (error: any) {
              return new Response(error.message, { status: 400 });
            }
          }
          default:
            return new Response(`Unknown action ${action} for POST`, {
              status: 400,
            });
        }
      case "PUT":
        switch (action) {
          case "mpu-uploadpart": {
            const uploadId = url.searchParams.get("uploadId");
            const partNumberString = url.searchParams.get("partNumber");
            if (partNumberString === null || uploadId === null) {
              return new Response("Missing partNumber or uploadId", {
                status: 400,
              });
            }
            if (request.body === null) {
              return new Response("Missing request body", { status: 400 });
            }


            const partNumber = parseInt(partNumberString);
            const multipartUpload = env.MY_BUCKET.resumeMultipartUpload(
              key,
              uploadId
            );
            try {
              const uploadedPart: R2UploadedPart =
                await multipartUpload.uploadPart(partNumber, request.body);
              return new Response(JSON.stringify(uploadedPart));
            } catch (error: any) {
              return new Response(error.message, { status: 400 });
            }
          }
          default:
            return new Response(`Unknown action ${action} for PUT`, {
              status: 400,
            });
        }
      case "GET":
        if (action !== "get") {
          return new Response(`Unknown action ${action} for GET`, {
            status: 400,
          });
        }
        const object = await env.MY_BUCKET.get(key);
        if (object === null) {
          return new Response("Object Not Found", { status: 404 });
        }
        const headers = new Headers();
        object.writeHttpMetadata(headers);
        headers.set("etag", object.httpEtag);
        return new Response(object.body, { headers });
      case "DELETE":
        switch (action) {
          case "mpu-abort": {
            const uploadId = url.searchParams.get("uploadId");
            // The rest of the abort logic would go here...
            return new Response("Abort logic not fully implemented", { status: 501 });
          }
          default:
            return new Response(`Unknown action ${action} for DELETE`, {
              status: 400,
            });
        }
      default:
        return new Response(`Unsupported method ${request.method}`, {
          status: 405,
        });
    }
  },
};
```

--------------------------------

### Set custom header for all PutObject requests with boto3

Source: https://developers.cloudflare.com/r2/llms-full

This Python code configures boto3 to automatically add the 'cf-create-bucket-if-missing: true' header to all PutObject requests made to Cloudflare R2. It utilizes boto3's event system to intercept and modify outgoing requests.

```python
import boto3


client = boto3.resource('s3',
  endpoint_url = 'https://<accountid>.r2.cloudflarestorage.com',
  aws_access_key_id = '<access_key_id>',
  aws_secret_access_key = '<access_key_secret>'
)


event_system = client.meta.events


# Define function responsible for adding the header
def add_custom_header(params, **kwargs):
    params["headers"]['cf-create-bucket-if-missing'] = 'true'


event_system.register('before-call.s3.PutObject', add_custom_header)


response = client.put_object(Bucket="my_bucket", Key="my_file", Body="file_contents")
print(response)
```

--------------------------------

### R2MultipartUpload Definition

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Represents an ongoing multipart upload in R2. It includes the key and upload ID, and provides methods to upload parts, abort the upload, and complete the upload with uploaded parts.

```typescript
interface R2MultipartUpload {
  key: string;
  uploadId: string;
  uploadPart(partNumber: number, value: ReadableStream | ArrayBuffer | ArrayBufferView | string | Blob, options?: R2MultipartOptions): Promise<R2UploadedPart>;
  abort(): Promise<void>;
  complete(uploadedParts: R2UploadedPart[]): Promise<R2Object>;
}
```

--------------------------------

### CopyObject - Cloudflare R2

Source: https://developers.cloudflare.com/r2/llms-full

Copies an object to another location. Supports various operation metadata, system metadata, conditional operations, storage classes, and SSE-C. Does not support ACLs, website redirects, or request payer.

```bash
aws s3api copy-object --bucket DEST_BUCKET --key DEST_KEY --copy-source SOURCE_BUCKET/SOURCE_KEY --metadata-directive REPLACE --cache-control "max-age=3600" --content-type "text/plain" --storage-class STANDARD --sse-customer-algorithm AES256 --sse-customer-key YOUR_KEY --sse-customer-key-md5 YOUR_KEY_MD5
```

--------------------------------

### Improve S3 GetObject Ranges and Partial Reads

Source: https://developers.cloudflare.com/r2/platform/release-notes

Enhances the S3 API GetObject operation by making byte ranges inclusive and ensuring partial reads return a 206 Partial Content response code. This improves byte-level access to objects.

```S3 API
GetObject ranges (inclusive)
Partial reads (206 Partial Content)
```

--------------------------------

### Upload an Object to Cloudflare R2 using Postman

Source: https://developers.cloudflare.com/r2/llms-full

Upload an object (e.g., an image file) to an R2 bucket using the 'PUT PutObject' request in Postman. Ensure the 'r2-object' variable is set and the file is attached in binary format.

```Postman
Set 'r2-object' variable to the desired object name (e.g., 'cat-pic.jpg').
Navigate to Cloudflare R2 folder > Objects folder > Multipart folder > PUT PutObject.
In the Body tab, select 'binary' and attach the file.
Select Send.
Expected response: 200 OK.
```

--------------------------------

### Write DataFrame to Iceberg Table (Python)

Source: https://developers.cloudflare.com/r2/llms-full

This code snippet demonstrates creating a simple Spark DataFrame and writing it to the previously defined Iceberg table. It uses the 'iceberg' format and 'append' mode for writing.

```Python
# Create a simple DataFrame
df = spark.createDataFrame(
    [(1, "Alice"), (2, "Bob"), (3, "Charlie")],
    ["id", "name"]
)


# Write the DataFrame to the Iceberg table
df.write \
    .format("iceberg") \
    .mode("append") \
    .save("default.my_table")
```

--------------------------------

### S3 DeleteObjects Capacity

Source: https://developers.cloudflare.com/r2/llms-full

The `DeleteObjects` operation can now handle up to 1000 objects at a time, improving efficiency for bulk deletions.

```bash
curl -X POST \
  -H "Content-Type: application/xml" \
  http://<bucket-name>.r2.cloudflarestorage.com/?delete \
  --data-binary @delete-objects.xml
```

--------------------------------

### Import unpdf Modules in TypeScript

Source: https://developers.cloudflare.com/r2/llms-full

Imports the necessary functions 'extractText' and 'getDocumentProxy' from the 'unpdf' library into a TypeScript file.

```typescript
import { extractText, getDocumentProxy } from "unpdf";
```

--------------------------------

### S3 Multipart Upload ETags

Source: https://developers.cloudflare.com/r2/llms-full

Multipart uploads now include a `-N` suffix on the ETag, representing the number of parts the file was published with.

```bash
curl -I http://<bucket-name>.r2.cloudflarestorage.com/<object-key>
```

--------------------------------

### Head Bucket - S3 API Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

Details the 'HeadBucket' operation for the S3 API in Cloudflare R2. While the core operation is implemented, specific features like 'Bucket Owner' and 'x-amz-expected-bucket-owner' are not.

```S3 API
HeadBucket
```

--------------------------------

### Merge Metadata Directive in CopyObject (R2)

Source: https://developers.cloudflare.com/r2/api/s3/extensions

Cloudflare R2 extends the `x-amz-metadata-directive` for `CopyObject` operations to include a `MERGE` value. This directive copies all metadata from the source object and replaces only the metadata keys specified in the request, offering a more granular control than `COPY` or `REPLACE`.

```HTTP
COPY /source-object HTTP/1.1
Host: your-bucket.your-account.r2.cloudflarestorage.com
Destination: /destination-object
X-Amz-Metadata-Directive: MERGE
X-Amz-Meta-Custom-Header: new-value


```

--------------------------------

### Cloudflare R2 S3 API: Multipart Upload Part Size Enforcement

Source: https://developers.cloudflare.com/r2/platform/release-notes

Changes the enforcement of multipart upload part sizes. Instead of checking on every part upload, the enforcement is now done only when the upload is completed, improving performance.

```http
POST /my-object?uploads HTTP/1.1
Host: your-bucket.s3.amazonaws.com

POST /my-object?uploadId=EXAMPLEID&partNumber=1&entitySize=1048576 HTTP/1.1
Host: your-bucket.s3.amazonaws.com

POST /my-object?uploadId=EXAMPLEID&complete HTTP/1.1
Host: your-bucket.s3.amazonaws.com


```

--------------------------------

### API: Fix ListBuckets name_contains Parameter

Source: https://developers.cloudflare.com/r2/platform/release-notes

Corrects an issue with the `name_contains` parameter in the `ListBuckets` API action. Previously, it would incorrectly search within jurisdiction names, leading to inaccurate results.

```bash
GET /?name_contains=my-bucket
```

--------------------------------

### S3 Conditional Headers

Source: https://developers.cloudflare.com/r2/llms-full

When conditional headers are provided to S3 API `UploadObject` or `CreateMultipartUpload` operations, a `412 Precondition Failed` status code is returned if the conditions are not met.

```bash
curl -X PUT \
  -H "If-Match: \"aRandomETag\"" \
  http://<bucket-name>.r2.cloudflarestorage.com/<object-key>
```

--------------------------------

### Cloudflare R2 S3 API: HeadObject CORS Rendering

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixes the CORS rendering for the S3 `HeadObject` operation, ensuring that CORS headers are correctly applied when retrieving object metadata.

```http
HEAD /my-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com
Origin: https://example.com

HTTP/1.1 200 OK
Access-Control-Allow-Origin: https://example.com
...
```

--------------------------------

### Cloudflare R2 S3 API: CORS Preflight Fix

Source: https://developers.cloudflare.com/r2/platform/release-notes

Addresses a bug in the S3 API's handling of CORS preflight requests, enabling the use of the S3 SDK directly from web browsers. This ensures proper CORS configuration for cross-origin requests.

```http
OPTIONS /my-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com
Access-Control-Request-Method: GET
Access-Control-Request-Headers: Authorization
Origin: https://example.com


```

--------------------------------

### Write Queue Messages to R2 Bucket

Source: https://developers.cloudflare.com/r2/llms-full

This TypeScript code defines a Cloudflare Worker's queue handler. It processes batches of messages, serializes them into a JSON string, and uploads the content as a JSON file to a specified R2 bucket using the LOG_SINK binding.

```ts
export interface Env {
  LOG_SINK: R2Bucket;
}


export default {
  async queue(batch, env): Promise<void> {
    const batchId = new Date().toISOString().replace(/[:.]/g, "-");
    const fileName = `upload-logs-${batchId}.json`;


    // Serialize the entire batch of messages to JSON
    const fileContent = new TextEncoder().encode(
      JSON.stringify(batch.messages),
    );


    // Write the batch of messages to R2
    await env.LOG_SINK.put(fileName, fileContent, {
      httpMetadata: {
        contentType: "application/json",
      },
    });
  },
} satisfies ExportedHandler<Env>;
```

--------------------------------

### Set Custom Headers with JavaScript (aws-sdk-js-v3)

Source: https://developers.cloudflare.com/r2/llms-full

This JavaScript code snippet shows how to set custom headers for an S3 PutObject request using the aws-sdk-js-v3. It involves creating a middleware that adds the specified headers to the request before it is sent.

```javascript
import {
  PutObjectCommand,
  S3Client,
} from "@aws-sdk/client-s3";


// Assuming ACCOUNT_ID, ACCESS_KEY_ID, SECRET_ACCESS_KEY are defined
// const ACCOUNT_ID = 'YOUR_ACCOUNT_ID';
// const ACCESS_KEY_ID = 'YOUR_ACCESS_KEY_ID';
// const SECRET_ACCESS_KEY = 'YOUR_SECRET_ACCESS_KEY';


const client = new S3Client({
  region: "auto",
  endpoint: `https://${ACCOUNT_ID}.r2.cloudflarestorage.com`,
  credentials: {
    accessKeyId: ACCESS_KEY_ID,
    secretAccessKey: SECRET_ACCESS_KEY,
  },
});


const command = new PutObjectCommand({
  Bucket: "my_bucket",
  Key: "my_key",
  Body: "my_data"
});


const headers = { 'If-Match': '"29d911f495d1ba7cb3a4d7d15e63236a"' }
command.middlewareStack.add(
  (next) =>
    (args) => {
      const r = args.request as RequestInit


      Object.entries(headers).forEach(
        ([k, v]: [key: string, value: string]): void => {
          r.headers[k] = v
        },
      )


      return next(args)
    },
  { step: 'build', name: 'customHeaders' },
)
// const response = await client.send(command);
// console.log(response);
```

--------------------------------

### Handle Non-Encoded Parameters in S3 Authorization

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixes an issue where S3 API requests were not handling non-encoded parameters used for the authorization signature correctly. This ensures proper authentication.

```S3 API
Non-encoded parameters for authorization signature
```

--------------------------------

### AbortMultipartUpload - Cloudflare R2

Source: https://developers.cloudflare.com/r2/llms-full

Allows aborting a multipart upload. This operation does not support the 'x-amz-request-payer' header.

```bash
aws s3api abort-multipart-upload --bucket BUCKET --key KEY --upload-id UPLOAD_ID
```

--------------------------------

### Bind R2 Bucket to Worker (TOML)

Source: https://developers.cloudflare.com/r2/llms-full

This TOML configuration snippet demonstrates how to bind an R2 bucket to a Cloudflare Worker. It requires defining the `binding` as a JavaScript variable name and `bucket_name` as the name of the R2 bucket. This configuration enables the Worker to access and manage the specified R2 bucket.

```toml
[[r2_buckets]]
binding = 'MY_BUCKET' # <~ valid JavaScript variable name
bucket_name = '<YOUR_BUCKET_NAME>'
```

--------------------------------

### Check R2 Response Headers

Source: https://developers.cloudflare.com/r2/platform/troubleshooting

When encountering CORS errors with Cloudflare R2, inspect the response headers in your browser's developer tools. Look for `cf-cache-status` and `cf-mitigated` to diagnose the issue. The presence or absence of these headers indicates whether the request was blocked by WAF rules or Hotlink Protection.

```HTTP
cf-cache-status
cf-mitigated
```

--------------------------------

### R2Object Structure

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

An R2Object contains metadata about an object stored in Cloudflare R2. It includes an array of objects, a truncated flag, an optional cursor for pagination, and delimited prefixes if a delimiter was specified in the list request.

```javascript
{
  objects: Array<R2Object>,
  truncated: boolean,
  cursor?: string,
  delimitedPrefixes?: Array<string>
}
```

--------------------------------

### Handle Event Notifications in Worker (TypeScript)

Source: https://developers.cloudflare.com/r2/tutorials/upload-logs-event-notifications

TypeScript code for a Worker's `queue` handler. This function processes batches of event notification messages and writes them to a specified R2 log sink bucket.

```TypeScript
import { R2Bucket } from "@cloudflare/workers-types/r2";

interface Env {
  YOUR_BUCKET_BINDING: R2Bucket;
}

interface QueueMessage {
  key: string;
  bucket: string;
  event: string;
}

export default {
  async queue(batch: QueueMessage[], env: Env) {
    const logEntries = batch.map(message => JSON.stringify(message)).join('\n');
    await env.YOUR_BUCKET_BINDING.put(
      `logs/${Date.now()}.log`,
      logEntries
    );
  },
};

```

--------------------------------

### Implement DeleteObject and DeleteObjects

Source: https://developers.cloudflare.com/r2/llms-full

The DeleteObject and DeleteObjects operations in Cloudflare R2 are implemented. However, features like multi-factor authentication (MFA), object locking bypass, Request Payer, and Bucket Owner specific headers are not supported.

```S3 API
DeleteObject:
  Unsupported Features:
    x-amz-mfa: ❌
    x-amz-bypass-governance-retention: ❌
    x-amz-request-payer: ❌
    x-amz-expected-bucket-owner: ❌
DeleteObjects:
  Unsupported Features:
    x-amz-mfa: ❌
    x-amz-bypass-governance-retention: ❌
    x-amz-request-payer: ❌
    x-amz-expected-bucket-owner: ❌
```

--------------------------------

### Set Custom Header Per PutObject Request with boto3

Source: https://developers.cloudflare.com/r2/examples/aws/custom-header

This Python snippet illustrates how to set a custom header for individual `PutObject` requests using `boto3`. It involves a two-step process using `boto3`'s event system to bypass parameter validation and inject the header into the request context.

```python
import boto3

# Create an R2 client
client = boto3.client('s3', endpoint_url='https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com', aws_access_key_id='YOUR_ACCESS_KEY_ID', aws_secret_access_key='YOUR_SECRET_ACCESS_KEY')

# Function to move custom arguments to context before validation
def move_custom_args_to_context(request, **kwargs):
    if 'ExtraArgs' in request.context:
        request.context['custom_headers'] = request.context.get('ExtraArgs', {}).get('CustomHeaders', {})

# Function to add headers from context to the request
def add_headers_from_context(request, **kwargs):
    custom_headers = request.context.get('custom_headers', {})
    for key, value in custom_headers.items():
        request.headers[key] = value

# Register the functions to the event system
client.meta.events.register('before-call.s3.PutObject', move_custom_args_to_context)
client.meta.events.register('before-send.s3.PutObject', add_headers_from_context)

# Example of making a request with a custom header:
# try:
#     response = client.put_object(
#         Bucket='my-bucket',
#         Key='my-object',
#         Body=b'Hello, R2!',
#         CustomHeaders={'X-If-Match': 'some-etag-value'}
#     )
#     print('Object uploaded successfully with custom header!')
# except Exception as e:
#     print(f'Error uploading object: {e}')

```

--------------------------------

### S3 ListBuckets and S3 Actions - x-id Parameter

Source: https://developers.cloudflare.com/r2/llms-full

Cloudflare R2 now ignores the `x-id` query parameter for `S3 ListBuckets` and all other S3 actions to prevent potential inconsistencies or security issues.

```javascript
// When calling S3 ListBuckets, the 'x-id' parameter is ignored.
// Example: GET /?list-type=2&x-id=some-value

// Similarly, 'x-id' is ignored for all other S3 actions.
```

--------------------------------

### S3 UploadPart Error Code Fix

Source: https://developers.cloudflare.com/r2/platform/release-notes

Fixed a regression in the S3 API UploadPart operation where TooMuchConcurrency & NoSuchUpload errors were incorrectly returned as NoSuchBucket.

```text
Fixed a regression in the S3 API `UploadPart` operation where `TooMuchConcurrency` & `NoSuchUpload` errors were being returned as `NoSuchBucket`.
```

--------------------------------

### Filter R2 Bucket Notification by Suffix using Wrangler CLI

Source: https://developers.cloudflare.com/r2/llms-full

Command to create an R2 bucket notification rule with suffix filtering using the Wrangler CLI. This ensures notifications are sent only for objects ending with a specified suffix.

```sh
# Filter using suffix
$ npx wrangler r2 bucket notification create <BUCKET_NAME> --event-type <EVENT_TYPE> --queue <QUEUE_NAME> --suffix "<SUFFIX_VALUE>"
```

--------------------------------

### Put Bucket Lifecycle Configuration - S3 API Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

Outlines the 'PutBucketLifecycleConfiguration' operation for the S3 API in Cloudflare R2. Features related to checksums and bucket owner information are not implemented.

```S3 API
PutBucketLifecycleConfiguration
```

--------------------------------

### S3 DeleteObjects Key Trimming Fix

Source: https://developers.cloudflare.com/r2/llms-full

The S3 `DeleteObjects` operation no longer trims spaces from around keys before deletion. This resolves an issue where files with leading/trailing spaces could not be deleted, or incorrect files were deleted.

```javascript
// No direct code example, this is a behavior change in the S3 API implementation.
```

--------------------------------

### Conditional PutObject Operations (R2)

Source: https://developers.cloudflare.com/r2/api/s3/extensions

Cloudflare R2's `PutObject` operation supports conditional uploads using standard S3 headers like `If-Match`, `If-None-Match`, `If-Modified-Since`, and `If-Unmodified-Since`. If the object's current state does not meet the specified conditions, the operation will fail with a `412 PreconditionFailed` error.

```HTTP
PUT /my-object HTTP/1.1
Host: your-bucket.your-account.r2.cloudflarestorage.com
Content-Type: application/octet-stream
Content-Length: 1234
If-Match: "a1b2c3d4e5f6"

[...object data...]
```

--------------------------------

### Set custom header for all PutObject requests with aws-sdk-js-v3

Source: https://developers.cloudflare.com/r2/llms-full

This TypeScript code demonstrates how to add a custom header ('cf-create-bucket-if-missing: true') to all PutObject requests using the aws-sdk-js-v3. It leverages the SDK's middleware stack to intercept and modify requests before they are sent.

```typescript
import {
  PutObjectCommand,
  S3Client,
} from "@aws-sdk/client-s3";


const client = new S3Client({
  region: "auto",
  endpoint: `https://${ACCOUNT_ID}.r2.cloudflarestorage.com`,
  credentials: {
    accessKeyId: ACCESS_KEY_ID,
    secretAccessKey: SECRET_ACCESS_KEY,
  },
});


client.middlewareStack.add(
  (next, context) => async (args) => {
      const r = args.request as RequestInit
      r.headers["cf-create-bucket-if-missing"] = "true";


      return await next(args)
    },
  { step: 'build', name: 'customHeaders' },
)


const command = new PutObjectCommand({
  Bucket: "my_bucket",
  Key: "my_key",
  Body: "my_data"
});


const response = await client.send(command);


console.log(response);
```

--------------------------------

### S3 PutObject Content-Type Inference

Source: https://developers.cloudflare.com/r2/platform/release-notes

Uploads now automatically infer Content-Type based on file body if not explicitly set in PutObject request. This functionality is planned for multipart operations in the future.

```text
Uploads will automatically infer the `Content-Type` based on file body if one is not explicitly set in the `PutObject` request. This functionality will come to multipart operations in the future.
```

--------------------------------

### Cloudflare R2 S3 API: CORS Configuration Bug Fix

Source: https://developers.cloudflare.com/r2/platform/release-notes

Resolves a bug where a missing CORS configuration could result in a 403 Forbidden response. Now, requests without a matching CORS configuration will be handled more gracefully.

```http
GET /my-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com
Origin: https://unconfigured-origin.com

HTTP/1.1 403 Forbidden
...
```

--------------------------------

### Cloudflare R2 S3 API: Public Buckets Content-Range

Source: https://developers.cloudflare.com/r2/platform/release-notes

Prevents public buckets from returning the `Content-Range` response header unless the response is actually partial. This avoids confusion for non-ranged requests.

```http
GET /my-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com

HTTP/1.1 200 OK
Content-Type: application/octet-stream
Content-Length: 51200
...

GET /my-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com
Range: bytes=0-1023

HTTP/1.1 206 Partial Content
Content-Range: bytes 0-1023/51200
...
```

--------------------------------

### R2UploadedPart Structure

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Represents a part uploaded during a multipart upload. Contains the part number and its etag, used to complete the multipart upload.

```javascript
{
  partNumber: number,
  etag: string
}
```

--------------------------------

### Set custom header per PutObject request with boto3

Source: https://developers.cloudflare.com/r2/llms-full

This Python code shows how to set custom headers on a per-request basis using boto3 for Cloudflare R2. It involves registering two event handlers to move custom header arguments into the request context before parameter validation, allowing them to be applied to the outgoing request.

```python
import boto3


client = boto3.resource('s3',
  endpoint_url = 'https://<accountid>.r2.cloudflarestorage.com',
  aws_access_key_id = '<access_key_id>',
  aws_secret_access_key = '<access_key_secret>'
)


event_system = client.meta.events


# Moves the custom headers from the parameters to the request context
def process_custom_arguments(params, context, **kwargs):
    if (custom_headers := params.pop("custom_headers", None)):
        context["custom_headers"] = custom_headers



```

--------------------------------

### Change Object Storage Class with AWS CLI

Source: https://developers.cloudflare.com/r2/llms-full

Demonstrates how to change the storage class of an object in Cloudflare R2 from STANDARD to STANDARD_IA using the AWS CLI. This operation utilizes the `CopyObject` API and requires specifying the endpoint URL, bucket name, object key, source, and the desired storage class.

```sh
aws s3api copy-object \
  --endpoint-url https://<ACCONUT_ID>.r2.cloudflarestorage.com \
  --bucket bucket-name \
  --key path/to/object.txt \
  --copy-source /bucket-name/path/to/object.txt \
  --storage-class STANDARD_IA
```

--------------------------------

### Query R2 Bucket Operations Volume

Source: https://developers.cloudflare.com/r2/llms-full

This GraphQL query retrieves the sum of requests for R2 operations within a specified time period and optionally filtered by bucket name. It groups results by action type.

```graphql
query R2VolumeExample(
  $accountTag: string!
  $startDate: Time
  $endDate: Time
  $bucketName: string
) {
  viewer {
    accounts(filter: { accountTag: $accountTag }) {
      r2OperationsAdaptiveGroups(
        limit: 10000
        filter: {
          datetime_geq: $startDate
          datetime_leq: $endDate
          bucketName: $bucketName
        }
      ) {
        sum {
          requests
        }
        dimensions {
          actionType
        }
      }
    }
  }
}
```

--------------------------------

### Set Bucket Lock Configuration via Wrangler

Source: https://developers.cloudflare.com/r2/buckets/bucket-locks

Allows setting the entire bucket lock configuration for an R2 bucket using a JSON file with the Wrangler CLI. This method is useful for defining multiple rules or complex configurations.

```bash
r2 bucket lock set
```

--------------------------------

### R2Object Definition

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Defines the metadata for an object stored in Cloudflare R2. This includes properties like key, version, size, etag, upload time, HTTP metadata, custom metadata, range, checksums, storage class, and SSE-C key information.

```typescript
interface R2Object {
  key: string;
  version: string;
  size: number;
  etag: string;
  httpEtag: string;
  uploaded: Date;
  httpMetadata: R2HTTPMetadata;
  customMetadata: Record<string, string>;
  range: R2Range;
  checksums: R2Checksums;
  storageClass: 'Standard' | 'InfrequentAccess';
  ssecKeyMd5: string;
  writeHttpMetadata(headers: Headers): void;
}
```

--------------------------------

### Cloudflare R2 S3 API: GetObject Content-Range Header

Source: https://developers.cloudflare.com/r2/platform/release-notes

Modifies the `GetObject` operation so that the `Content-Range` response header is only returned for ranged requests. This prevents unnecessary headers for full object retrievals.

```http
GET /my-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com

HTTP/1.1 200 OK
Content-Type: application/octet-stream
Content-Length: 51200
...

GET /my-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com
Range: bytes=0-1023

HTTP/1.1 206 Partial Content
Content-Range: bytes 0-1023/51200
...
```

--------------------------------

### Support GetBucketEncryption, PutBucketEncryption, DeleteBucketEncryption

Source: https://developers.cloudflare.com/r2/platform/release-notes

Adds support for GetBucketEncryption, PutBucketEncryption, and DeleteBucketEncryption operations in the S3 API. Currently, only AES256 encryption is supported.

```S3 API
GetBucketEncryption
PutBucketEncryption
DeleteBucketEncryption (AES256)
```

--------------------------------

### Conditional Operations with R2Conditional

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Perform conditional operations on R2 objects by passing an R2Conditional object to R2GetOptions or R2PutOptions. Conditions include etagMatches, etagDoesNotMatch, uploadedBefore, and uploadedAfter.

```javascript
{
  etagMatches?: string,
  etagDoesNotMatch?: string,
  uploadedBefore?: Date,
  uploadedAfter?: Date
}
```

--------------------------------

### Bind R2 Bucket in Wrangler Configuration

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-usage

This demonstrates how to bind an R2 bucket to a Cloudflare Worker by adding configuration to the Wrangler file. It requires specifying a `binding` name and the `bucket_name` of the R2 bucket.

```json
{
  "version": "1",
  "name": "my-worker",
  "main": "src/index.js",
  "compatibility_flags": [
    "nodejs_compat"
  ],
  "env": {
    "production": {
      "routes": [
        {
          "pattern": "*",
          "zone_name": "example.com"
        }
      ]
    }
  },
  "routes": [
    {
      "pattern": "*",
      "zone_name": "example.com"
    }
  ],
  "r2_buckets": [
    {
      "binding": "MY_BUCKET",
      "bucket_name": "<YOUR_BUCKET_NAME>"
    }
  ]
}

```

--------------------------------

### Implement HeadObject with Conditional Operations and SSE-C

Source: https://developers.cloudflare.com/r2/llms-full

The HeadObject operation in Cloudflare R2 supports conditional operations (If-Match, If-Modified-Since, If-None-Match, If-Unmodified-Since) and Server-Side Encryption Customer-Provided (SSE-C) headers. It also supports the Range header, though it has no effect on HeadObject.

```S3 API
HeadObject:
  Conditional Operations:
    If-Match: ✅
    If-Modified-Since: ✅
    If-None-Match: ✅
    If-Unmodified-Since: ✅
  Range:
    Range: ✅
    partNumber: ✅
  SSE-C:
    x-amz-server-side-encryption-customer-algorithm: ✅
    x-amz-server-side-encryption-customer-key: ✅
    x-amz-server-side-encryption-customer-key-MD5: ✅
```

--------------------------------

### Extract Text from PDF in Cloudflare Worker

Source: https://developers.cloudflare.com/r2/llms-full

Processes messages from a queue, retrieves PDF files from an R2 bucket, extracts their textual content using 'unpdf', and logs a portion of the extracted text.

```typescript
async queue(batch, env) {
  for(let message of batch.messages) {
    console.log(`Processing file: ${message.body.object.key}`);
    // Get the file from the R2 bucket
    const file = await env.MY_BUCKET.get(message.body.object.key);
    if (!file) {
        console.error(`File not found: ${message.body.object.key}`);
        continue;
      }
    // Extract the textual content from the PDF
    const buffer = await file.arrayBuffer();
    const document = await getDocumentProxy(new Uint8Array(buffer));


    const {text} = await extractText(document, {mergePages: true});
    console.log(`Extracted text: ${text.substring(0, 100)}...`);
    }
}
```

--------------------------------

### Disable R2 Data Catalog via Wrangler CLI

Source: https://developers.cloudflare.com/r2/data-catalog/manage-catalogs

Disables the R2 Data Catalog on a specified bucket using the Wrangler CLI. This action immediately stops catalog interface requests and makes existing Iceberg table references inaccessible.

```bash
r2 bucket catalog disable command
```

--------------------------------

### Configure R2 with Terraform AWS Provider

Source: https://developers.cloudflare.com/r2/llms-full

This HCL code configures the Terraform AWS provider to interact with Cloudflare R2. It sets up the necessary credentials, endpoints, and provider-specific configurations like skipping region and credential validation. It also defines an S3 bucket resource and its CORS and lifecycle configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "~> 5"
    }
  }
}


provider "aws" {
  region = "us-east-1"


  access_key = <R2 Access Key>
  secret_key = <R2 Secret Key>


  # Required for R2.
  # These options disable S3-specific validation on the client (Terraform) side.
  skip_credentials_validation = true
  skip_region_validation      = true
  skip_requesting_account_id  = true


  endpoints {
    s3 = "https://<account id>.r2.cloudflarestorage.com"
  }
}


resource "aws_s3_bucket" "default" {
  bucket = "<org>-test"
}


resource "aws_s3_bucket_cors_configuration" "default" {
  bucket   = aws_s3_bucket.default.id


  cors_rule {
    allowed_methods = ["GET"]
    allowed_origins = ["*"]
  }
}


resource "aws_s3_bucket_lifecycle_configuration" "default" {
  bucket = aws_s3_bucket.default.id


  rule {
    id     = "expire-bucket"
    status = "Enabled"
    expiration {
      days = 1
    }
  }


  rule {
    id     = "abort-multipart-upload"
    status = "Enabled"
    abort_incomplete_multipart_upload {
      days_after_initiation = 1
    }
  }
}
```

--------------------------------

### Cloudflare R2 S3 API: HeadBucket Region Header

Source: https://developers.cloudflare.com/r2/platform/release-notes

Updates the `HeadBucket` operation in the S3 API to set the `x-amz-bucket-region` header to `auto` in the response. This provides information about the bucket's region.

```http
HTTP/1.1 200 OK
...
x-amz-bucket-region: auto
...
```

--------------------------------

### Create S3 IAM Policy for R2 Access

Source: https://developers.cloudflare.com/r2/llms-full

This JSON policy grants the necessary permissions for Cloudflare R2 to access objects in a specified Amazon S3 bucket. It allows the 's3:GetObject' action on all objects within the bucket.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": ["s3:GetObject"],
      "Resource": ["arn:aws:s3:::<BucketName>/*"]
    }
  ]
}
```

--------------------------------

### Fix Workers R2 Bindings .put()

Source: https://developers.cloudflare.com/r2/platform/release-notes

Resolves an issue with the `.put()` binding for Workers R2, ensuring proper object uploads.

```JavaScript
const r2 = env.YOUR_R2_BUCKET;
r2.put(key, value);
```

--------------------------------

### Cloudflare R2 S3 API: ExposeHeader Response Header

Source: https://developers.cloudflare.com/r2/platform/release-notes

Corrects the behavior of the `Access-Control-Expose-Headers` response header, ensuring it is not rendered if `ExposeHeader` is not defined in the CORS configuration.

```http
HTTP/1.1 200 OK
Access-Control-Expose-Headers: Content-Length
...

HTTP/1.1 200 OK
...

```

--------------------------------

### Connect R2 Bucket to Custom Domain via API

Source: https://developers.cloudflare.com/r2/buckets/public-buckets

This snippet demonstrates how to connect an R2 bucket to a custom domain using the Cloudflare API. It involves making a PUT request to the R2 API endpoint for custom domains, specifying the account ID, bucket name, and the custom domain details.

```javascript
async function connectBucketToCustomDomain(accountId, bucketName, customDomain) {
  const url = `https://api.cloudflare.com/client/v4/accounts/${accountId}/r2/buckets/${bucketName}/custom-domains`;
  const response = await fetch(url, {
    method: 'PUT',
    headers: {
      'Authorization': `Bearer YOUR_CLOUDFLARE_API_TOKEN`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      domain: customDomain
    })
  });
  if (!response.ok) {
    throw new Error(`Failed to connect domain: ${response.statusText}`);
  }
  return await response.json();
}
```

--------------------------------

### Disable Sippy on R2 bucket using Wrangler

Source: https://developers.cloudflare.com/r2/data-migration/sippy

This command disables the Sippy feature on your R2 bucket using the Wrangler CLI. After running this command, Sippy will no longer copy data from your source storage to R2 on demand.

```bash
r2 bucket sippy disable
```

--------------------------------

### Presigned URL: Conditional Upload with `If-Unmodified-Since`

Source: https://developers.cloudflare.com/r2/llms-full

Illustrates how to sign a request for a presigned URL with conditional upload headers, specifically `If-Unmodified-Since`. This ensures that an object is only overwritten if it has not been modified since a specified date. The code first signs the request and then uses the signed URL to perform the PUT operation.

```javascript
const signed = await client.sign(
    new Request(url, {
      method: "PUT",
    }),
    {
      aws: { signQuery: true },
      headers: {
        "If-Unmodified-Since": "Tue, 28 Sep 2021 16:00:00 GMT",
      },
    }
  );
  // Use the presigned URL to upload the file
  const response = await fetch(signed.url, {
    method: "PUT",
    body: file,
    headers: {
      "If-Unmodified-Since": "Tue, 28 Sep 2021 16:00:00 GMT",
    },
  });
```

--------------------------------

### S3 PutBucketCors API - AllowedHeaders Validation

Source: https://developers.cloudflare.com/r2/llms-full

Ensures that the `AllowedHeaders` property in `PutBucketCors` actions only accepts valid origin values, preventing misconfigurations.

```javascript
// Example of a valid PutBucketCors configuration
{
  "CORSRules": [
    {
      "AllowedHeaders": ["*"],
      "AllowedMethods": ["GET", "PUT", "POST", "DELETE", "HEAD"],
      "AllowedOrigins": ["https://example.com"],
      "ExposeHeaders": ["ETag"],
      "MaxAgeSeconds": 3000
    }
  ]
}
```

--------------------------------

### Handle Event Notifications with TypeScript Worker

Source: https://developers.cloudflare.com/r2/llms-full

This TypeScript code updates a Cloudflare Worker to include a queue handler. The `queue` function is designed to process messages from a queue, logging the name of the processed file. It serves as a placeholder for future logic to extract text, summarize, and save content.

```ts
export default {
  async fetch(request, env, ctx): Promise<Response> {
    // No changes in the fetch handler
  },
  async queue(batch, env) {
    for (let message of batch.messages) {
      console.log(`Processing the file: ${message.body.object.key}`);
    }
  },
} satisfies ExportedHandler<Env>;
```

--------------------------------

### Cloudflare R2 S3 API: Public Read ACL

Source: https://developers.cloudflare.com/r2/platform/release-notes

Removes the rejection of requests with the `x-amz-acl: public-read` header for public buckets. This allows objects to be explicitly set as publicly readable via the S3 API.

```http
PUT /my-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com
X-Amz-Acl: public-read

[Object Data]

```

--------------------------------

### Cloudflare R2 ListMultipartUploads Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

The ListMultipartUploads operation lists in-progress multipart uploads in Cloudflare R2. It supports query parameters like delimiter, encoding-type, and max-uploads.

```API
ListMultipartUploads
```

--------------------------------

### Cloudflare R2 S3 API: SHA256 and SHA1 Checksums

Source: https://developers.cloudflare.com/r2/platform/release-notes

Highlights the enhancement of the S3 API's `putObject` operation to support SHA256 and SHA1 checksums. This aligns R2's S3 compatibility with the worker bindings' existing support for these checksums.

```http
PUT /my-object HTTP/1.1
Host: your-bucket.s3.amazonaws.com
Date: Tue, 27 Oct 2023 10:00:00 GMT
Authorization: AWS AKIAIOSFODNN7EXAMPLE:xR4q1+q...=
X-Amz-Content-Sha256: YOUR_SHA256_CHECKSUM
Content-Type: application/octet-stream
Content-Length: 1024

[Object Data]

```

--------------------------------

### Configure S3 Client for Cloudflare R2

Source: https://developers.cloudflare.com/r2/llms-full

This snippet demonstrates how to configure the AWS SDK S3 client to interact with Cloudflare R2. It requires R2 endpoint, account ID, access key ID, and secret access key for authentication. The region is set to 'auto'.

```javascript
import S3 from "aws-sdk/clients/s3.js";


// Configure the S3 client to talk to R2.
const client = new S3({
  endpoint: "https://<account_id>.r2.cloudflarestorage.com",
  credentials: {
    accessKeyId: "<access_key_id>",
    secretAccessKey: "<access_key_secret>",
  },
  region: "auto",
});
```

--------------------------------

### Bind R2 Bucket to Worker (JSONC)

Source: https://developers.cloudflare.com/r2/llms-full

This JSONC configuration snippet shows how to bind an R2 bucket to a Cloudflare Worker. You need to specify a `binding` name (a valid JavaScript variable identifier) and the `bucket_name` of your R2 bucket. This binding allows your Worker to interact with the R2 bucket.

```json
{
  "r2_buckets": [
    {
      "binding": "MY_BUCKET",
      "bucket_name": "<YOUR_BUCKET_NAME>"
    }
  ]
}
```

--------------------------------

### Set Custom Header on All PutObject Requests with boto3

Source: https://developers.cloudflare.com/r2/examples/aws/custom-header

This snippet demonstrates how to configure a custom header for all `PutObject` requests made using the Python `boto3` library. It leverages `boto3`'s event system to intercept and modify outgoing requests before they are sent, ensuring a specific header is always included.

```python
import boto3

# Create an R2 client
client = boto3.client('s3', endpoint_url='https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com', aws_access_key_id='YOUR_ACCESS_KEY_ID', aws_secret_access_key='YOUR_SECRET_ACCESS_KEY')

# Function to add a custom header
def add_custom_header(request, **kwargs):
    request.headers['X-Custom-Header'] = 'MyValue'

# Register the function to the 'before-send' event for PutObject
client.meta.events.register('before-send.s3.PutObject', add_custom_header)

# Now, any PutObject request made with this client will include the custom header
# Example:
# try:
#     response = client.put_object(Bucket='my-bucket', Key='my-object', Body=b'Hello, R2!')
#     print('Object uploaded successfully!')
# except Exception as e:
#     print(f'Error uploading object: {e}')

```

--------------------------------

### Cloudflare R2 Endpoint Configuration

Source: https://developers.cloudflare.com/r2/api/tokens

Configure the endpoint in your S3 client to connect to Cloudflare R2. This includes standard endpoints and jurisdiction-specific ones for EU and FedRAMP.

```text
https://<ACCOUNT_ID>.r2.cloudflarestorage.com
```

```text
https://<ACCOUNT_ID>.eu.r2.cloudflarestorage.com
```

```text
https://<ACCOUNT_ID>.fedramp.r2.cloudflarestorage.com
```

--------------------------------

### Cloudflare Worker for R2 Multipart Uploads

Source: https://developers.cloudflare.com/r2/llms-full

This JavaScript code defines a Cloudflare Worker that handles multipart uploads to an R2 bucket. It supports actions like creating, resuming, aborting, and uploading parts of a multipart upload, as well as standard DELETE operations. The worker uses environment variables to access the R2 bucket.

```javascript
if (uploadId === null) {
              return new Response("Missing uploadId", { status: 400 });
            }
            const multipartUpload = env.MY_BUCKET.resumeMultipartUpload(
              key,
              uploadId
            );


            try {
              multipartUpload.abort();
            } catch (error: any) {
              return new Response(error.message, { status: 400 });
            }
            return new Response(null, { status: 204 });
          }
          case "delete": {
            await env.MY_BUCKET.delete(key);
            return new Response(null, { status: 204 });
          }
          default:
            return new Response(`Unknown action ${action} for DELETE`, {
              status: 400,
            });
        }
      default:
        return new Response("Method Not Allowed", {
          status: 405,
          headers: { Allow: "PUT, POST, GET, DELETE" },
        });
    }
  },
} satisfies ExportedHandler<Env>;
```
```

--------------------------------

### Cloudflare R2 PutObject Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

The PutObject operation uploads an object to Cloudflare R2. It supports system metadata, storage class, and SSE-C, but not object lifecycle or ACLs.

```API
PutObject
```

--------------------------------

### Disable R2 Data Catalog using Wrangler CLI

Source: https://developers.cloudflare.com/r2/llms-full

Disables the R2 Data Catalog on a specified bucket using the Wrangler CLI. This action makes any Iceberg table references stored in the catalog inaccessible.

```bash
npx wrangler r2 bucket catalog disable <BUCKET_NAME>
```

--------------------------------

### Cloudflare R2 ListObjects Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

The ListObjects operation lists objects within a bucket in Cloudflare R2. It supports query parameters such as delimiter, encoding-type, marker, max-keys, and prefix.

```API
ListObjects
```

--------------------------------

### S3 PutObject Content-Type Inference

Source: https://developers.cloudflare.com/r2/llms-full

Uploads will now automatically infer the `Content-Type` based on the file body if one is not explicitly set in the `PutObject` request. This feature will be extended to multipart operations in the future.

```javascript
await env.BUCKET.put(key, fileContent); // Content-Type will be inferred
```

--------------------------------

### Cloudflare R2 Permissions

Source: https://developers.cloudflare.com/r2/api/tokens

Defines permission groups for Cloudflare R2 access, specifying the scope (Account or Bucket) and the actions allowed (e.g., read, write, list).

```text
`Workers R2 Storage Write`| Account| Can create, delete, and list buckets, edit bucket configuration, and read, write, and list objects.
```

```text
`Workers R2 Storage Read`| Account| Can list buckets and view bucket configuration, and read and list objects.
```

```text
`Workers R2 Storage Bucket Item Write`| Bucket| Can read, write, and list objects in buckets.
```

```text
`Workers R2 Storage Bucket Item Read`| Bucket| Can read and list objects in buckets.
```

```text
`Workers R2 Data Catalog Write`| Account| Can read from and write to data catalogs. This permission allows access to the Iceberg REST catalog interface.
```

```text
`Workers R2 Data Catalog Read`| Account| Can read from data catalogs. This permission allows read-only access to the Iceberg REST catalog interface.
```

--------------------------------

### Delete S3 Bucket Lifecycle Configuration

Source: https://developers.cloudflare.com/r2/llms-full

This code snippet shows how to delete the lifecycle configuration for a specified bucket using the configured S3 client for Cloudflare R2. It requires the bucket name as input.

```javascript
// Delete lifecycle configuration for bucket
await client
  .deleteBucketLifecycle({
    Bucket: "bucketName",
  })
  .promise();
```

--------------------------------

### Disable Sippy via Wrangler CLI

Source: https://developers.cloudflare.com/r2/llms-full

Disables the Sippy on-demand migration feature for a specified R2 bucket using the Wrangler CLI. This command reverts the bucket's configuration to not use the on-demand migration service.

```sh
npx wrangler r2 bucket sippy disable <BUCKET_NAME>
```

--------------------------------

### Handle Event Notifications in Worker (TypeScript)

Source: https://developers.cloudflare.com/r2/tutorials/summarize-pdf

TypeScript code for a Cloudflare Worker to handle event notifications from a queue. The current implementation logs the name of the file from each message in the batch. Future steps will involve processing the PDF content and summarizing it.

```typescript
import { Hono } from 'hono'
import { serveStatic } from 'hono/serve-static'
import { Bindings } from './bindings'

const app = new Hono<Bindings>()

app.get('/', serveStatic({ root: './' }))

app.post('/api/upload', async (c) => {
  const file = await c.req.parseFile('pdfFile')
  await c.env.MY_BUCKET.put(file.name, file.content)
  // Send the file to the queue for processing
  await c.env.MY_QUEUE.send({
    fileName: file.name,
    bucketName: c.env.MY_BUCKET.name
  })
  return c.text('File uploaded successfully!')
})

app.get('*', (c) => {
  return c.text('Not Found', 404)
})

// Queue handler
app.queue('MY_QUEUE', async (batch, env) => {
  for (const message of batch.messages) {
    console.log(`Processing file: ${message.body.fileName}`)
    // TODO: Extract text from PDF, summarize using Workers AI, and save to R2
  }
})

export default {
  async fetch(request: Request, env: Env, ctx: ExecutionContext) {
    return app.fetch(request, env, ctx)
  },
}

```

--------------------------------

### Delete R2 Bucket with Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Deletes an R2 bucket using the Wrangler command-line tool. The bucket must be empty before it can be deleted.

```sh
wrangler r2 bucket delete BUCKET_TO_DELETE
```

--------------------------------

### Add Bucket Lock Rule via Wrangler

Source: https://developers.cloudflare.com/r2/buckets/bucket-locks

Enables adding a bucket lock rule to an R2 bucket using the Wrangler CLI. This command allows for the configuration of retention policies on objects within the bucket.

```bash
r2 bucket lock add
```

--------------------------------

### Delete Object using Wrangler CLI

Source: https://developers.cloudflare.com/r2/llms-full

This shell command demonstrates how to delete a specific object from a Cloudflare R2 bucket using the Wrangler command-line interface. It requires the bucket name and the path to the object.

```sh
wrangler r2 object delete test-bucket/foo.png
```

--------------------------------

### Handle Invalid Cursors in S3 List Operations

Source: https://developers.cloudflare.com/r2/platform/release-notes

Ensures that invalid cursors in S3 API list operations no longer cause InternalErrors. Instead, appropriate error messages are returned, improving error handling and user feedback.

```S3 API
Invalid cursors for list operations
```

--------------------------------

### Cloudflare R2 HeadObject Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

The HeadObject operation retrieves metadata from an object in Cloudflare R2. It supports conditional operations like If-Match and If-Modified-Since, as well as SSE-C.

```API
HeadObject
```

--------------------------------

### R2 Extended Metadata with Unicode

Source: https://developers.cloudflare.com/r2/llms-full

Cloudflare R2 allows Unicode characters in metadata keys and values through the `customMetadata` field. This functionality is automatically handled by encoding/decoding RFC 2047 compliant headers for S3-compatible API access. Metadata length limits apply to the decoded Unicode value.

```HTTP
GET /my-object HTTP/1.1
Host: your-bucket.your-account.r2.cloudflarestorage.com

x-amz-meta-unicode-key: unicode-value

```

```HTTP
PUT /my-object HTTP/1.1
Host: your-bucket.your-account.r2.cloudflarestorage.com
Content-Type: text/plain

x-amz-meta-unicode-key: unicode-value

```

--------------------------------

### Cloudflare R2 CopyObject Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

The CopyObject operation copies an object within Cloudflare R2. It supports operation metadata, system metadata, conditional operations, and storage class.

```API
CopyObject
```

--------------------------------

### DMARC Email Worker for R2 Storage and Analytics

Source: https://developers.cloudflare.com/r2/llms-full

This Cloudflare worker script processes incoming DMARC reports, stores them in R2, and generates analytics. It's designed to handle email processing and data aggregation.

```javascript
// Example of a Cloudflare Worker script for DMARC report processing

addEventListener('fetch', event => {
  event.respondWith(handleRequest(event.request))
})

async function handleRequest(request) {
  // Parse incoming DMARC report (e.g., from email)
  const reportData = await parseDmarcReport(request.body);

  // Store report data in R2
  await storeReportInR2(reportData);

  // Generate analytics (optional)
  await generateAnalytics(reportData);

  return new Response('DMARC report processed', { status: 200 })
}

async function parseDmarcReport(reportBody) {
  // Implementation to parse DMARC report XML or other formats
  // This is a placeholder and would involve XML parsing libraries or regex
  console.log('Parsing DMARC report...');
  return { /* parsed data */ };
}

async function storeReportInR2(reportData) {
  // Use Cloudflare R2 bindings to put the object
  // Example: await env.YOUR_R2_BUCKET.put('dmarc-reports/' + Date.now() + '.json', JSON.stringify(reportData));
  console.log('Storing report in R2...');
  // Replace with actual R2 put operation using bindings
}

async function generateAnalytics(reportData) {
  // Logic to aggregate data and generate insights
  console.log('Generating analytics...');
  // This could involve further processing or writing to another service
}
```

--------------------------------

### Cloudflare R2 GetObject Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

The GetObject operation retrieves an object from Cloudflare R2. It supports conditional operations, range retrieval, and SSE-C.

```API
GetObject
```

--------------------------------

### Delete Bucket - S3 API Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

Details the 'DeleteBucket' operation for the S3 API in Cloudflare R2. The 'Bucket Owner' and 'x-amz-expected-bucket-owner' features are not implemented.

```S3 API
DeleteBucket
```

--------------------------------

### R2ObjectBody Definition

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Represents an object's metadata combined with its body, returned when retrieving an object from R2. It includes the object's value as a ReadableStream and methods to access it as an ArrayBuffer, string, JSON, or Blob.

```typescript
interface R2ObjectBody extends R2Object {
  body: ReadableStream;
  bodyUsed: boolean;
  arrayBuffer(): Promise<ArrayBuffer>;
  text(): Promise<string>;
  json<T>(): Promise<T>;
  blob(): Promise<Blob>;
}
```

--------------------------------

### Remove Lifecycle Rule with Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

Removes a specific lifecycle rule from an R2 bucket using the Wrangler CLI. Requires the bucket name and the rule ID to identify the rule to be removed.

```sh
npx wrangler r2 bucket lifecycle remove <BUCKET_NAME> --id <RULE_ID>
```

--------------------------------

### Fix DeleteObjects Operation Errors

Source: https://developers.cloudflare.com/r2/platform/release-notes

Resolves an issue where the S3 API DeleteObjects operation would sometimes return an error even after an object had been successfully deleted.

```S3 API
DeleteObjects operation errors
```

--------------------------------

### Configure CORS for Cloudflare R2 Buckets

Source: https://developers.cloudflare.com/r2/index

Shows how to configure Cross-Origin Resource Sharing (CORS) for a Cloudflare R2 bucket. CORS settings control which web domains are permitted to access your bucket's contents, enhancing security and enabling web application integration.

```JavaScript
async function configureCors(accountId, bucketName, corsConfig) {
  const url = `https://api.cloudflare.com/client/v4/accounts/${accountId}/r2/buckets/${bucketName}/cors`;
  const response = await fetch(url, {
    method: 'PUT',
    headers: {
      'Authorization': `Bearer YOUR_API_TOKEN`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify(corsConfig)
  });
  const data = await response.json();
  console.log(data);
}

// Example CORS configuration
// const corsConfig = {
//   "cors": [
//     {
//       "origin": ["https://example.com"],
//       "method": ["GET", "PUT", "POST", "DELETE", "OPTIONS"],
//       "maxAgeSeconds": 86400
//     }
//   ]
// };
```

--------------------------------

### R2 Object Checksums

Source: https://developers.cloudflare.com/r2/api/workers/workers-api-reference

Checksums for R2 objects, including MD5, SHA-1, SHA-256, SHA-384, and SHA-512. MD5 is included by default for non-multipart objects.

```javascript
{
  md5?: ArrayBuffer,
  sha1?: ArrayBuffer,
  sha256?: ArrayBuffer,
  sha384?: ArrayBuffer,
  sha512?: ArrayBuffer
}
```

--------------------------------

### Remove Custom Domain from R2 Bucket

Source: https://developers.cloudflare.com/r2/llms-full

Procedure for removing a custom domain from an R2 bucket, which disconnects the domain and deletes its configuration. The CNAME record is also removed.

```bash
Step 1: In R2, select the bucket.
Step 2: Go to Settings > Custom Domains.
Step 3: Next to the domain, select ... and Remove domain.
Step 4: Confirm removal in the confirmation window.
```

--------------------------------

### Cloudflare R2 CompleteMultipartUpload Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

The CompleteMultipartUpload operation finalizes a multipart upload in Cloudflare R2. It does not support bucket owner or request payer.

```API
CompleteMultipartUpload
```

--------------------------------

### Delete Object via Wrangler CLI

Source: https://developers.cloudflare.com/r2/objects/delete-objects

Deletes a specific object from an R2 bucket using the Wrangler CLI. This command targets a single object identified by its path within the bucket.

```bash
wrangler r2 object delete {bucket}/{path/to/object}
```

--------------------------------

### Delete Cloudflare R2 Bucket with Wrangler

Source: https://developers.cloudflare.com/r2/buckets/create-buckets

Deletes a specified bucket from Cloudflare R2 using the `r2 bucket delete` command. The bucket must be empty before deletion can occur.

```bash
r2 bucket delete <bucket-name>
```

--------------------------------

### Remove lifecycle rule using Wrangler

Source: https://developers.cloudflare.com/r2/buckets/object-lifecycles

This command removes a specific lifecycle rule from your R2 bucket using the Wrangler CLI.

```bash
wrangler r2 bucket lifecycle remove <bucket-name> --rule-id=<rule-id>
```

--------------------------------

### Delete Bucket Cors - S3 API Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

Outlines the 'DeleteBucketCors' operation for the S3 API in Cloudflare R2. Features related to 'Bucket Owner' and 'x-amz-expected-bucket-owner' are not implemented.

```S3 API
DeleteBucketCors
```

--------------------------------

### Put Bucket Cors - S3 API Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

Details the 'PutBucketCors' operation for the S3 API in Cloudflare R2. Features related to checksums and bucket owner information are not implemented.

```S3 API
PutBucketCors
```

--------------------------------

### Cloudflare R2 AbortMultipartUpload Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

The AbortMultipartUpload operation cancels a multipart upload in Cloudflare R2. It does not support request payer.

```API
AbortMultipartUpload
```

--------------------------------

### Cloudflare R2 DeleteObjects Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

The DeleteObjects operation deletes multiple objects from Cloudflare R2. It does not support MFA or object locking.

```API
DeleteObjects
```

--------------------------------

### Remove R2 Bucket Lock Rule with Wrangler

Source: https://developers.cloudflare.com/r2/llms-full

This command uses Wrangler to remove a specific bucket lock rule from a Cloudflare R2 bucket. It requires both the bucket name and the rule ID to be provided.

```sh
npx wrangler r2 bucket lock remove <BUCKET_NAME> --id <RULE_ID>
```

--------------------------------

### Cloudflare R2 DeleteObject Operation

Source: https://developers.cloudflare.com/r2/api/s3/api

The DeleteObject operation deletes a single object from Cloudflare R2. It does not support MFA or object locking.

```API
DeleteObject
```

--------------------------------

### Disable Domain Access for R2 Bucket

Source: https://developers.cloudflare.com/r2/llms-full

Instructions for disabling public access to an R2 bucket through a specific custom domain. This action affects only the specified domain, not other access methods.

```bash
Step 1: In R2, select the bucket.
Step 2: Go to Settings > Custom Domains.
Step 3: Next to the domain, select ... and Disable domain.
Step 4: Verify the Access to Bucket badge updates to Not allowed.
```

--------------------------------

### Retrieve S3 API Credentials from API Token

Source: https://developers.cloudflare.com/r2/api/tokens

Explains how to extract the Access Key ID and Secret Access Key from a Cloudflare API token response. The Access Key ID is the token's 'id', and the Secret Access Key is the SHA-256 hash of the token's 'value'.

```text
Access Key ID: The `id` of the API token.
```

```text
Secret Access Key: The SHA-256 hash of the API token `value`.
```

--------------------------------

### Remove Bucket Lock Rule via Wrangler

Source: https://developers.cloudflare.com/r2/buckets/bucket-locks

Removes a specific bucket lock rule from an R2 bucket using the Wrangler CLI. This action allows for the modification or deletion of existing retention policies.

```bash
r2 bucket lock remove
```

--------------------------------

### Disable Public Access for R2 Bucket Custom Domain via API

Source: https://developers.cloudflare.com/r2/buckets/public-buckets

This snippet shows how to disable public access for a custom domain associated with an R2 bucket using the Cloudflare API. It requires a DELETE request to the specific custom domain endpoint for the bucket.

```javascript
async function disableBucketCustomDomain(accountId, bucketName, customDomain) {
  const url = `https://api.cloudflare.com/client/v4/accounts/${accountId}/r2/buckets/${bucketName}/custom-domains/${customDomain}`;
  const response = await fetch(url, {
    method: 'DELETE',
    headers: {
      'Authorization': `Bearer YOUR_CLOUDFLARE_API_TOKEN`
    }
  });
  if (!response.ok) {
    throw new Error(`Failed to disable domain: ${response.statusText}`);
  }
  return await response.json();
}
```