================
CODE SNIPPETS
================
### Generate Veo 3 Video from Image (JavaScript, Go)

Source: https://ai.google.dev/gemini-api/docs/video

These snippets demonstrate the full workflow for generating a video using the Veo 3 model, starting with an image generated by Imagen. It covers generating an initial image, using it as input for video generation, polling the operation status until completion, and finally downloading or saving the generated video.

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = "Panning wide shot of a calico kitten sleeping in the sunshine";

// Step 1: Generate an image with Imagen.
const imagenResponse = await ai.models.generateImages({
  model: "imagen-4.0-generate-001",
  prompt: prompt,
});

// Step 2: Generate video with Veo 3 using the image.
let operation = await ai.models.generateVideos({
  model: "veo-3.0-generate-001",
  prompt: prompt,
  image: {
    imageBytes: imagenResponse.generatedImages[0].image.imageBytes,
    mimeType: "image/png",
  },
});

// Poll the operation status until the video is ready.
while (!operation.done) {
  console.log("Waiting for video generation to complete...")
  await new Promise((resolve) => setTimeout(resolve, 10000));
  operation = await ai.operations.getVideosOperation({
    operation: operation,
  });
}

// Download the video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "veo3_with_image_input.mp4",
});
console.log(`Generated video saved to veo3_with_image_input.mp4`);
```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := "Panning wide shot of a calico kitten sleeping in the sunshine"

    // Step 1: Generate an image with Imagen.
    imagenResponse, err := client.Models.GenerateImages(
        ctx,
        "imagen-4.0-generate-001",
        prompt,
        nil, // GenerateImagesConfig
    )
    if err != nil {
        log.Fatal(err)
    }

    // Step 2: Generate video with Veo 3 using the image.
    operation, err := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        imagenResponse.GeneratedImages[0].Image,
        nil, // GenerateVideosConfig
    )
    if err != nil {
        log.Fatal(err)
    }

    // Poll the operation status until the video is ready.
    for !operation.Done {
        log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "veo3_with_image_input.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

--------------------------------

### Generate Veo 3 Video with Negative Prompt in Python

Source: https://ai.google.dev/gemini-api/docs/video

This Python snippet demonstrates how to generate a video using the `veo-3.0-generate-001` model, specifying a `negative_prompt` to guide the generation process by excluding undesirable elements. It initializes the Gemini client, makes the video generation request, and includes a polling mechanism to wait for the operation to complete.

```python
import time
from google import genai
from google.genai import types

client = genai.Client()

operation = client.models.generate_videos(
    model="veo-3.0-generate-001",
    prompt="A cinematic shot of a majestic lion in the savannah.",
    config=types.GenerateVideosConfig(negative_prompt="cartoon, drawing, low quality"),
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)
```

--------------------------------

### Generate Videos with Veo 3

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue

This API endpoint allows you to generate a video using the specified Veo model from a text prompt. The video generation process is asynchronous, returning an operation object that needs to be polled for completion and to retrieve the final video.

```APIDOC
## POST /models:generateVideos

### Description
This endpoint initiates the generation of a high-fidelity video using a specified Veo model based on a text prompt. The process is asynchronous; an operation resource is returned immediately, which must be polled to track the status and retrieve the final video. The Veo 3 model supports generating 8-second 720p or 1080p videos with native audio.

### Method
POST

### Endpoint
`/v1/models:generateVideos` (Conceptual endpoint for client library function `client.models.generate_videos`)

### Parameters
#### Request Body
- **model** (string) - Required - The identifier of the Veo model to use for video generation (e.g., `veo-3.0-generate-001`, `veo-3.0-fast-generate-001`, `veo-2.0-generate-001`).
- **prompt** (string) - Required - The text prompt describing the desired video content. This prompt guides the AI in generating visual and audio elements.

### Request Example
```json
{
  "model": "veo-3.0-generate-001",
  "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'"
}
```

### Response
#### Success Response (200 OK - Initial Operation)
The initial response is an `Operation` object, indicating that the video generation has started asynchronously. This operation object contains a unique name that can be used to poll its status until the `done` field is true.
- **name** (string) - The unique name of the operation, typically in the format `operations/video-generation-XXXXXXXXX`. This name is used to poll the operation's status.
- **metadata** (object) - Additional metadata about the operation, which may include creation time and other task-specific details.
- **done** (boolean) - A boolean flag indicating whether the operation has completed. Initially `false`.

#### Response Example (Initial - 200 OK)
```json
{
  "name": "operations/video-generation-12345",
  "metadata": {
    "@type": "type.googleapis.com/google.ai.generative.api.OperationMetadata",
    "createTime": "2023-10-27T10:00:00Z"
  },
  "done": false
}
```

#### Success Response (200 OK - Completed Operation)
Once the `done` field of the polled `Operation` object becomes `true`, the `response` field will be populated with the actual video generation result, or an `error` field if the operation failed.
- **name** (string) - (Same as initial) The unique name of the operation.
- **metadata** (object) - (Same as initial) Updated metadata, potentially including `endTime`.
- **done** (boolean) - `true`, indicating the operation has finished.
- **response** (object) - Contains the result of the video generation.
  - **@type** (string) - Type identifier for the response object, e.g., `type.googleapis.com/google.ai.generative.api.GenerateVideoResponse`.
  - **video_uri** (string) - A URL pointing to the generated video file.
  - **video_id** (string) - A unique identifier for the generated video.
- **error** (object) - (Optional) If the operation failed, this field will contain details about the error.

#### Response Example (Completed - 200 OK)
```json
{
  "name": "operations/video-generation-12345",
  "metadata": {
    "@type": "type.googleapis.com/google.ai.generative.api.OperationMetadata",
    "createTime": "2023-10-27T10:00:00Z",
    "endTime": "2023-10-27T10:01:30Z"
  },
  "done": true,
  "response": {
    "@type": "type.googleapis.com/google.ai.generative.api.GenerateVideoResponse",
    "video_uri": "https://storage.googleapis.com/gemini-api-videos/generated-video-123.mp4",
    "video_id": "generated-video-123"
  }
}
```
```

--------------------------------

### POST /models/generateVideos

Source: https://ai.google.dev/gemini-api/docs/video

Initiates the generation of a video using the specified Veo model and parameters, optionally with an initial image.

```APIDOC
## POST /models/generateVideos

### Description
Initiates the generation of a video using the specified Veo model and parameters, optionally with an initial image.

### Method
POST

### Endpoint
/models/generateVideos

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **model** (string) - Required - The ID of the Veo model to use (e.g., "veo-3.0-generate-001").
- **prompt** (string) - Required - The text description for the video. Supports audio cues.
- **negativePrompt** (string) - Optional - Text describing what not to include in the video.
- **image** (Image object) - Optional - An initial image to animate.
- **aspectRatio** (string) - Optional - The video's aspect ratio. Valid values: "16:9" (default, 720p & 1080p), "9:16" (720p).
- **resolution** (string) - Optional - The video's resolution. Valid values: "720p" (default), "1080p" (16:9 only).
- **personGeneration** (string) - Optional - Controls the generation of people. Valid values: "allow_all", "allow_adult", "dont_allow". (Limitations apply based on model and text/image-to-video).
- **seed** (integer) - Optional - Seed for improved determinism in Veo 3 models.

### Request Example
```json
{
  "model": "veo-3.0-generate-001",
  "prompt": "A cinematic shot of a majestic lion in the savannah.",
  "negativePrompt": "cartoon, drawing, low quality",
  "image": {
    "imageBytes": "[base64_encoded_image_data]",
    "mimeType": "image/png"
  },
  "aspectRatio": "16:9",
  "resolution": "720p",
  "personGeneration": "allow_all"
}
```

### Response
#### Success Response (200)
- **name** (string) - The full resource name of the asynchronous operation.
- **done** (boolean) - Indicates if the operation has completed.
- **response** (object) - Present when `done` is true. Contains the result of the operation.
  - **generatedVideos** (array) - An array of generated video objects.
    - **video** (object) - Details about the generated video.
      - **name** (string) - Resource name of the video file (e.g., "files/video-abcde").
      - **mimeType** (string) - MIME type of the video (e.g., "video/mp4").
      - **videoBytes** (string) - Base64 encoded video data (may be empty if file needs to be downloaded separately).

#### Response Example
```json
{
  "name": "operations/video-generation-12345",
  "done": false
}
```
After polling completes:
```json
{
  "name": "operations/video-generation-12345",
  "done": true,
  "response": {
    "generatedVideos": [
      {
        "video": {
          "name": "files/video-abcde",
          "mimeType": "video/mp4",
          "videoBytes": "[base64_encoded_video_data]"
        }
      }
    ]
  }
}
```
```

--------------------------------

### Download Generated Veo 3 Video in Python

Source: https://ai.google.dev/gemini-api/docs/video

This Python snippet demonstrates how to download a previously generated video once the operation has completed. It accesses the first generated video from the `operation.response` object and saves it to a specified MP4 file locally.

```python
video = operation.response.generated_videos[0]
client.files.download(file=video.video)
video.video.save("veo3_with_image_input.mp4")
print("Generated video saved to veo3_with_image_input.mp4")
```

--------------------------------

### Generate Videos with Dialogue and Sound Effects using Python and Gemini API

Source: https://ai.google.dev/gemini-api/docs/video

This Python snippet demonstrates how to generate a video with a detailed text prompt, including dialogue and sound effects, using the Google Gemini API's Veo 3 model. It initializes the client, defines a prompt, calls the `generate_videos` method with the `veo-3.0-generate-001` model, and includes a polling mechanism to wait for the video generation to complete.

```python
import time
from google import genai
from google.genai import types

client = genai.Client()

prompt = """A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'"""

operation = client.models.generate_videos(
    model="veo-3.0-generate-001",
    prompt=prompt,
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)
```

--------------------------------

### POST /models/generateImages

Source: https://ai.google.dev/gemini-api/docs/video

Initiates the generation of an image based on a text prompt using the specified Imagen model.

```APIDOC
## POST /models/generateImages

### Description
Initiates the generation of an image based on a text prompt using the specified Imagen model.

### Method
POST

### Endpoint
/models/generateImages

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **model** (string) - Required - The ID of the Imagen model to use (e.g., "imagen-4.0-generate-001").
- **prompt** (string) - Required - The text description for the image to generate.

### Request Example
```json
{
  "model": "imagen-4.0-generate-001",
  "prompt": "Panning wide shot of a calico kitten sleeping in the sunshine"
}
```

### Response
#### Success Response (200)
- **generatedImages** (array) - A list of generated image objects.
  - **image** (object) - Details about the generated image.
    - **imageBytes** (string) - Base64 encoded image data.
    - **mimeType** (string) - MIME type of the image (e.g., "image/png").

#### Response Example
```json
{
  "generatedImages": [
    {
      "image": {
        "imageBytes": "[base64_encoded_image_data]",
        "mimeType": "image/png"
      }
    }
  ]
}
```
```

--------------------------------

### JavaScript: Generate Video from Image, Poll, and Download with Veo 3

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue

This JavaScript example uses the `@google/genai` library to orchestrate video generation. It first generates an initial image using `imagen-4.0-generate-001`, then feeds this image as input to the `veo-3.0-generate-001` model to create a video. The code includes a polling mechanism to wait for the asynchronous video generation to complete before finally downloading and saving the resulting video to `veo3_with_image_input.mp4`.

```JavaScript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = "Panning wide shot of a calico kitten sleeping in the sunshine";

// Step 1: Generate an image with Imagen.
const imagenResponse = await ai.models.generateImages({
  model: "imagen-4.0-generate-001",
  prompt: prompt,
});

// Step 2: Generate video with Veo 3 using the image.
let operation = await ai.models.generateVideos({
  model: "veo-3.0-generate-001",
  prompt: prompt,
  image: {
    imageBytes: imagenResponse.generatedImages[0].image.imageBytes,
    mimeType: "image/png",
  },
});

// Poll the operation status until the video is ready.
while (!operation.done) {
  console.log("Waiting for video generation to complete...")
  await new Promise((resolve) => setTimeout(resolve, 10000));
  operation = await ai.operations.getVideosOperation({
    operation: operation,
  });
}

// Download the video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "veo3_with_image_input.mp4",
});
console.log(`Generated video saved to veo3_with_image_input.mp4`);
```

--------------------------------

### POST /v1/models/{model_id}:generateVideos

Source: https://ai.google.dev/gemini-api/docs/video_hl=id

This endpoint generates a video using the specified Veo 3 model. It can take a text prompt and an optional initial image to animate, along with various configuration options.

```APIDOC
## POST /v1/models/{model_id}:generateVideos

### Description
This endpoint generates a video using the specified Veo 3 model. It can take a text prompt and an optional initial image to animate, along with various configuration options.

### Method
POST

### Endpoint
/v1/models/{model_id}:generateVideos

### Parameters
#### Path Parameters
- **model_id** (string) - Required - The ID of the Veo model to use (e.g., "veo-3.0-generate-001").

#### Request Body
- **prompt** (string) - Required - Text description for the video. Supports audio cues.
- **negativePrompt** (string) - Optional - Text describing what should *not* be included in the video.
- **image** (object) - Optional - Initial image to animate.
  - **imageBytes** (string) - Base64 encoded image data.
  - **mimeType** (string) - Mime type of the image (e.g., "image/png").
- **aspectRatio** (string) - Optional - Aspect ratio of the video. Supported values: `"16:9"` (default, 720p & 1080p), `"9:16"` (720p).
- **resolution** (string) - Optional - Resolution of the video. Supported values: `"720p"` (default), `"1080p"` (16:9 only).
- **personGeneration** (string) - Optional - Controls the generation of people. Supported values: `"allow_all"`, `"allow_adult"`, `"dont_allow"`. (Restrictions apply based on text-to-video or image-to-video).
- **seed** (integer) - Optional - Seed for generation (Veo 3 only). Does not guarantee determinism but slightly improves it.

### Request Example
{
  "model": "veo-3.0-generate-001",
  "prompt": "A cinematic shot of a majestic lion in the savannah.",
  "image": {
    "imageBytes": "base64encodedimagedata...",
    "mimeType": "image/png"
  },
  "negativePrompt": "cartoon, drawing, low quality",
  "aspectRatio": "16:9",
  "resolution": "1080p",
  "personGeneration": "allow_all"
}

### Response
#### Success Response (200)
- **name** (string) - The unique name of the operation (e.g., "operations/12345").
- **done** (boolean) - Indicates if the operation is complete.

#### Response Example
{
  "name": "operations/veo-video-op-12345",
  "metadata": {},
  "done": false,
  "response": null,
  "error": null
}
```

--------------------------------

### Download Veo 3 Generated Video (Python)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=sq

This Python snippet demonstrates how to download a video generated by the Veo 3 model. It assumes an 'operation' object with a completed video generation response. It extracts the video file from the response and saves it to a local MP4 file. An authenticated `client` instance is required for the `files.download` method.

```python
# Download the video.
video = operation.response.generated_videos[0]
client.files.download(file=video.video)
video.video.save("veo3_with_image_input.mp4")
print("Generated video saved to veo3_with_image_input.mp4")
```

--------------------------------

### Generate, Poll, and Download Video with Gemini API (Multi-language)

Source: https://ai.google.dev/gemini-api/docs/video

This comprehensive snippet illustrates the end-to-end process of video generation using the Gemini API. It covers initiating a video creation request with a specified prompt and configuration, implementing a polling mechanism to asynchronously monitor the operation's status until completion, and finally downloading the generated video asset to a local file system. Examples are provided for Python, JavaScript, Go, and Shell, demonstrating client library usage and direct API calls.

```python
import time
from google import genai
from google.genai import types

client = genai.Client()

# Start the video generation job with a specific model and prompt.
operation = client.models.generate_videos(
    model="veo-3.0-generate-001",
    prompt="A cinematic shot of a majestic lion in the savannah.",
)

# Use operation.name to get the operation object (optional, for explicit refresh)
# This step is often useful if you need to fetch the operation status later
# or if the initial operation object doesn't have all the polling methods.
operation = types.GenerateVideosOperation(name=operation.name)

# Poll the operation status until the video is ready.
# The loop checks the job status every 10 seconds.
print("Waiting for video generation to complete...")
while not operation.done:
    time.sleep(10)
    operation = client.operations.get(operation)

# Once the operation is done, download and save the generated video.
# The generated video is typically found in the response of the completed operation.
generated_video = operation.response.generated_videos[0]
client.files.download(file=generated_video.video)
generated_video.video.save("parameters_example.mp4")
print("Generated video saved to parameters_example.mp4")
```

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

let operation = await ai.models.generateVideos({
  model: "veo-3.0-generate-001",
  prompt: "A cinematic shot of a majestic lion in the savannah.",
  config: {
    aspectRatio: "16:9",
    negativePrompt: "cartoon, drawing, low quality"
  },
});

// Poll the operation status until the video is ready.
while (!operation.done) {
  console.log("Waiting for video generation to complete...")
  await new Promise((resolve) => setTimeout(resolve, 10000));
  operation = await ai.operations.getVideosOperation({
    operation: operation,
  });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "parameters_example.mp4",
});
console.log(`Generated video saved to parameters_example.mp4`);
```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    videoConfig := &genai.GenerateVideosConfig{
        AspectRatio: "16:9",
        NegativePrompt: "cartoon, drawing, low quality",
    }

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        "A cinematic shot of a majestic lion in the savannah.",
        nil,
        videoConfig,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
        log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil) // This prepares the video data.
    fname := "parameters_example.mp4"
    // Save the video bytes to a local file.
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

```bash
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A cinematic shot of a majestic lion in the savannah."
      }
    ],
    "parameters": {
      "aspectRatio": "16:9",
      "negativePrompt": "cartoon, drawing, low quality"
    }
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o parameters_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

--------------------------------

### GET /operations/{operationId}

Source: https://ai.google.dev/gemini-api/docs/video

Retrieves the status and results of a previously initiated video generation operation.

```APIDOC
## GET /operations/{operationId}

### Description
Retrieves the status and results of a previously initiated video generation operation.

### Method
GET

### Endpoint
/operations/{operationId}

### Parameters
#### Path Parameters
- **operationId** (string) - Required - The ID of the video generation operation (e.g., "video-generation-12345").

#### Query Parameters
N/A

#### Request Body
N/A

### Request Example
N/A

### Response
#### Success Response (200)
- **name** (string) - The full resource name of the asynchronous operation.
- **done** (boolean) - Indicates if the operation has completed.
- **response** (object) - Present when `done` is true. Contains the result of the operation.
  - **generatedVideos** (array) - An array of generated video objects.
    - **video** (object) - Details about the generated video.
      - **name** (string) - Resource name of the video file (e.g., "files/video-abcde").
      - **mimeType** (string) - MIME type of the video (e.g., "video/mp4").
      - **videoBytes** (string) - Base64 encoded video data (may be empty if file needs to be downloaded separately).

#### Response Example
```json
{
  "name": "operations/video-generation-12345",
  "done": true,
  "response": {
    "generatedVideos": [
      {
        "video": {
          "name": "files/video-abcde",
          "mimeType": "video/mp4",
          "videoBytes": "[base64_encoded_video_data]"
        }
      }
    ]
  }
}
```
```

--------------------------------

### Generate Video with Veo 3 and Poll for Completion (Multi-language)

Source: https://ai.google.dev/gemini-api/docs/video_hl=tr

These examples illustrate how to initiate video generation using Google's Veo 3 model via the Gemini API. The JavaScript and Go examples first generate an image using Imagen, then use that image as input for Veo 3 video generation, and finally download the result. The Python example shows text-to-video generation with a negative prompt. All examples include polling logic to wait for the asynchronous video generation process to complete.

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = "Panning wide shot of a calico kitten sleeping in the sunshine";

// Step 1: Generate an image with Imagen.
const imagenResponse = await ai.models.generateImages({
  model: "imagen-4.0-generate-001",
  prompt: prompt,
});

// Step 2: Generate video with Veo 3 using the image.
let operation = await ai.models.generateVideos({
  model: "veo-3.0-generate-001",
  prompt: prompt,
  image: {
    imageBytes: imagenResponse.generatedImages[0].image.imageBytes,
    mimeType: "image/png",
  },
});

// Poll the operation status until the video is ready.
while (!operation.done) {
  console.log("Waiting for video generation to complete...")
  await new Promise((resolve) => setTimeout(resolve, 10000));
  operation = await ai.operations.getVideosOperation({
    operation: operation,
  });
}

// Download the video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "veo3_with_image_input.mp4",
});
console.log(`Generated video saved to veo3_with_image_input.mp4`);
```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := "Panning wide shot of a calico kitten sleeping in the sunshine"

    // Step 1: Generate an image with Imagen.
    imagenResponse, err := client.Models.GenerateImages(
        ctx,
        "imagen-4.0-generate-001",
        prompt,
        nil, // GenerateImagesConfig
    )
    if err != nil {
        log.Fatal(err)
    }

    // Step 2: Generate video with Veo 3 using the image.
    operation, err := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        imagenResponse.GeneratedImages[0].Image,
        nil, // GenerateVideosConfig
    )
    if err != nil {
        log.Fatal(err)
    }

    // Poll the operation status until the video is ready.
    for !operation.Done {
        log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "veo3_with_image_input.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

```python
import time
from google import genai
from google.genai import types

client = genai.Client()

operation = client.models.generate_videos(
    model="veo-3.0-generate-001",
    prompt="A cinematic shot of a majestic lion in the savannah.",
    config=types.GenerateVideosConfig(negative_prompt="cartoon, drawing, low quality"),
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)
```

--------------------------------

### Generate and Monitor Veo Video with Gemini API (Node.js)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue

This Node.js example demonstrates how to initiate a video generation job using the Google Gemini API's Veo model. It shows how to create a video from a text prompt and then poll the operation's status asynchronously until the video generation is complete. Dependencies include the `@google/genai` library.

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

// After starting the job, you get an operation object.
let operation = await ai.models.generateVideos({
  model: "veo-3.0-generate-001",
  prompt: "A cinematic shot of a majestic lion in the savannah."
});

// Alternatively, you can use operation.name to get the operation.
// operation = types.GenerateVideosOperation(name=operation.name)

// This loop checks the job status every 10 seconds.
while (!operation.done) {
    await new Promise((resolve) => setTimeout(resolve, 1000));
    // Refresh the operation object to get the latest status.
    operation = await ai.operations.getVideosOperation({ operation });
}

// Once done, the result is in operation.response.
// ... process and download your video ...
```

--------------------------------

### Go: Generate Video from Image, Poll, and Download with Veo 3

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue

This Go example demonstrates a full video generation workflow using the `google.golang.org/genai` client. It begins by generating an image with `imagen-4.0-generate-001`, which is then used as the source for video creation via the `veo-3.0-generate-001` model. The program includes a polling loop to monitor the video generation status and, upon completion, saves the generated video bytes to `veo3_with_image_input.mp4`.

```Go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := "Panning wide shot of a calico kitten sleeping in the sunshine"

    // Step 1: Generate an image with Imagen.
    imagenResponse, err := client.Models.GenerateImages(
        ctx,
        "imagen-4.0-generate-001",
        prompt,
        nil, // GenerateImagesConfig
    )
    if err != nil {
        log.Fatal(err)
    }

    // Step 2: Generate video with Veo 3 using the image.
    operation, err := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        imagenResponse.GeneratedImages[0].Image,
        nil, // GenerateVideosConfig
    )
    if err != nil {
        log.Fatal(err)
    }

    // Poll the operation status until the video is ready.
    for !operation.Done {
        log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "veo3_with_image_input.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

--------------------------------

### Generate and Monitor Video Creation with Google Gemini API (JavaScript)

Source: https://ai.google.dev/gemini-api/docs/video

This JavaScript code demonstrates how to initiate a video generation task using the Google Gemini API's 'veo-3.0-generate-001' model and then continuously poll the operation's status until the video generation is complete. It utilizes the `@google/genai` library and includes a loop with a 1-second delay to refresh the operation object and check for completion.

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

// After starting the job, you get an operation object.
let operation = await ai.models.generateVideos({
  model: "veo-3.0-generate-001",
  prompt: "A cinematic shot of a majestic lion in the savannah.",
});

// Alternatively, you can use operation.name to get the operation.
// operation = types.GenerateVideosOperation(name=operation.name)

// This loop checks the job status every 10 seconds.
while (!operation.done) {
    await new Promise((resolve) => setTimeout(resolve, 1000));
    // Refresh the operation object to get the latest status.
    operation = await ai.operations.getVideosOperation({ operation });
}

// Once done, the result is in operation.response.
// ... process and download your video ...
```

--------------------------------

### POST /models/{modelId}:generateVideos

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue

This endpoint initiates the asynchronous generation of a video based on a text prompt and optional image input, with various parameters to control the output. It returns a long-running operation that can be polled for completion.

```APIDOC
## POST /models/{modelId}:generateVideos

### Description
This endpoint initiates the asynchronous generation of a video based on a text prompt and optional image input, with various parameters to control the output. It returns a long-running operation that can be polled for completion.

### Method
POST

### Endpoint
`/models/{modelId}:generateVideos`

### Parameters
#### Path Parameters
- **modelId** (string) - Required - The ID of the video generation model to use (e.g., `veo-3.0-generate-001`, `veo-2.0-generate-001`).

#### Request Body
- **prompt** (string) - Required - The text description for the video. Supports audio cues.
- **negativePrompt** (string) - Optional - Text describing what not to include in the video.
- **image** (object) - Optional - An initial image object to animate. See `Image` object structure below.
- **aspectRatio** (string) - Optional - The video's aspect ratio. Accepted values: `"16:9"` (default), `"9:16"`. Supported resolutions vary by model.
- **resolution** (string) - Optional - The video's resolution. Accepted values: `"720p"` (default), `"1080p"` (only with `"16:9"` aspect ratio for Veo 3 & Veo 3 Fast, unsupported for Veo 2).
- **personGeneration** (string) - Optional - Controls the generation of people. Accepted values: `"allow_all"`, `"allow_adult"`, `"dont_allow"`. (Behavior varies by Veo version and input type: Text-to-video vs. Image-to-video).
- **seed** (integer) - Optional - A seed value used by Veo 3 models to slightly improve determinism for generated videos.

### Image object structure
- **imageBytes** (string) - Required (within Image object) - Base64 encoded image data.
- **mimeType** (string) - Required (within Image object) - MIME type of the image (e.g., `"image/png"`, `"image/jpeg"`).

### Request Example
```json
{
  "modelId": "veo-3.0-generate-001",
  "prompt": "A cinematic shot of a majestic lion in the savannah.",
  "negativePrompt": "cartoon, drawing, low quality",
  "image": {
    "imageBytes": "base64EncodedImageData",
    "mimeType": "image/png"
  },
  "aspectRatio": "16:9",
  "resolution": "1080p",
  "personGeneration": "allow_all"
}
```

### Response
#### Success Response (200 OK)
Returns a long-running `Operation` object which needs to be polled to get the final video generation status and result.

- **name** (string) - The unique identifier for the long-running operation.
- **metadata** (object) - Service-specific metadata associated with the operation.
- **done** (boolean) - If the value is `false`, it means the operation is still in progress. If `true`, the operation is completed, and either `error` or `response` is available.
- **error** (object) - The error result of the operation in case of failure or cancellation.
- **response** (object) - The normal response of the operation in case of success. Contains a `GeneratedVideos` object.

#### `GeneratedVideos` Object
- **generatedVideos** (array of object) - A list of generated video objects.
  - **video** (object) - Contains details about the generated video.
    - **videoBytes** (string) - Base64 encoded video data (may not always be present, depending on download method).
    - **uri** (string) - URI to download the video.
    - **mimeType** (string) - MIME type of the video (e.g., `"video/mp4"`).

#### Response Example
```json
{
  "name": "operations/generateVideos-1234567890",
  "metadata": {},
  "done": false,
  "response": {
    "@type": "type.googleapis.com/google.genai.v1beta.GenerateVideosResponse",
    "generatedVideos": [
      {
        "video": {
          "uri": "gs://bucket/path/to/video.mp4",
          "mimeType": "video/mp4"
        }
      }
    ]
  }
}
```
```

--------------------------------

### Generate and Download Video with Veo 3.0 via cURL (Bash)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=tr

This Bash script demonstrates how to generate a video using the Veo 3.0 model by making direct HTTP requests with `curl`. It polls the operation status using `jq` to parse the JSON response until the video is ready. Finally, it extracts the video URI and downloads the generated video to `dialogue_example.mp4`.

```bash
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That'\''s the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
      }
    ]
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

--------------------------------

### Generate Video from Image with Veo 3 (Python)

Source: https://ai.google.dev/gemini-api/docs/video_hl=tr

This Python code demonstrates a two-step process: first, it generates an image using the `imagen-4.0-generate-001` model from a text prompt. Then, it uses this newly generated image as an input along with the prompt to generate a video using the `veo-3.0-generate-001` model, polling until the video generation is complete.

```python
import time
from google import genai

client = genai.Client()

prompt = "Panning wide shot of a calico kitten sleeping in the sunshine"

# Step 1: Generate an image with Imagen.
imagen = client.models.generate_images(
    model="imagen-4.0-generate-001",
    prompt=prompt,
)

# Step 2: Generate video with Veo 3 using the image.
operation = client.models.generateVideos(
    model="veo-3.0-generate-001",
    prompt=prompt,
    image=imagen.generated_images[0].image,
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)
```

--------------------------------

### Veo Video Generation Parameters

Source: https://ai.google.dev/gemini-api/docs/video_hl=ko

Describes the parameters available for controlling the video generation process in the Veo API. Includes details on prompt, negativePrompt, image, aspectRatio, resolution, and personGeneration.

```APIDOC
## Veo API Parameters and Specifications

The following parameters can be set in the API request to control the video generation process.

| Parameter | Description | Veo 3 and Veo 3 Fast | Veo 2 |
|---|---|---|---|
| `prompt` | Text description of the video. Supports audio signals. | `string` | `string` |
| `negativePrompt` | Text describing what should not be included in the video. | `string` | `string` |
| `image` | Initial image to be animated. | `Image` object | `Image` object |
| `aspectRatio` | Aspect ratio of the video. | `"16:9"` (default, 720p and 1080p), `"9:16"` (720p) | `"16:9"` (default, 720p), `"9:16"` (720p) |
| `resolution` | Resolution of the video. | `"720p"` (default), `"1080p"` (16:9 only) | Not supported |
| `personGeneration` | Controls person generation. (See restrictions for regional restrictions.) | Text-to-video: only `"allow_all"` Image-to-video: only `"allow_adult"` | Text-to-video: `"allow_all"`, `"allow_adult"`, `"dont_allow"` Image-to-video: `"allow_adult"`, `"dont_allow"` |

The `seed` parameter is also available in the Veo 3 model. It provides a slight improvement, although it does not guarantee determinism.

You can customize video generation by setting parameters in the request. For example, you can guide the model by specifying a `negativePrompt`.
```

--------------------------------

### GET /v1/operations/{operation_id}

Source: https://ai.google.dev/gemini-api/docs/video_hl=id

This endpoint retrieves the current status and results of an asynchronous video generation operation identified by its unique ID.

```APIDOC
## GET /v1/operations/{operation_id}

### Description
This endpoint retrieves the current status and results of an asynchronous video generation operation identified by its unique ID.

### Method
GET

### Endpoint
/v1/operations/{operation_id}

### Parameters
#### Path Parameters
- **operation_id** (string) - Required - The unique identifier of the operation, obtained from the initial `generateVideos` call (e.g., "operations/veo-video-op-12345").

### Request Example
{}

### Response
#### Success Response (200)
- **name** (string) - The unique name of the operation.
- **done** (boolean) - `true` if the operation is complete, `false` otherwise.
- **response** (object) - Present if `done` is `true`. Contains the results.
  - **generatedVideos** (array) - An array of generated video objects.
  - **generatedVideos[].video.name** (string) - Resource name of the video file.
  - **generatedVideos[].video.mimeType** (string) - Mime type of the video (e.g., "video/mp4").
  - **generatedVideos[].video.videoBytes** (string) - Base64 encoded video data (if directly embedded in response).
- **error** (object) - Present if the operation failed.

#### Response Example
{
  "name": "operations/veo-video-op-12345",
  "metadata": {},
  "done": true,
  "response": {
    "@type": "type.googleapis.com/google.ai.generativelanguage.v1beta.GenerateVideosResponse",
    "generatedVideos": [
      {
        "video": {
          "name": "files/veo-video-file-67890",
          "mimeType": "video/mp4",
          "videoBytes": "base64encodedvideodata..."
        }
      }
    ]
  },
  "error": null
}
```

--------------------------------

### POST /v1/models/{model_id}:generateImages

Source: https://ai.google.dev/gemini-api/docs/video_hl=id

This endpoint generates an image based on a text prompt using the specified Imagen model.

```APIDOC
## POST /v1/models/{model_id}:generateImages

### Description
This endpoint generates an image based on a text prompt using the specified Imagen model.

### Method
POST

### Endpoint
/v1/models/{model_id}:generateImages

### Parameters
#### Path Parameters
- **model_id** (string) - Required - The ID of the Imagen model to use (e.g., "imagen-4.0-generate-001").

#### Request Body
- **prompt** (string) - Required - The text description for the image to be generated.

### Request Example
{
  "model": "imagen-4.0-generate-001",
  "prompt": "Panning wide shot of a calico kitten sleeping in the sunshine"
}

### Response
#### Success Response (200)
- **generatedImages** (array) - An array of generated images.
- **generatedImages[].image.imageBytes** (string) - Base64 encoded image data.
- **generatedImages[].image.mimeType** (string) - Mime type of the image (e.g., "image/png").

#### Response Example
{
  "generatedImages": [
    {
      "image": {
        "imageBytes": "base64encodedimagedata...",
        "mimeType": "image/png"
      }
    }
  ]
}
```

--------------------------------

### POST /models/generateVideos (Python Example)

Source: https://ai.google.dev/gemini-api/docs/video_hl=ko

An example of generating a video using the Veo model in Python. Demonstrates setting the prompt and negative prompt parameters.

```APIDOC
## POST /models/generateVideos

### Description
This endpoint generates a video using the Veo model with specified parameters.

### Method
POST

### Endpoint
/models/generateVideos

### Request Body
- **model** (string) - Required - The model to use for video generation (e.g., "veo-3.0-generate-001")
- **prompt** (string) - Required - A text description of the video to generate
- **config** (object) - Optional - Configuration options for video generation
  - **negative_prompt** (string) - Optional - A text description of what should not be included in the video

### Request Example
```json
{
  "model": "veo-3.0-generate-001",
  "prompt": "A cinematic shot of a majestic lion in the savannah.",
  "config": {
    "negative_prompt": "cartoon, drawing, low quality"
  }
}
```

### Response
#### Success Response (200)
- **operation** (object) - An object representing the video generation operation.

#### Response Example
```json
{
  "operation": {
    "name": "operations/video-000",
    "done": false
  }
}
```

```

--------------------------------

### POST /models/veo-3.0-generate-001:predictLongRunning (Image-to-Video)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=ja

Initiates a long-running operation to generate a video from a text prompt and an initial image using the Veo 3.0 model. The response includes an operation name for polling.

```APIDOC
## POST /models/veo-3.0-generate-001:predictLongRunning

### Description
Sends a request to generate a video using the Veo 3.0 model based on a text prompt and an optional starting image. This is a long-running operation, and the response will contain an operation name to poll for completion.

### Method
POST

### Endpoint
`${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning`

### Parameters
#### Path Parameters
- **model_id** (string) - Required - The ID of the model to use, in this case, `veo-3.0-generate-001`.

#### Request Body
- **instances** (array of objects) - Required - An array containing one or more video generation requests.
  - **prompt** (string) - Required - The text prompt describing the desired video content.
  - **image** (object) - Optional - An image to use as a starting point or input for the video generation.
    - **imageBytes** (string) - Required - Base64 encoded byte string of the image.
    - **mimeType** (string) - Required - The MIME type of the image (e.g., `image/png`).

### Request Example
```json
{
  "instances": [{
      "prompt": "Panning wide shot of a calico kitten sleeping in the sunshine",
      "image": {
          "imageBytes": "iVBORw0KGgoAAAANSUhEUgA...",
          "mimeType": "image/png"
      }
    }
  ]
}
```

### Response
#### Success Response (200)
- **name** (string) - The unique identifier for the long-running operation.

#### Response Example
```json
{
  "name": "operations/generate-video-fedcba9876543210"
}
```
```

--------------------------------

### POST /models/veo-3.0-generate-001:predictLongRunning (Text-to-Video)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=ja

Initiates a long-running operation to generate a video from a text prompt using the Veo 3.0 model. The response includes an operation name used for polling the status.

```APIDOC
## POST /models/veo-3.0-generate-001:predictLongRunning

### Description
Sends a request to generate a video using the Veo 3.0 model based on a text prompt. This is a long-running operation, and the response will contain an operation name to poll for completion.

### Method
POST

### Endpoint
`${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning`

### Parameters
#### Path Parameters
- **model_id** (string) - Required - The ID of the model to use, in this case, `veo-3.0-generate-001`.

#### Request Body
- **instances** (array of objects) - Required - An array containing one or more video generation requests.
  - **prompt** (string) - Required - The text prompt describing the desired video content.

### Request Example
```json
{
  "instances": [{
      "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That's the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
    }
  ]
}
```

### Response
#### Success Response (200)
- **name** (string) - The unique identifier for the long-running operation.

#### Response Example
```json
{
  "name": "operations/generate-video-0123456789abcdef"
}
```
```

--------------------------------

### Generate Video from Image using Imagen and Veo 3 (Python, JavaScript, Go)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=ja

This code demonstrates a multi-step process: first, it generates an image from a text prompt using Google's Imagen model. Subsequently, this generated image is used as input for the Veo 3 model to create a video. The script includes polling logic to await the completion of the video generation operation and then downloads the resulting video file locally.

```python
import time
from google import genai

client = genai.Client()

prompt = "Panning wide shot of a calico kitten sleeping in the sunshine"

# Step 1: Generate an image with Imagen.
imagen = client.models.generate_images(
    model="imagen-4.0-generate-001",
    prompt=prompt,
)

# Step 2: Generate video with Veo 3 using the image.
operation = client.models.generate_videos(
    model="veo-3.0-generate-001",
    prompt=prompt,
    image=imagen.generated_images[0].image,
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

# Download the video.
video = operation.response.generated_videos[0]
client.files.download(file=video.video)
video.video.save("veo3_with_image_input.mp4")
print("Generated video saved to veo3_with_image_input.mp4")
```

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = "Panning wide shot of a calico kitten sleeping in the sunshine";

// Step 1: Generate an image with Imagen.
const imagenResponse = await ai.models.generateImages({
  model: "imagen-4.0-generate-001",
  prompt: prompt,
});

// Step 2: Generate video with Veo 3 using the image.
let operation = await ai.models.generateVideos({
  model: "veo-3.0-generate-001",
  prompt: prompt,
  image: {
    imageBytes: imagenResponse.generatedImages[0].image.imageBytes,
    mimeType: "image/png",
  },
});

// Poll the operation status until the video is ready.
while (!operation.done) {
  console.log("Waiting for video generation to complete...")
  await new Promise((resolve) => setTimeout(resolve, 10000));
  operation = await ai.operations.getVideosOperation({
    operation: operation,
  });
}

// Download the video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "veo3_with_image_input.mp4",
});
console.log(`Generated video saved to veo3_with_image_input.mp4`);
```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := "Panning wide shot of a calico kitten sleeping in the sunshine"

    // Step 1: Generate an image with Imagen.
    imagenResponse, err := client.Models.GenerateImages(
        ctx,
        "imagen-4.0-generate-001",
        prompt,
        nil, // GenerateImagesConfig
    )
    if err != nil {
        log.Fatal(err)
    }

    // Step 2: Generate video with Veo 3 using the image.
    operation, err := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        imagenResponse.GeneratedImages[0].Image,
        nil, // GenerateVideosConfig
    )
    if err != nil {
        log.Fatal(err)
    }

    // Poll the operation status until the video is ready.
    for !operation.Done {
        log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "veo3_with_image_input.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

--------------------------------

### POST /models/generateVideos (Veo 3)

Source: https://ai.google.dev/gemini-api/docs/video_hl=ar

Generates a video based on a textual prompt and optional image input, allowing customization of video attributes like aspect ratio and resolution. The operation is asynchronous and requires polling for completion.

```APIDOC
## POST /models/generateVideos

### Description
Generates a video based on a textual prompt and optional image input using the Veo 3 or Veo 3 Fast models. This endpoint supports both text-to-video and image-to-video generation, with options to customize output video properties. Video generation is an asynchronous operation that returns an operation object which needs to be polled for the final result.

### Method
POST

### Endpoint
`/models/generateVideos`

### Parameters
#### Request Body
- **model** (string) - Required - The model to use for video generation (e.g., `"veo-3.0-generate-001"`, `"veo-3.0-fast"`).
- **prompt** (string) - Required - The textual description of the video to generate. Supports audio cues.
- **negativePrompt** (string) - Optional - Text describing content that should not be included in the video.
- **image** (object) - Optional - An `Image` object used for image-to-video generation. This object typically contains `imageBytes` (base64 encoded image data) and `mimeType`.
- **aspectRatio** (string) - Optional - The aspect ratio of the video. For Veo 3, options include `"16:9"` (default, 720p and 1080p), `"9:16"` (720p). For Veo 2, `"16:9"` (default, 720p), `"9:16"` (720p).
- **resolution** (string) - Optional - The resolution of the video. For Veo 3, options include `"720p"` (default), `"1080p"` (for 16:9 only). Not available for Veo 2.
- **personGeneration** (string) - Optional - Controls the generation of people. Constraints apply based on generation type (text-to-video or image-to-video) and model version. E.g., `"allow_all"`, `"allow_adult"`, `"dont_allow"`.
- **seed** (integer) - Optional - A seed for video generation. Does not guarantee deterministic results but can improve consistency.

### Request Example
```json
{
  "model": "veo-3.0-generate-001",
  "prompt": "A cinematic shot of a majestic lion in the savannah.",
  "negativePrompt": "cartoon, drawing, low quality",
  "aspectRatio": "16:9",
  "resolution": "720p"
}
```

### Response
#### Success Response (200 OK - Initial Operation)
When calling `generateVideos`, the API returns an `Operation` object immediately, as video generation is an asynchronous task.

- **name** (string) - The resource name of the long-running operation.
- **done** (boolean) - Indicates if the operation has completed. Will be `false` initially.
- **metadata** (object) - Contains operation-specific metadata, including the prompt, model, and timestamps.

#### Response Example (Initial Operation)
```json
{
  "name": "operations/generate-video-12345",
  "done": false,
  "metadata": {
    "@type": "type.googleapis.com/google.cloud.genai.v1.GenerateVideosMetadata",
    "prompt": "A cinematic shot of a majestic lion in the savannah.",
    "model": "veo-3.0-generate-001",
    "createTime": "2024-01-01T12:00:00Z"
  }
}
```

#### Success Response (200 OK - Polled Operation Complete)
After polling the operation until `done` is `true`, the `response` field will contain the details of the generated videos.

- **name** (string) - The resource name of the long-running operation.
- **done** (boolean) - `true`, indicating the operation has completed.
- **response** (object) - The result of the operation.
  - **@type** (string) - Type of the response object (e.g., `"type.googleapis.com/google.cloud.genai.v1.GenerateVideosResponse"`).
  - **generatedVideos** (array) - A list of generated video objects.
    - **video** (object) - Details about an individual generated video.
      - **name** (string) - The resource name of the video file.
      - **mimeType** (string) - The MIME type of the video (e.g., `"video/mp4"`).
      - **uri** (string) - A URI to the video file, typically for downloading.
      - **videoBytes** (string) - (Optional) Base64 encoded video content, if requested or available directly.

#### Response Example (Polled Operation Complete)
```json
{
  "name": "operations/generate-video-12345",
  "done": true,
  "response": {
    "@type": "type.googleapis.com/google.cloud.genai.v1.GenerateVideosResponse",
    "generatedVideos": [
      {
        "video": {
          "name": "files/video-abcde",
          "mimeType": "video/mp4",
          "uri": "gs://bucket/path/to/video.mp4"
        }
      }
    ]
  },
  "metadata": {
    "@type": "type.googleapis.com/google.cloud.genai.v1.GenerateVideosMetadata",
    "prompt": "A cinematic shot of a majestic lion in the savannah.",
    "model": "veo-3.0-generate-001",
    "createTime": "2024-01-01T12:00:00Z",
    "updateTime": "2024-01-01T12:05:00Z"
  }
}
```
```

--------------------------------

### GET /files/{fileId}

Source: https://ai.google.dev/gemini-api/docs/video

Retrieves the raw binary content of a generated file, such as a video, by its file ID.

```APIDOC
## GET /files/{fileId}

### Description
Retrieves the raw binary content of a generated file, such as a video, by its file ID.

### Method
GET

### Endpoint
/files/{fileId}

### Parameters
#### Path Parameters
- **fileId** (string) - Required - The ID of the file to retrieve (e.g., "video-abcde" from `video.video.name`).

#### Query Parameters
N/A

#### Request Body
N/A

### Request Example
N/A

### Response
#### Success Response (200)
- The binary content of the requested file (e.g., an MP4 video stream).

#### Response Example
```text
[Binary content of the .mp4 file]
```
```

--------------------------------

### Generate Video from Image using Imagen and Veo 3 (Python)

Source: https://ai.google.dev/gemini-api/docs/video_hl=es-419

This Python snippet illustrates a two-step process: first, it generates an image using the Imagen model from a textual prompt. Then, it uses the generated image as an input, along with the original prompt, to generate a video using the Veo 3 model, continuously polling for the video generation to complete. This example does not include the final download step.

```python
import time
from google import genai

client = genai.Client()

prompt = "Panning wide shot
```

--------------------------------

### Generate Video from Image using Imagen and Veo (Python)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=ar

This Python example demonstrates generating a video by first creating an image with the Imagen model and then using that generated image as a starting point for video creation with the Veo 3 model. The script includes polling to wait for the video generation to complete.

```python
import time
from google import genai

client = genai.Client()

prompt = "Panning wide shot of a calico kitten sleeping in the sunshine"

# Step 1: Generate an image with Imagen.
imagen = client.models.generate_images(
    model="imagen-4.0-generate-001",
    prompt=prompt,
)

# Step 2: Generate video with Veo 3 using the image.
operation = client.models.generate_videos(
    model="veo-3.0-generate-001",
    prompt=prompt,
    image=imagen.generated_images[0].image,
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)
```

--------------------------------

### GET /v1/files/{file_id}

Source: https://ai.google.dev/gemini-api/docs/video_hl=id

This endpoint allows downloading a specific generated video file identified by its file ID. The client library handles the actual byte stream retrieval and local saving.

```APIDOC
## GET /v1/files/{file_id}

### Description
This endpoint allows downloading a specific generated video file identified by its file ID. The client library handles the actual byte stream retrieval and local saving.

### Method
GET

### Endpoint
/v1/files/{file_id}

### Parameters
#### Path Parameters
- **file_id** (string) - Required - The unique identifier of the video file (e.g., "files/veo-video-file-67890"), obtained from the `GetVideoOperation` response.

### Request Example
{}

### Response
#### Success Response (200)
- The response is the raw binary content of the video file.

#### Response Example
{}

```

--------------------------------

### POST /models/veo-3.0-generate-001:predictLongRunning - Generate Video from Image and Text Prompt

Source: https://ai.google.dev/gemini-api/docs/video_hl=tr

Initiates a long-running operation to create a video from a text prompt, using an initial image as a starting point. The API returns an operation name, which must be used to poll for the video generation status until completion.

```APIDOC
## POST /models/veo-3.0-generate-001:predictLongRunning

### Description
Initiates a long-running operation to create a video from a text prompt, using an initial image as a starting point. The API returns an operation name, which must be used to poll for the video generation status until completion.

### Method
POST

### Endpoint
/models/veo-3.0-generate-001:predictLongRunning

### Parameters
#### Request Body
- **instances** (array of object) - Required - An array containing video generation requests.
  - **prompt** (string) - Required - The textual description for the video.
  - **image** (object) - Required - The image to use as a starting frame.
    - **imageData** (string) - Required - Base64 encoded image data.
    - **mimeType** (string) - Required - MIME type of the image (e.g., `image/jpeg`).

### Request Example
```json
{
  "instances": [
    {
      "prompt": "Panning wide shot of a calico kitten sleeping in the sunshine",
      "image": {
        "imageData": "BASE64_ENCODED_IMAGE_DATA",
        "mimeType": "image/jpeg"
      }
    }
  ]
}
```

### Response
#### Success Response (200)
- **name** (string) - The unique identifier for the long-running operation.
- **done** (boolean) - Indicates if the operation is complete. Initially `false`.

#### Response Example
```json
{
  "name": "operations/generate-video-67890",
  "done": false
}
```
```

--------------------------------

### Generate and Download Video from Text Prompt with Gemini API

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=fr

These examples show how to use the Gemini API's Veo 3.0 model to generate a video from a text prompt. The code initiates the video generation, polls the operation status until completion, and then downloads the final MP4 video file.

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.0-generate-001",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);
```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

```shell
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That\'s the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
      }
    ]
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

--------------------------------

### POST /models/veo-3.0-generate-001:predictLongRunning - Generate Video from Text Prompt

Source: https://ai.google.dev/gemini-api/docs/video_hl=tr

Initiates a long-running operation to create a video based on a textual description. The API returns an operation name, which must be used to poll for the video generation status until completion.

```APIDOC
## POST /models/veo-3.0-generate-001:predictLongRunning

### Description
Initiates a long-running operation to create a video based on a textual description. The API returns an operation name, which must be used to poll for the video generation status until completion.

### Method
POST

### Endpoint
/models/veo-3.0-generate-001:predictLongRunning

### Parameters
#### Request Body
- **instances** (array of object) - Required - An array containing video generation requests.
  - **prompt** (string) - Required - The textual description for the video.

### Request Example
```json
{
  "instances": [{
      "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That's the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
    }
  ]
}
```

### Response
#### Success Response (200)
- **name** (string) - The unique identifier for the long-running operation.
- **done** (boolean) - Indicates if the operation is complete. Initially `false`.

#### Response Example
```json
{
  "name": "operations/generate-video-12345",
  "done": false
}
```
```

--------------------------------

### Generate and Download Video from Text Prompt (Multi-language)

Source: https://ai.google.dev/gemini-api/docs/video_hl=ar

These examples illustrate how to generate a video using the Gemini API's Veo 3.0 model from a text prompt. The code initiates a video generation operation, polls its status until completion, and then downloads the resulting MP4 video. This process typically involves an API client, a prompt, and a polling mechanism for long-running operations.

```JavaScript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.\nA man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.0-generate-001",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);
```

```Go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

```Shell
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That's the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
      }
    ]
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

--------------------------------

### Generate and Download Video from Text Prompt using Gemini API

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=bn

This snippet demonstrates how to generate a video using the Gemini API's Veo 3.0 model from a text prompt and then download the resulting video file. It includes polling logic to wait for the video generation to complete. Examples are provided for JavaScript, Go, and Shell.

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.\nA man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.0-generate-001",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);

```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}

```

```shell
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That\'\'s the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
      }
    ]
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done

```

--------------------------------

### Generate, Poll, and Download Video from Text Prompt

Source: https://ai.google.dev/gemini-api/docs/video_hl=th

These code snippets illustrate the end-to-end process of generating a video using a text prompt with the Gemini API's Veo 3.0 model. They include initiating the video generation, continuously polling the operation status until the video is ready, and finally downloading the generated video to a local file.

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.\n    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.0-generate-001",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);
```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
        A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

```bash
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That's the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
      }
    ]
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

--------------------------------

### Generate Video from Image using Gemini API in Python

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=bn

This Python snippet illustrates a two-step process: first, it generates an image using the Imagen model from a text prompt. Then, it uses this generated image as the starting frame to generate a video with the Veo 3.0 model. The code includes polling to await video generation completion.

```python
import time
from google import genai

client = genai.Client()

prompt = "Panning wide shot of a calico kitten sleeping in the sunshine"

# Step 1: Generate an image with Imagen.
imagen = client.models.generate_images(
    model="imagen-4.0-generate-001",
    prompt=prompt,
)

# Step 2: Generate video with Veo 3 using the image.
operation = client.models.generate_videos(
    model="veo-3.0-generate-001",
    prompt=prompt,
    image=imagen.generated_images[0].image,
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

```

--------------------------------

### Generate and Download Video from Text Prompt using Gemini API

Source: https://ai.google.dev/gemini-api/docs/video_hl=ru

This example demonstrates how to generate a video from a textual prompt using the Gemini API's Veo 3.0 model. It includes steps for initiating the video generation, polling the operation status until completion, and finally downloading the generated video file to a local path. An initialized Gemini client and an API key are required for authentication.

```python
generated_video = operation.response.generated_videos[0]
client.files.download(file=generated_video.video)
generated_video.video.save("dialogue_example.mp4")
print("Generated video saved to dialogue_example.mp4")
```

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.0-generate-001",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);
```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

```bash
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That's the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
      }
    ]
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

--------------------------------

### Generate and Download Video Asynchronously with Gemini API

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=fr

This snippet demonstrates how to generate a video using the Gemini API's `veo-3.0-generate-001` model, poll the long-running operation until completion, and then download the generated video. It requires a configured Gemini API client and an API key. The output is an MP4 file saved locally.

```python
import time
from google import genai
from google.genai import types

# Ensure client is initialized (e.g., genai.configure(api_key="YOUR_API_KEY"))
client = genai.Client()

# Start video generation and get an operation object.
operation = client.models.generate_videos(
    model="veo-3.0-generate-001",
    prompt="A cinematic shot of a majestic lion in the savannah.",
    config=types.GenerateVideosConfig(
        aspect_ratio="16:9",
        negative_prompt="cartoon, drawing, low quality",
    )
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

# Download the generated video.
generated_video = operation.response.generated_videos[0]
generated_video.video.save("parameters_example.mp4")
print("Generated video saved to parameters_example.mp4")
```

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({}); // Initialize with API key if needed: { apiKey: "YOUR_API_KEY" }

let operation = await ai.models.generateVideos({
  model: "veo-3.0-generate-001",
  prompt: "A cinematic shot of a majestic lion in the savannah.",
  config: {
    aspectRatio: "16:9",
    negativePrompt: "cartoon, drawing, low quality"
  },
});

// Poll the operation status until the video is ready.
while (!operation.done) {
  console.log("Waiting for video generation to complete...")
  await new Promise((resolve) => setTimeout(resolve, 10000));
  operation = await ai.operations.getVideosOperation({
    operation: operation,
  });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "parameters_example.mp4",
});
console.log(`Generated video saved to parameters_example.mp4`);
```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    // Initialize client. Replace nil with actual client options if needed (e.g., with API key).
    // client, err := genai.NewClient(ctx, option.WithAPIKey("YOUR_API_KEY"))
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }
    defer client.Close()

    videoConfig := &genai.GenerateVideosConfig{
        AspectRatio: "16:9",
        NegativePrompt: "cartoon, drawing, low quality",
    }

    operation, err := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        "A cinematic shot of a majestic lion in the savannah.",
        nil, // No request config
        videoConfig,
    )
    if err != nil {
        log.Fatal(err)
    }

    // Poll the operation status until the video is ready.
    for !operation.Done {
        log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, err = client.Operations.GetVideosOperation(ctx, operation, nil)
        if err != nil {
            log.Fatal(err)
        }
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    fname := "parameters_example.mp4"
    err = os.WriteFile(fname, video.Video.VideoBytes, 0644) // Write the bytes to a file
    if err != nil {
        log.Fatal(err)
    }
    log.Printf("Generated video saved to %s\n", fname)
}
```

```bash
# Note: This script uses jq to parse the JSON response.
# Ensure GEMINI_API_KEY environment variable is set.
# Install jq: sudo apt-get install jq or brew install jq

BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A cinematic shot of a majestic lion in the savannah."
      }
    ],
    "parameters": {
      "aspectRatio": "16:9",
      "negativePrompt": "cartoon, drawing, low quality"
    }
  }' | jq -r .name)

if [ -z "$operation_name" ]; then
  echo "Error: Failed to start video generation or get operation name."
  exit 1
fi

echo "Operation name: $operation_name"

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check for cURL errors
  if [ $? -ne 0 ]; then
    echo "Error: cURL failed to retrieve operation status."
    exit 1
  fi

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq -r .done) # Use -r for raw output

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')

    if [ -z "$video_uri" ] || [ "$video_uri" = "null" ]; then
      echo "Error: Could not extract video URI from response."
      echo "${status_response}"
      exit 1
    fi

    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o parameters_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    if [ $? -ne 0 ]; then
      echo "Error: Failed to download video."
      exit 1
    fi
    echo "Generated video saved to parameters_example.mp4"
    break
  elif [ "${is_done}" = "false" ]; then
    # Wait for 10 seconds before checking again.
    echo "Waiting for video generation to complete..."
    sleep 10
  else
    echo "Error: Unexpected 'done' status: ${is_done}. Response: ${status_response}"
    exit 1
  fi
done
```

--------------------------------

### Generate and Download Video from Text Prompt using Gemini API

Source: https://ai.google.dev/gemini-api/docs/video_hl=he

This set of snippets demonstrates how to generate a video from a text prompt using the Gemini API's Veo 3.0 model. It covers initiating the video generation, polling the operation status until completion, and finally downloading the generated MP4 file. The examples use a descriptive prompt to create a dialogue scene.

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.0-generate-001",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);

```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}

```

```bash
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That'\''s the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
      }
    ]
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done

```

--------------------------------

### POST /models/{modelId}:generateVideos

Source: https://ai.google.dev/gemini-api/docs/video_hl=tr

Generates a video based on a text prompt and optional input image using the specified Google Veo model. This endpoint allows customization of video attributes such as aspect ratio, resolution, and character generation.

```APIDOC
## POST /models/{modelId}:generateVideos

### Description
Generates a video based on a text prompt and optional input image using the specified Google Veo model. This endpoint allows customization of video attributes such as aspect ratio, resolution, and character generation.

### Method
POST

### Endpoint
/models/{modelId}:generateVideos

### Parameters
#### Path Parameters
- **modelId** (string) - Required - The ID of the Veo model to use for video generation (e.g., "veo-3.0-generate-001", "veo-2.0-generate-001").

#### Request Body
- **prompt** (string) - Required - The text description for the video content. Supports audio cues.
- **negativePrompt** (string) - Optional - Text describing elements or styles that should be excluded from the video.
- **image** (object) - Optional - An object containing an initial image to guide the animation.
  - **image.imageBytes** (string) - Required if `image` is provided - Base64 encoded image data.
  - **image.mimeType** (string) - Required if `image` is provided - The MIME type of the image (e.g., "image/png").
- **aspectRatio** (string) - Optional - The aspect ratio of the generated video. Supported values:
  - `"16:9"` (default for 720p and 1080p)
  - `"9:16"` (for 720p only)
- **resolution** (string) - Optional - The resolution of the generated video. Supported values:
  - `"720p"` (default)
  - `"1080p"` (only for 16:9 aspect ratio, not supported for Veo 2 models)
- **personGeneration** (string) - Optional - Controls the generation of characters or people in the video. Specific values depend on the model and input type (text-to-video vs. image-to-video). Examples: `"allow_all"`, `"allow_adult"`, `"dont_allow"`.
- **seed** (integer) - Optional - A seed value to improve determinism in video generation. Does not guarantee full determinism.

### Request Example
```json
{
  "prompt": "Panning wide shot of a calico kitten sleeping in the sunshine",
  "negativePrompt": "cartoon, drawing, low quality",
  "image": {
    "imageBytes": "[base64_encoded_image_data]",
    "mimeType": "image/png"
  },
  "aspectRatio": "16:9",
  "resolution": "1080p",
  "personGeneration": "allow_all",
  "seed": 12345
}
```

### Response
#### Success Response (200 OK)
Returns an asynchronous operation object.
- **name** (string) - The unique name of the operation, which can be used to poll for status.
- **metadata** (object) - Additional operation-specific metadata.
- **done** (boolean) - Indicates if the operation has completed. If `true`, the `response` or `error` field will be populated.
- **response** (object) - Populated if `done` is `true` and the operation succeeded. Contains the generated video details.
  - **generatedVideos** (array) - An array of generated video objects.
    - **generatedVideos[0].video** (object) - Reference to the generated video file.
      - **name** (string) - The resource name of the video file (e.g., "files/video-abcde").
      - **mimeType** (string) - The MIME type of the video file (e.g., "video/mp4").
- **error** (object) - Populated if `done` is `true` and the operation failed. Contains error details.

#### Response Example (Pending Operation)
```json
{
  "name": "operations/generateVideos-12345",
  "metadata": {},
  "done": false
}
```

#### Response Example (Completed Operation)
```json
{
  "name": "operations/generateVideos-12345",
  "metadata": {},
  "done": true,
  "response": {
    "generatedVideos": [
      {
        "video": {
          "name": "files/video-abcde",
          "mimeType": "video/mp4"
        },
        "assetId": "asset-123",
        "createTime": "2023-10-27T10:00:00Z"
      }
    ]
  }
}
```
```

--------------------------------

### Generate and Download Video from Text Prompt with Gemini API

Source: https://ai.google.dev/gemini-api/docs/video_hl=pl

This collection of code snippets demonstrates how to generate a video from a text prompt using the Google Gemini API's Veo 3.0 model, poll its status for completion, and then download the resulting video to a local file. Examples are provided for JavaScript, Go, and Shell (cURL/jq).

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.0-generate-001",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);
```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

```shell
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https:
```

--------------------------------

### POST /models/{model_id}:predictLongRunning - Generate Video

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=pl

Initiates a long-running operation to generate a video from a text prompt or an initial image. This endpoint is typically used with models like `veo-3.0-generate-001`.

```APIDOC
## POST /models/{model_id}:predictLongRunning

### Description
Initiates a long-running operation to generate a video from a text prompt or an initial image. This endpoint is typically used with models like `veo-3.0-generate-001`.

### Method
POST

### Endpoint
/models/{model_id}:predictLongRunning

### Parameters
#### Path Parameters
- **model_id** (string) - Required - The ID of the model to use for video generation (e.g., `veo-3.0-generate-001`).

#### Request Body
- **instances** (array of objects) - Required - A list of generation requests.
  - **prompt** (string) - Required - The text prompt describing the desired video.
  - **image** (object) - Optional - An image object to use as a starting frame for video generation. This is provided by a successful image generation call.

### Request Example
```json
{
  "instances": [{
      "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'"
    }
  ]
}
```

### Response
#### Success Response (200)
- **name** (string) - The resource name of the long-running operation.
- **metadata** (object) - Additional metadata about the operation, including creation time.
- **done** (boolean) - Indicates if the operation is complete (initially `false`).

#### Response Example
```json
{
  "name": "operations/generate-video-12345",
  "metadata": {
    "@type": "type.googleapis.com/google.cloud.generativelanguage.v1beta.GenerateVideoMetadata",
    "createTime": "2024-01-01T12:00:00Z"
  },
  "done": false
}
```
```

--------------------------------

### Generate and Download Video from Text Prompt (JS, Go, Bash)

Source: https://ai.google.dev/gemini-api/docs/video_hl=fr

This code demonstrates the end-to-end process of generating a video from a text prompt using the `veo-3.0-generate-001` model of the Google Gemini API. It initiates the generation, polls the operation status until the video is complete, and then downloads the resulting MP4 file to local storage.

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.0-generate-001",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);
```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

```bash
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application
```

--------------------------------

### Generate and Download Video using Gemini API with Asynchronous Polling

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=hi

These code examples demonstrate how to create a video using the Gemini API's `veo-3.0-generate-001` model. The process involves sending a prompt and configuration, then entering a polling loop to check the operation's `done` status every few seconds until the video generation is complete. Once finished, the first generated video is downloaded and saved to a local file.

```python
import time
from google import genai

client = genai.Client()

# After starting the job, you get an operation object.
# This initiates the video generation.
operation = client.models.generate_videos(
    model="veo-3.0-generate-001",
    prompt="A cinematic shot of a majestic lion in the savannah.",
)

# Poll the operation status until the video is ready.
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    # Re-fetch the operation to get its latest status
    operation = client.operations.get(operation)

# Download the generated video.
generated_video = operation.response.generated_videos[0]
client.files.download(file=generated_video.video)
generated_video.video.save("parameters_example.mp4")
print("Generated video saved to parameters_example.mp4")
```

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

let operation = await ai.models.generateVideos({
  model: "veo-3.0-generate-001",
  prompt: "A cinematic shot of a majestic lion in the savannah.",
  config: {
    aspectRatio: "16:9",
    negativePrompt: "cartoon, drawing, low quality"
  },
});

// Poll the operation status until the video is ready.
while (!operation.done) {
  console.log("Waiting for video generation to complete...")
  await new Promise((resolve) => setTimeout(resolve, 10000));
  operation = await ai.operations.getVideosOperation({
    operation: operation,
  });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "parameters_example.mp4",
});
console.log(`Generated video saved to parameters_example.mp4`);
```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    videoConfig := &genai.GenerateVideosConfig{
        AspectRatio: "16:9",
        NegativePrompt: "cartoon, drawing, low quality",
    }

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        "A cinematic shot of a majestic lion in the savannah.",
        nil,
        videoConfig,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
        log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "parameters_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

```shell
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A cinematic shot of a majestic lion in the savannah."
      }
    ],
    "parameters": {
      "aspectRatio": "16:9",
      "negativePrompt": "cartoon, drawing, low quality"
    }
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o parameters_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

--------------------------------

### Generate and Download Video from Text Prompt using Gemini API

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=id

This example demonstrates how to generate a video from a text prompt using the Google Gemini API's Veo 3.0 model, poll for its completion, and then download the resulting video. It covers initializing the client, sending a generation request, and saving the video file locally across multiple programming languages.

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.0-generate-001",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);
```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

```shell
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That's the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
      }
    ]
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

--------------------------------

### Generate, Poll, and Download Video with Gemini API

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=es-419

These examples illustrate the complete workflow for generating a video using the Gemini API's `veo-3.0-generate-001` model. It includes sending a generation request, polling the asynchronous operation status until completion, and finally downloading the generated video to a local file. The `aspectRatio` and `negativePrompt` are configured for the video generation.

```python
import time
from google import genai
from google.genai import types

client = genai.Client()

# After starting the job, you get an operation object.
operation = client.models.generate_videos(
    model="veo-3.0-generate-001",
    prompt="A cinematic shot of a majestic lion in the savannah.",
)

# Alternatively, you can use operation.name to get the operation.
operation = types.GenerateVideosOperation(name=operation.name)

# This loop checks the job status every 10 seconds.
while not operation.done:
    time.sleep(10)
    # Refresh the operation object to get the latest status.
    operation = client.operations.get(operation)

# Once done, the result is in operation.response.
generated_video = operation.response.generated_videos[0]
client.files.download(file=generated_video.video)
generated_video.video.save("parameters_example.mp4")
print("Generated video saved to parameters_example.mp4")
```

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

let operation = await ai.models.generateVideos({
  model: "veo-3.0-generate-001",
  prompt: "A cinematic shot of a majestic lion in the savannah.",
  config: {
    aspectRatio: "16:9",
    negativePrompt: "cartoon, drawing, low quality"
  },
});

// Poll the operation status until the video is ready.
while (!operation.done) {
  console.log("Waiting for video generation to complete...")
  await new Promise((resolve) => setTimeout(resolve, 10000));
  operation = await ai.operations.getVideosOperation({
    operation: operation,
  });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "parameters_example.mp4",
});
console.log(`Generated video saved to parameters_example.mp4`);
```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    videoConfig := &genai.GenerateVideosConfig{
        AspectRatio: "16:9",
        NegativePrompt: "cartoon, drawing, low quality",
    }

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        "A cinematic shot of a majestic lion in the savannah.",
        nil,
        videoConfig,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
        log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "parameters_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

```shell
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A cinematic shot of a majestic lion in the savannah."
      }
    ],
    "parameters": {
      "aspectRatio": "16:9",
      "negativePrompt": "cartoon, drawing, low quality"
    }
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o parameters_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

--------------------------------

### Generate and Download Video from Text Prompt

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=de

Illustrates how to generate a video using the Gemini API's 'veo-3.0-generate-001' model from a text prompt. The process involves initiating video generation, polling the operation status until completion, and then downloading the final .mp4 video file to a specified path. An API key is required.

```JavaScript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.0-generate-001",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);
```

```Go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

```Shell
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That'\''s the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
      }
    ]
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

--------------------------------

### Generate and Download Video from Text Prompt (Gemini API)

Source: https://ai.google.dev/gemini-api/docs/video_hl=tr

These code examples illustrate how to use the Google Gemini API to generate a video from a text prompt using the `veo-3.0-generate-001` model. The process involves initiating a generation operation, polling its status until completion, and then downloading the resulting video file to local storage.

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.0-generate-001",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);
```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

```bash
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{ "instances": [{ "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That\'s the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\"" } ] }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

--------------------------------

### Generate Text with Gemini Developer API and Vertex AI in Go

Source: https://ai.google.dev/gemini-api/docs/migrate-to-cloud

This code demonstrates how to generate text using the Gemini 2.0 Flash model in Go. It provides side-by-side examples for both the Gemini Developer API and the Vertex AI Gemini API, using the `google.golang.org/genai` library. Ensure the `google.golang.org/genai` library is installed to use these examples.

```go
import (
  "context"
  "encoding/json"
  "fmt"
  "log"
  "google.golang.org/genai"
)

// Your Google API key
const apiKey = "your-api-key"

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  // Call the GenerateContent method.
  result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me about New York?"), nil)

}

```

```go
import (
  "context"
  "encoding/json"
  "fmt"
  "log"
  "google.golang.org/genai"
)

// Your GCP project
const project = "your-project"

// A GCP location like "us-central1"
const location = "some-gcp-location"

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, &genai.ClientConfig
  {
        Project:  project,
      Location: location,
      Backend:  genai.BackendVertexAI,
  })

  // Call the GenerateContent method.
  result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me about New York?"), nil)

}

```

--------------------------------

### Configure Gemini API Key Environment Variable

Source: https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example

This snippet shows how to set the `GOOGLE_GENERATIVE_AI_API_KEY` environment variable. This key is essential for authenticating with the Gemini API, as the Google Generative AI Provider automatically looks for it in this variable.

```bash
export GOOGLE_GENERATIVE_AI_API_KEY="YOUR_API_KEY_HERE"
```

```cmd
setx GOOGLE_GENERATIVE_AI_API_KEY "YOUR_API_KEY_HERE"
```

--------------------------------

### Generate Content: Gemini Developer API vs. Vertex AI (Go)

Source: https://ai.google.dev/gemini-api/docs/migrate-to-cloud_hl=tr

This Go language example demonstrates generating content using the `google.golang.org/genai` library. It presents two client initialization patterns: one for the Gemini Developer API requiring an API key, and another for the Vertex AI Gemini API requiring project and location configuration, both performing a `GenerateContent` call.

```go
import (
  "context"
  "encoding/json"
  "fmt"
  "log"
  "google.golang.org/genai"
)

// Your Google API key
const apiKey = "your-api-key"

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  // Call the GenerateContent method.
  result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me about New York?"), nil)

}
```

```go
import (
  "context"
  "encoding/json"
  "fmt"
  "log"
  "google.golang.org/genai"
)

// Your GCP project
const project = "your-project"

// A GCP location like "us-central1"
const location = "some-gcp-location"

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, &genai.ClientConfig
  {
        Project:  project,
      Location: location,
      Backend:  genai.BackendVertexAI,
  })

  // Call the GenerateContent method.
  result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me about New York?"), nil)

}
```

--------------------------------

### Import Python Libraries for Google GenAI and Audio Processing

Source: https://ai.google.dev/gemini-api/docs/live_hl=it

This snippet imports essential Python libraries for asynchronous operations, I/O, file path manipulation, and audio processing. It includes `asyncio`, `io`, `pathlib`, `wave`, `soundfile` for audio file reading/writing, and `librosa` for audio analysis, alongside `google.genai` for interacting with the Gemini API.

```python
import asyncio
import io
from pathlib import Path
import wave
from google import genai
from google.genai import types
import soundfile as sf
import librosa
```

--------------------------------

### Generate Content: Gemini Developer API vs. Vertex AI (Python)

Source: https://ai.google.dev/gemini-api/docs/migrate-to-cloud_hl=tr

This Python code demonstrates how to use the `google-genai` library to generate text content. It presents two configurations: one for the Gemini Developer API, and another for the Vertex AI Gemini API, showing the `client` initialization differences and a common `generate_content` call.

```python
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.0-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

```python
from google import genai

client = genai.Client(
    vertexai=True, project='your-project-id', location='us-central1'
)

response = client.models.generate_content(
    model="gemini-2.0-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

--------------------------------

### Generate Gemini Embeddings and Calculate Cosine Similarity in Go

Source: https://ai.google.dev/gemini-api/docs/embeddings_hl=th

This Go program demonstrates how to obtain text embeddings using the `google.golang.org/genai` package and then calculate their cosine similarity. It includes a custom `cosineSimilarity` function to perform the vector comparison.

```go
package main

import (
    "context"
    "fmt"
    "log"
    "math"

    "google.golang.org/genai"
)

// cosineSimilarity calculates the similarity between two vectors.
func cosineSimilarity(a, b []float32) (float64, error) {
    if len(a) != len(b) {
        return 0, fmt.Errorf("vectors must have the same length")
    }

    var dotProduct, aMagnitude, bMagnitude float64
    for i := 0; i < len(a); i++ {
        dotProduct += float64(a[i] * b[i])
        aMagnitude += float64(a[i] * a[i])
        bMagnitude += float64(b[i] * b[i])
    }

    if aMagnitude == 0 || bMagnitude == 0 {
        return 0, nil
    }

    return dotProduct / (math.Sqrt(aMagnitude) * math.Sqrt(bMagnitude)), nil
}

func main() {
    ctx := context.Background()
    client, _ := genai.NewClient(ctx, nil)
    defer client.Close()

    texts := []string{
        "What is the meaning of life?",
        "What is the purpose of existence?",
        "How do I bake a cake?",
    }

    var contents []*genai.Content
    for _, text := range texts {
        contents = append(contents, genai.NewContentFromText(text, genai.RoleUser))
    }

    result, _ := client.Models.EmbedContent(ctx,
        "gemini-embedding-001",
        contents,
        &genai.EmbedContentRequest{TaskType: genai.TaskTypeSemanticSimilarity},
    )

    embeddings := result.Embeddings

    for i := 0; i < len(texts); i++ {
        for j := i + 1; j < len(texts); j++ {
            similarity, _ := cosineSimilarity(embeddings[i].Values, embeddings[j].Values)
            fmt.Printf("Similarity between '%s' and '%s': %.4f\n", texts[i], texts[j], similarity)
        }
    }
}
```

--------------------------------

### Generate Text with Gemini Developer API and Vertex AI in Python

Source: https://ai.google.dev/gemini-api/docs/migrate-to-cloud

This code demonstrates how to generate text using the Gemini 2.0 Flash model in Python. It provides side-by-side examples for both the Gemini Developer API and the Vertex AI Gemini API, leveraging the `google-genai` library. Ensure the `google-genai` library is installed to use these examples.

```python
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.0-flash", contents="Explain how AI works in a few words"
)
print(response.text)

```

```python
from google import genai

client = genai.Client(
    vertexai=True, project='your-project-id', location='us-central1'
)

response = client.models.generate_content(
    model="gemini-2.0-flash", contents="Explain how AI works in a few words"
)
print(response.text)

```

--------------------------------

### Generate Simple Text with Gemini using AI SDK

Source: https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example

This TypeScript code demonstrates a basic interaction with the Gemini API using the AI SDK. It initializes the Gemini 2.5 Flash model and uses `generateText` to get a response to a simple prompt, then logs it to the console. This serves as a quick verification of the setup.

```typescript
import { google } from "@ai-sdk/google";
import { generateText } from "ai";

async function main() {
  const { text } = await generateText({
    model: google("gemini-2.5-flash"),
    prompt: 'What is plant-based milk?',
  });

  console.log(text);
}

main().catch(console.error);
```

--------------------------------

### Extract Structured Chart Data and Generate Chart.js Configurations with Gemini API (TypeScript)

Source: https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example

This snippet extends the market research script to extract structured data from the research results for charting. It utilizes AI SDK's `generateObject` with a Zod schema for data validation and includes a helper function (`createChartConfig`) to convert the extracted data into a Chart.js compatible format, showing the full `main.ts` file after adding the new functionality.

```typescript
import { google } from "@ai-sdk/google";
import { generateText, generateObject } from "ai";
import { z } from "zod/v4";
import { ChartConfiguration } from "chart.js";

// Helper function to create Chart.js configurations
function createChartConfig({
  labels,
  data,
  label,
  type,
  colors,
}: {
  labels: string[];
  data: number[];
  label: string;
  type: "bar" | "line";
  colors: string[];
}): ChartConfiguration {
  return {
    type: type,
    data: {
      labels: labels,
      datasets: [
        {
          label: label,
          data: data,
          borderWidth: 1,
          ...(type === "bar" && { backgroundColor: colors }),
          ...(type === "line" && colors.length > 0 && { borderColor: colors[0] }),
        },
      ],
    },
    options: {
      animation: { duration: 0 }, // Disable animations for static PDF rendering
    },
  };
}

async function main() {
  // Step 1: Search market trends
  const { text: marketTrends, sources } = await generateText({
    model: google("gemini-2.5-flash"),
    tools: {
      google_search: google.tools.googleSearch({}),
    },
    prompt: `Search the web for market trends for plant-based milk in North America for 2024-2025.
          I need to know the market size, key players and their market share, and primary consumer drivers.
          `,
  });

  console.log("Market trends found.");

  // Step 2: Extract chart data
  const { object: chartData } = await generateObject({
    model: google("gemini-2.5-flash"),
    schema: z.object({
      chartConfigurations: z
        .array(
          z.object({
            type: z.enum(["bar", "line"]).describe('The type of chart to generate. Either "bar" or "line"'),
            labels: z.array(z.string()).describe("A list of chart labels"),
            data: z.array(z.number()).describe("A list of the chart data"),
            label: z.string().describe("A label for the chart"),
            colors: z.array(z.string()).describe('A list of colors to use for the chart, e.g. "rgba(255, 99, 132, 0.8)"'),
          }),
        )
        .describe("A list of chart configurations"),
    }),
    prompt: `Given the following market trends text, come up with a list of 1-3 meaningful bar or line charts
    and generate chart data.
    
Market Trends:
${marketTrends}
`,
  });

  const chartConfigs = chartData.chartConfigurations.map(createChartConfig);

  console.log("Chart configurations generated.");
}

main().catch(console.error);
```

--------------------------------

### Call Gemini Robotics-ER 1.5 for Image Analysis (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview_hl=tr

This Python script uses the `google.genai` library to interact with the `gemini-robotics-er-1.5-preview` model. It loads an image, constructs a prompt requesting an air quality reading and image zoom, then sends it to the model. The response is iterated to print any text or executable code parts generated by the model, enabling dynamic image manipulation.

--------------------------------

### Generate Text with Gemini Developer API and Vertex AI in JavaScript/TypeScript

Source: https://ai.google.dev/gemini-api/docs/migrate-to-cloud

This code demonstrates how to generate text using the Gemini 2.0 Flash model in JavaScript/TypeScript. It provides side-by-side examples for both the Gemini Developer API and the Vertex AI Gemini API, utilizing the `@google/genai` library. Ensure the `@google/genai` library is installed to use these examples.

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();

```

```javascript
import { GoogleGenAI } from '@google/genai';
const ai = new GoogleGenAI({
  vertexai: true,
  project: 'your_project',
  location: 'your_location',
});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();

```

--------------------------------

### Generate Image with Gemini API (Go)

Source: https://ai.google.dev/gemini-api/docs/image-generation_hl=he

This Go snippet illustrates how to use the `google.golang.org/genai` library to interact with the Gemini API. It reads two image files, creates a multimodal prompt with a text instruction, and then calls the `GenerateContent` method to produce a new image, which is saved locally.

```go
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imgData1, _ := os.ReadFile("/path/to/your/dress.png")
  imgData2, _ := os.ReadFile("/path/to/your/model.png")

  parts := []*genai.Part{
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData1,
      },
    },
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData2,
      },
    },
    genai.NewPartFromText("Create a professional e-commerce fashion photo. Take the blue floral dress from the first image and let the woman from the second image wear it. Generate a realistic, full-body shot of the woman wearing the dress, with the lighting and shadows adjusted to match the outdoor environment."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash-image",
      contents,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "fashion_ecommerce_shot.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

--------------------------------

### POST /v1beta/models/gemini-robotics-er-1.5-preview:generateContent

Source: https://ai.google.dev/gemini-api/docs/robotics-overview

This endpoint allows you to send multimodal input (image and text) to the Gemini Robotics-ER 1.5 model to perform tasks like object detection and spatial reasoning. It returns structured JSON output with detected object coordinates and labels.

```APIDOC
## POST /v1beta/models/gemini-robotics-er-1.5-preview:generateContent

### Description
This endpoint facilitates sending an image along with a natural language prompt to the Gemini Robotics-ER 1.5 model. The model processes the input to identify and label objects within the image, returning their normalized [y, x] coordinates in a specified JSON format.

### Method
POST

### Endpoint
https://generativelanguage.googleapis.com/v1beta/models/gemini-robotics-er-1.5-preview:generateContent

### Parameters
#### Headers
- **x-goog-api-key** (string) - Required - Your API key for authentication.
- **Content-Type** (string) - Required - Must be `application/json`.

#### Request Body
- **contents** (array of objects) - Required - A list of content parts for the model.
  - **parts** (array of objects) - Required - A list of individual content parts.
    - **inlineData** (object) - Optional - Contains base64 encoded image data.
      - **mimeType** (string) - Required - The MIME type of the image (e.g., "image/png").
      - **data** (string) - Required - The base64 encoded image string.
    - **text** (string) - Required - The natural language prompt for the model (e.g., "Point to no more than 10 items in the image...").
- **generationConfig** (object) - Optional - Configuration settings for the content generation.
  - **temperature** (number) - Optional - Controls the randomness of the output. Value typically between 0.0 and 1.0 (e.g., 0.5).
  - **thinkingConfig** (object) - Optional - Configuration for the model's thinking budget.
    - **thinkingBudget** (number) - Optional - Specifies the computational budget for the model's reasoning. (e.g., 0 for low latency).

### Request Example
```bash
IMAGE_BASE64=$(base64 -w 0 my-image.png)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-robotics-er-1.5-preview:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "inlineData": {
              "mimeType": "image/png",
              "data": "'""${IMAGE_BASE64}""'"
            }
          },
          {
            "text": "Point to no more than 10 items in the image. The label returned should be an identifying name for the object detected. The answer should follow the json format: [{\"point\": [y, x], \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000."
          }
        ]
      }
    ],
    "generationConfig": {
      "temperature": 0.5,
      "thinkingConfig": {
        "thinkingBudget": 0
      }
    }
  }'
```

### Response
#### Success Response (200)
- **[array of objects]** - A JSON array, where each object represents a detected item.
  - **point** (array of numbers) - The normalized [y, x] coordinates of the detected object, ranging from 0-1000.
  - **label** (string) - An identifying name for the detected object.

#### Response Example
```json
[
  {"point": [376, 508], "label": "small banana"},
  {"point": [287, 609], "label": "larger banana"},
  {"point": [223, 303], "label": "pink starfruit"},
  {"point": [435, 172], "label": "paper bag"},
  {"point": [270, 786], "label": "green plastic bowl"},
  {"point": [488, 775], "label": "metal measuring cup"},
  {"point": [673, 580], "label": "dark blue bowl"},
  {"point": [471, 353], "label": "light blue bowl"},
  {"point": [492, 497], "label": "bread"},
  {"point": [525, 429], "label": "lime"}
]
```
```

--------------------------------

### Generate, Poll, and Download Video from Text (Go)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=ar

This Go example uses the `google.golang.org/genai` package to generate a video from a text prompt. It includes a polling mechanism to wait for the video generation to complete before downloading and saving the resulting MP4 file locally as 'dialogue_example.mp4'.

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

--------------------------------

### GET /v1beta/{operation_name}

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=hi

Retrieves the current status and results of a long-running operation. This endpoint should be polled periodically until the `done` field in the response becomes `true`.

```APIDOC
## GET /v1beta/{operation_name}

### Description
Once a video generation operation is initiated, you must poll this endpoint to check its progress. When the operation is complete (`done` is `true`), the `response` field will contain the generated video's details, including a URI for download.

### Method
GET

### Endpoint
/v1beta/{operation_name}

### Parameters
#### Path Parameters
- **operation_name** (string) - Required - The full resource name of the operation returned by the `generateVideo` call (e.g., `operations/generate-video-1234567890`).

### Response
#### Success Response (200)
- **name** (string) - The resource name of the operation.
- **metadata** (object) - Contains operation-specific metadata.
- **done** (boolean) - `true` if the operation is complete, `false` otherwise.
- **response** (object) - Present if `done` is `true`. Contains the final results of the operation.
  - **generateVideoResponse** (object) - Specific response for video generation operations.
    - **generatedSamples** (array<object>) - A list of generated video samples.
      - **video** (object) - Details of the generated video file.
        - **uri** (string) - A URL from which the generated video can be downloaded.

#### Response Example (In Progress)
```json
{
  "name": "operations/generate-video-1234567890",
  "metadata": {
    "@type": "type.googleapis.com/google.cloud.generativelanguage.v1beta.GenerateVideoMetadata",
    "createTime": "2023-10-27T10:00:00Z"
  },
  "done": false
}
```

#### Response Example (Completed)
```json
{
  "name": "operations/generate-video-1234567890",
  "metadata": {
    "@type": "type.googleapis.com/google.cloud.generativelanguage.v1beta.GenerateVideoMetadata",
    "createTime": "2023-10-27T10:00:00Z",
    "endTime": "2023-10-27T10:01:30Z"
  },
  "done": true,
  "response": {
    "@type": "type.googleapis.com/google.cloud.generativelanguage.v1beta.GenerateVideoResponse",
    "generatedSamples": [
      {
        "video": {
          "uri": "https://storage.googleapis.com/generated-videos/video-123.mp4?alt=media&token=..."
        }
      }
    ]
  }
}
```
```

--------------------------------

### Install Core AI SDK and TypeScript Dependencies

Source: https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example

This snippet installs essential dependencies for the AI application, including `@ai-sdk/google` for Gemini integration, `ai` for AI functionalities, `zod` for schema validation, and development dependencies like `@types/node`, `tsx`, and `typescript`. It also initializes `tsconfig.json`. After running, remember to comment out `"verbatimModuleSyntax": true` in `tsconfig.json` to prevent a TypeScript compiler error.

```bash
npm install ai @ai-sdk/google zod
npm install -D @types/node tsx typescript && npx tsc --init
```

```bash
pnpm add ai @ai-sdk/google zod
pnpm add -D @types/node tsx typescript
```

```bash
yarn add ai @ai-sdk/google zod
yarn add -D @types/node tsx typescript && yarn tsc --init
```

--------------------------------

### Perform Initial Market Research with Gemini API and Google Search (TypeScript)

Source: https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example

This snippet initializes a TypeScript application using AI SDK to perform market research. It uses the Gemini API with the `google_search` tool to query for specific market trends and logs the results to the console.

```typescript
import { google } from "@ai-sdk/google";
import { generateText } from "ai";

async function main() {
  // Step 1: Search market trends
  const { text: marketTrends, sources } = await generateText({
    model: google("gemini-2.5-flash"),
    tools: {
      google_search: google.tools.googleSearch({}),
    },
    prompt: `Search the web for market trends for plant-based milk in North America for 2024-2025.
          I need to know the market size, key players and their market share, and primary consumer drivers.
          `,
  });

  console.log("Market trends found:\n", marketTrends);
  // To see the sources, uncomment the following line:
  // console.log("Sources:\n", sources);
}

main().catch(console.error);
```

--------------------------------

### Generate Text from Audio with Gemini API in Go

Source: https://ai.google.dev/gemini-api/docs/audio

This Go code demonstrates how to upload an audio file to the Gemini API, create a multimodal prompt combining text and the uploaded audio, and then generate a textual response. It utilizes the `genai.NewPartFromURI` for audio input and `genai.Models.GenerateContent` for inference with the `gemini-2.5-flash` model.

```go
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromText("Provide a transcript of the speech " +
                            "between the timestamps 02:30 and 03:29."),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

--------------------------------

### Perform Spatial Reasoning to Identify Object for Removal with Gemini Robotics API (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview

This Python example demonstrates using the Gemini Robotics-ER 1.5-preview model for advanced spatial reasoning. It takes an image and a prompt to identify which object needs to be moved to create space for another item (e.g., a laptop). The model responds with the normalized [y, x] coordinates and a descriptive label of the object to be removed, showcasing its contextual understanding. A thinking_budget of 0 is used to optimize performance.

```python
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-ro botics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

prompt = """
          Point to the object that I need to remove to make room for my laptop
          The answer should follow the json format: [{"point": <point>,
          "label": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000.
        """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

--------------------------------

### Generate, Poll, and Download Video (Go)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=id

This Go example showcases video generation using the `google.golang.org/genai` library. It sends a video generation request, continuously polls the operation status, and finally downloads and saves the generated video to a local file.

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    videoConfig := &genai.GenerateVideosConfig{
        AspectRatio: "16:9",
        NegativePrompt: "cartoon, drawing, low quality",
    }

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        "A cinematic shot of a majestic lion in the savannah.",
        nil,
        videoConfig,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
        log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "parameters_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

--------------------------------

### Generate and Save PDF Report using Gemini and Puppeteer (TypeScript)

Source: https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example

This TypeScript code orchestrates the generation of a detailed financial market analysis report. It prompts the Gemini API to act as an expert report writer, providing it with market trends, chart configurations, and sources to produce a full HTML document. Subsequently, Puppeteer is used to launch a browser, set the generated HTML content, and save it as a PDF file named 'report.pdf'.

```typescript
// ... (imports from previous step)
import puppeteer from "puppeteer";

// ... (createChartConfig helper function from previous step)

async function main() {
  // ... (Step 1 and 2 from previous step)

  // Step 3: Generate the final HTML report and save it as a PDF
  const { text: htmlReport } = await generateText({
    model: google("gemini-2.5-flash"),
    prompt: `You are an expert financial analyst and report writer.
    Your task is to generate a comprehensive market analysis report in HTML format.

    **Instructions:**
    1.  Write a full HTML document.
    2.  Use the provided "Market Trends" text to write the main body of the report. Structure it with clear headings and paragraphs.
    3.  Incorporate the provided "Chart Configurations" to visualize the data. For each chart, you MUST create a unique <canvas> element and a corresponding <script> block to render it using Chart.js.
    4.  Reference the "Sources" at the end of the report.
    5.  Do not include any placeholder data; use only the information provided.
    6.  Return only the raw HTML code.

    **Chart Rendering Snippet:**
    Include this script in the head of the HTML: <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    For each chart, use a structure like below, ensuring the canvas 'id' is unique for each chart, and apply the correspinding config:

    ---
    <div style="width: 800px; height: 600px;">
      <canvas id="chart1"></canvas>
    </div>
    <script>
      new Chart(document.getElementById('chart1'), config);
    </script>
    ---
    (For the second chart, use 'chart2' and the corresponding config, and so on.)

    **Data:**
    - Market Trends: ${marketTrends}
    - Chart Configurations: ${JSON.stringify(chartConfigs)}
    - Sources: ${JSON.stringify(sources)}
    `,
  });

  // LLMs may wrap the HTML in a markdown code block, so strip it.
  const finalHtml = htmlReport.replace(/^```html\n/, "").replace(/\n```$/, "");

  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.setContent(finalHtml);
  await page.pdf({ path: "report.pdf", format: "A4" });
  await browser.close();

  console.log("\nReport generated successfully: report.pdf");
}

main().catch(console.error);
```

--------------------------------

### Identify Object for Spatial Reasoning with Gemini Robotics-ER 1.5 (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview_hl=ja

This Python code demonstrates Gemini Robotics-ER 1.5's advanced spatial reasoning by initializing the client and loading an image with a prompt. The model is asked to identify an object that needs to be moved to create space for a laptop, returning the object's point coordinates in a JSON format.

```Python
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

prompt = """
          Point to the object that I need to remove to make room for my laptop
          The answer should follow the json format: [{"point": <point>,
          "label": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000.
        """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

--------------------------------

### Generate Video and Download with Gemini API

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=es-419

These snippets demonstrate how to generate videos using the Gemini API in Javascript and Go, including polling for the operation status and downloading the generated video. They rely on the @google/genai library (JS) and google.golang.org/genai (Go).

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.0-generate-001",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);
```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

--------------------------------

### Stream Content with Google Gemini API (Aprs)

Source: https://ai.google.dev/gemini-api/docs/migrate_hl=fr

This snippet showcases the updated approach to streaming content generation from the Google Gemini API. It provides examples in Python, JavaScript, and Go, utilizing the `models.generate_content_stream` method for a more streamlined interaction. The examples iterate through the streamed chunks, printing or accumulating the text content.

```python
from google import genai

client = genai.Client()

for chunk in client.models.generate_content_stream(
  model='gemini-2.0-flash',
  contents='Tell me a story in 300 words.'
):
    print(chunk.text)
```

```javascript
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContentStream({
  model: "gemini-2.0-flash",
  contents: "Write a story about a magic backpack.",
});
let text = "";
for await (const chunk of response) {
  console.log(chunk.text);
  text += chunk.text;
}
```

```go
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

for result, err := range client.Models.GenerateContentStream(
    ctx,
    "gemini-2.0-flash",
    genai.Text("Write a story about a magic backpack."),
    nil,
) {
    if err != nil {
        log.Fatal(err)
    }
    fmt.Print(result.Candidates[0].Content.Parts[0].Text)
}
```

--------------------------------

### Generate Multimodal Content with Image and Text in Go

Source: https://ai.google.dev/gemini-api/docs/text-generation

This Go example illustrates how to send an image and a text prompt to the Gemini API. It reads an image file from a specified path, constructs inline data parts for the image and a text part, then uses the `genai` client library to perform content generation.

```go
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/organ.jpg"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
      genai.NewPartFromText("Tell me about this instrument"),
      &genai.Part{
          InlineData: &genai.Blob{
              MIMEType: "image/jpeg",
              Data:     imgData,
          },
      },
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

--------------------------------

### Define Custom Functions for Android Mobile Automation with Gemini API (Python)

Source: https://ai.google.dev/gemini-api/docs/computer-use_hl=sq

This Python code defines three custom functions: `open_app`, `long_press_at`, and `go_home`. These functions simulate actions on an Android device, providing a structured way for the Gemini model to interact with mobile applications and the device's UI, rather than relying on browser-specific actions.

```python
def open_app(app_name: str, intent: Optional[str] = None) -> Dict[str, Any]:
    """Opens an app by name.

    Args:
        app_name: Name of the app to open (any string).
        intent: Optional deep-link or action to pass when launching, if the app supports it.

    Returns:
        JSON payload acknowledging the request (app name and optional intent).
    """
    return {"status": "requested_open", "app_name": app_name, "intent": intent}

def long_press_at(x: int, y: int) -> Dict[str, int]:
    """Long-press at a specific screen coordinate.

    Args:
        x: X coordinate (absolute), scaled to the device screen width (pixels).
        y: Y coordinate (absolute), scaled to the device screen height (pixels).

    Returns:
        Object with the coordinates pressed and the duration used.
    """
    return {"x": x, "y": y}

def go_home() -> Dict[str, str]:
    """Navigates to the device home screen.

    Returns:
        A small acknowledgment payload.
    """
    return {"status": "home_requested"}
```

--------------------------------

### Generate Content: Gemini Developer API vs. Vertex AI (JavaScript/TypeScript)

Source: https://ai.google.dev/gemini-api/docs/migrate-to-cloud_hl=tr

This JavaScript/TypeScript example showcases how to generate text content using the `@google/genai` library. It illustrates client instantiation differences between the Gemini Developer API and the Vertex AI Gemini API, followed by an asynchronous `generateContent` call.

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

```javascript
import { GoogleGenAI } from '@google/genai';
const ai = new GoogleGenAI({
  vertexai: true,
  project: 'your_project',
  location: 'your_location',
});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

--------------------------------

### Run Basic Gemini Verification Script

Source: https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example

This snippet provides commands to compile and execute the `main.ts` file, which contains the basic Gemini text generation example. Running this command verifies that your environment and API key are correctly configured.

```bash
npx tsc && node main.js
```

```bash
pnpm tsx main.ts
```

```bash
yarn tsc && node main.js
```

--------------------------------

### GET /operations/{operationId}

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=sq

Retrieves the current status and details of a long-running operation, such as video generation, identified by its operation ID.

```APIDOC
## GET /operations/{operationId}

### Description
Retrieves the current status and details of a long-running operation, such as video generation, identified by its unique operation ID. This endpoint is used to poll for the completion and results of asynchronous tasks.

### Method
GET

### Endpoint
/operations/{operationId}

### Parameters
#### Path Parameters
- **operationId** (string) - Required - The unique identifier of the long-running operation.

### Request Example
(No request body for GET requests with path parameters)

### Response
#### Success Response (200 - Operation Object)
- **name** (string) - The unique identifier for the long-running operation.
- **done** (boolean) - Indicates if the operation has completed.
- **metadata** (object) - Contains operation-specific metadata (e.g., `state` for video generation).
- **response** (object) - Present if `done` is true, contains the actual result of the operation (e.g., `generatedVideos` for video generation).
- **error** (object) - Present if `done` is true and the operation failed, containing error details.

#### Response Example
```json
{
  "name": "operations/some-operation-id",
  "done": false,
  "metadata": {
    "@type": "type.googleapis.com/google.genai.v1.GenerateVideosMetadata",
    "state": "RUNNING"
  }
}
```

```json
{
  "name": "operations/some-operation-id",
  "done": true,
  "response": {
    "@type": "type.googleapis.com/google.genai.v1.GenerateVideosResponse",
    "generatedVideos": [
      {
        "video": {
          "name": "files/video-file-id",
          "mimeType": "video/mp4"
        }
      }
    ]
  }
}
```

```json
{
  "name": "operations/some-failed-operation-id",
  "done": true,
  "error": {
    "code": 500,
    "message": "Internal Server Error"
  }
}
```
```

--------------------------------

### Python: Start Video Generation Operation

Source: https://ai.google.dev/gemini-api/docs/video_hl=hi

This Python snippet demonstrates how to initiate a video generation task using the Google Gemini API client. It sends a prompt and configuration to the `generate_videos` method, which returns an `operation` object. Additionally, it shows how an `operation` object can be reconstructed or referenced later using its `name` property.

```python
import time
from google import genai
from google.genai import types

client = genai.Client()

# After starting the job, you get an operation object.
operation = client.models.generate_videos(
    model="veo-3.0-generate-001",
    prompt="A cinematic shot of a majestic lion in the savannah.",
)

# Alternatively, you can use operation.name to get the operation.
operation = types.GenerateVideosOperation(name=operation.name)
```

--------------------------------

### POST /v1beta/models/{model}:generateContent - Multimodal

Source: https://ai.google.dev/gemini-api/docs/vision_hl=vi

Generates content using a specified Gemini model, combining text with multiple images provided via both uploaded file URIs and inline base64 data.

```APIDOC
## POST /v1beta/models/{model}:generateContent

### Description
Generates content using a specified Gemini model, combining text with multiple images provided via both uploaded file URIs and inline base64 data.

### Method
POST

### Endpoint
`https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent`

### Parameters
#### Path Parameters
- **model** (string) - Required - The name of the model to use for generation (e.g., `gemini-2.5-flash`).

#### Query Parameters
(None)

#### Request Headers
- **x-goog-api-key** (string) - Required - Your Google API key.
- **Content-Type** (string) - Required - `application/json`.

#### Request Body
- **contents** (array<object>) - Required - An array of `Content` objects, representing the conversation turn.
    - **role** (string) - Optional - The role of the author of this content. Can be `user` or `model`. Defaults to `user` if not specified.
    - **parts** (array<object>) - Required - A list of `Part` objects, which can be text, image data, or file data.
        - **text** (string) - Optional - Text content.
        - **inline_data** (object) - Optional - Inline image data.
            - **mime_type** (string) - Required - The MIME type of the image (e.g., `image/png`, `image/jpeg`).
            - **data** (string) - Required - Base64 encoded image data.
        - **file_data** (object) - Optional - Reference to an uploaded file.
            - **mime_type** (string) - Required - The MIME type of the file.
            - **file_uri** (string) - Required - The URI of the file obtained from a file upload operation.

### Request Example
```json
{
  "contents": [
    {
      "parts": [
        {
          "text": "What is different between these two images?"
        },
        {
          "file_data": {
            "mime_type": "image/jpeg",
            "file_uri": "gemini-api://file.googleapis.com/uploaded-file-uri"
          }
        },
        {
          "inline_data": {
            "mime_type": "image/png",
            "data": "iVBORw0KGgoAAAANSUhEUgAAA...[base64 encoded image data]"
          }
        }
      ]
    }
  ]
}
```

### Response
#### Success Response (200)
- **candidates** (array<object>) - A list of generated response candidates.
    - **content** (object) - The generated content.
        - **parts** (array<object>) - List of parts in the generated content.
            - **text** (string) - The generated text.
    - **finish_reason** (string) - The reason why the model stopped generating (e.g., `STOP`, `MAX_TOKENS`).
    - **safety_ratings** (array<object>) - Safety attributes for the response.

#### Response Example
```json
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "The first image shows a cat sleeping on a blue blanket, while the second image shows the same cat awake and looking at the camera on a red blanket."
          }
        ]
      },
      "finish_reason": "STOP",
      "safety_ratings": []
    }
  ]
}
```
```

--------------------------------

### GET /operations/{operation_name}

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=ja

Retrieves the current status of a long-running operation, such as video generation. This endpoint is polled until the operation is marked as 'done'.

```APIDOC
## GET /operations/{operation_name}

### Description
Polls the status of a long-running operation (e.g., video generation). The response indicates whether the operation is `done` and, if so, contains the result or error.

### Method
GET

### Endpoint
`${BASE_URL}/operations/{operation_name}`

### Parameters
#### Path Parameters
- **operation_name** (string) - Required - The name of the operation obtained from a `predictLongRunning` request.

### Request Example
```bash
curl -s -H "x-goog-api-key: $GEMINI_API_KEY" \
  "${BASE_URL}/operations/generate-video-0123456789abcdef"
```

### Response
#### Success Response (200)
- **name** (string) - The name of the operation.
- **done** (boolean) - Indicates if the operation is complete (`true`) or still in progress (`false`).
- **response** (object) - Present when `done` is `true`. Contains the actual result of the operation.
  - **@type** (string) - The type of the response payload (e.g., `type.googleapis.com/google.ai.generativelanguage.v1beta.GenerateVideoResponse`).
  - **generateVideoResponse** (object) - The specific response for video generation.
    - **generatedSamples** (array of objects) - An array of generated video samples.
      - **video** (object) - The generated video details.
        - **uri** (string) - The URI from which the generated video can be downloaded.
        - **mimeType** (string) - The MIME type of the video (e.g., `video/mp4`).

#### Response Example
```json
{
  "name": "operations/generate-video-0123456789abcdef",
  "done": true,
  "response": {
    "@type": "type.googleapis.com/google.ai.generativelanguage.v1beta.GenerateVideoResponse",
    "generateVideoResponse": {
      "generatedSamples": [
        {
          "video": {
            "uri": "https://storage.googleapis.com/ai-platform-video-outputs/some_path/dialogue_example.mp4",
            "mimeType": "video/mp4"
          }
        }
      ]
    }
  }
}
```
```

--------------------------------

### POST /v1beta/models/{model_name}:generateContent

Source: https://ai.google.dev/gemini-api/docs/vision

Generates multi-modal content using a specified Gemini model. This endpoint supports combining text with multiple images, where images can be referenced by a URI (after being uploaded) or provided directly as base64 encoded inline data.

```APIDOC
## POST /v1beta/models/{model_name}:generateContent

### Description
Sends a request to the Gemini model to generate content based on a prompt that includes text and multiple images. Images can be supplied via a URI (from a prior file upload) or embedded directly as base64 data.

### Method
POST

### Endpoint
/v1beta/models/{model_name}:generateContent

### Parameters
#### Path Parameters
- **model_name** (string) - Required - The name of the Gemini model to use (e.g., 'gemini-2.5-flash').

#### Request Body
- **contents** (array of objects) - Required - A list of content blocks, typically representing a turn in a conversation or a single multi-modal prompt.
  - **parts** (array of objects) - Required - A list of parts that compose a content block.
    - **text** (string) - A text part of the prompt.
    - **file_data** (object) - An image part referenced by a URI.
      - **mime_type** (string) - Required - The MIME type of the image (e.g., 'image/jpeg', 'image/png').
      - **file_uri** (string) - Required - The URI of the previously uploaded file.
    - **inline_data** (object) - An image part provided directly as base64 encoded data.
      - **mime_type** (string) - Required - The MIME type of the image (e.g., 'image/jpeg', 'image/png').
      - **data** (string) - Required - Base64 encoded image data.

### Request Example
```json
{
  "contents": [
    {
      "parts": [
        {
          "text": "What is different between these two images?"
        },
        {
          "file_data": {
            "mime_type": "image/jpeg",
            "file_uri": "https://generativelanguage.googleapis.com/v1beta/files/your-file-id"
          }
        },
        {
          "inline_data": {
            "mime_type": "image/png",
            "data": "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="
          }
        }
      ]
    }
  ]
}
```

### Response
#### Success Response (200)
- **candidates** (array) - A list of generated content candidates.
  - **content** (object) - The generated content.
    - **parts** (array) - A list of parts that compose the generated content.
      - **text** (string) - The generated text response.
- **promptFeedback** (object) - Feedback about the prompt, including safety ratings.

#### Response Example
```json
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "The first image shows a mountain range under a clear sky, while the second image shows a similar mountain range but with a snowy peak and possibly a different lighting condition, suggesting a change in season or time of day."
          }
        ]
      },
      "finishReason": "STOP",
      "index": 0,
      "safetyRatings": [
        {
          "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
          "probability": "NEGLIGIBLE"
        },
        {
          "category": "HARM_CATEGORY_HATE_SPEECH",
          "probability": "NEGLIGIBLE"
        },
        {
          "category": "HARM_CATEGORY_HARASSMENT",
          "probability": "NEGLIGIBLE"
        },
        {
          "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
          "probability": "NEGLIGIBLE"
        }
      ]
    }
  ],
  "promptFeedback": {
    "safetyRatings": [
      {
        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
        "probability": "NEGLIGIBLE"
      },
      {
        "category": "HARM_CATEGORY_HATE_SPEECH",
        "probability": "NEGLIGIBLE"
      },
      {
        "category": "HARM_CATEGORY_HARASSMENT",
        "probability": "NEGLIGIBLE"
      },
      {
        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
        "probability": "NEGLIGIBLE"
      }
    ]
  }
}
```
```

--------------------------------

### Initiate Video Generation and Poll for Completion (Python)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=ko

This Python snippet initiates a video generation job with the Gemini API and then continuously polls the returned operation object. It checks the `done` status every 10 seconds until the video generation task is complete, demonstrating how to handle long-running asynchronous operations.

```python
import time
from google import genai
from google.genai import types

client = genai.Client()

# After starting the job, you get an operation object.
operation = client.models.generate_videos(
    model="veo-3.0-generate-001",
    prompt="A cinematic shot of a majestic lion in the savannah.",
)

# Alternatively, you can use operation.name to get the operation.
operation = types.GenerateVideosOperation(name=operation.name)

# This loop checks the job status every 10 seconds.
while not operation.done:
    time.sleep(10)
    # Refresh the operation object to get the latest status.
    operation = client.operations.get(operation)

# Once done, the result is in operation.response.
```

--------------------------------

### Generate Content from Image with Newer Gemini SDK (Python, JS, Go)

Source: https://ai.google.dev/gemini-api/docs/migrate_hl=fr

These examples showcase the updated Gemini API SDKs for generating text from image inputs, utilizing the `gemini-2.0-flash` model. The newer SDKs offer more convenient methods for image handling, such as direct `PIL.Image` object passing in Python, file upload and URI usage in JavaScript, and `genai.Blob` for inline data in Go. Ensure you have the latest `google-generativeai` (Python), `@google/genai` (JavaScript), and `genai` (Go) libraries installed.

```python
from google import genai
from PIL import Image

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=[
        'Tell me a story based on this image',
        Image.open(image_path)
    ]
)
print(response.text)
```

```javascript
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const organ = await ai.files.upload({
  file: "path/to/organ.jpg",
});

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: [
    createUserContent([
      "Tell me a story based on this image",
      createPartFromUri(organ.uri, organ.mimeType)
    ]),
  ],
});
console.log(response.text);
```

```go
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

imgData, err := os.ReadFile("path/to/organ.jpg")
if err != nil {
    log.Fatal(err)
}

parts := []*genai.Part{
    {Text: "Tell me a story based on this image"},
    {InlineData: &genai.Blob{Data: imgData, MIMEType: "image/jpeg"}},
}
contents := []*genai.Content{
    {Parts: parts},
}

result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", contents, nil)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

--------------------------------

### GET /operations/{operation_name}

Source: https://ai.google.dev/gemini-api/docs/video_hl=de

Retrieves the current status and results of a long-running video generation operation. This endpoint should be polled until the operation's 'done' field is true.

```APIDOC
## GET /operations/{operation_name}

### Description
Retrieves the current status and results of a long-running video generation operation. This endpoint should be polled until the operation's 'done' field is true.

### Method
GET

### Endpoint
/operations/{operation_name}

### Parameters
#### Path Parameters
- **operation_name** (string) - Required - The resource name of the operation (e.g., "operations/generate-video-12345") returned by the initial `predictLongRunning` request.

#### Query Parameters
- None

#### Request Body
- None

### Request Example
- Not applicable for GET request with path parameters.

### Response
#### Success Response (200)
- **name** (string) - The resource name of the operation.
- **metadata** (object) - Operation-specific metadata.
- **done** (boolean) - Indicates whether the operation has completed. `true` when the video is ready or an error occurred.
- **error** (object) - Contains error details if the operation failed.
- **response** (object) - The actual result if `done` is `true` and no error occurred.
  - **@type** (string) - Type URL for the response payload.
  - **generateVideoResponse** (object) - The specific video generation response.
    - **generatedSamples** (array of objects) - List of generated video samples.
      - **video** (object) - Details of the generated video.
        - **uri** (string) - The URI from which the generated video file can be downloaded.
        - **videoBytes** (string) - (Optional) Base64 encoded video content, typically for smaller videos or direct embedding.

#### Response Example
```json
{
  "name": "operations/generate-video-12345",
  "metadata": {},
  "done": true,
  "response": {
    "@type": "type.googleapis.com/google.genai.v1beta.GenerateVideoResponse",
    "generatedSamples": [
      {
        "video": {
          "uri": "https://storage.googleapis.com/genai-videos/video_id.mp4"
        }
      }
    ]
  }
}
```
```

--------------------------------

### Generate, Poll, and Download Video from Text Prompt

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=he

No description

--------------------------------

### GET /operations/{operation_name} - Get Operation Status

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=pl

Retrieves the current status and results of a previously initiated long-running operation, such as video generation. This endpoint is typically polled until the `done` field is `true`, at which point the response will contain the generated video's download URI.

```APIDOC
## GET /operations/{operation_name}

### Description
Retrieves the current status and results of a previously initiated long-running operation, such as video generation. This endpoint is typically polled until the `done` field is `true`, at which point the response will contain the generated video's download URI.

### Method
GET

### Endpoint
/operations/{operation_name}

### Parameters
#### Path Parameters
- **operation_name** (string) - Required - The resource name of the operation obtained from the initial generation request.

### Request Example
(No request body for GET requests with path parameters)

### Response
#### Success Response (200) - Pending
- **name** (string) - The resource name of the operation.
- **metadata** (object) - Additional metadata about the operation.
- **done** (boolean) - `false` if the operation is still in progress.

#### Success Response (200) - Done
- **name** (string) - The resource name of the operation.
- **metadata** (object) - Additional metadata.
- **done** (boolean) - `true` when the operation is complete.
- **response** (object) - The actual response data when `done` is `true`.
  - **generateVideoResponse** (object) - Specific to video generation.
    - **generatedSamples** (array of objects)
      - **video** (object)
        - **uri** (string) - A pre-signed URL to download the generated video.
        - **mimeType** (string) - The MIME type of the video (e.g., `video/mp4`).

#### Response Example (Pending)
```json
{
  "name": "operations/generate-video-12345",
  "metadata": {
    "@type": "type.googleapis.com/google.cloud.generativelanguage.v1beta.GenerateVideoMetadata",
    "createTime": "2024-01-01T12:00:00Z"
  },
  "done": false
}
```

#### Response Example (Done)
```json
{
  "name": "operations/generate-video-12345",
  "metadata": {
    "@type": "type.googleapis.com/google.cloud.generativelanguage.v1beta.GenerateVideoMetadata",
    "createTime": "2024-01-01T12:00:00Z",
    "doneTime": "2024-01-01T12:05:00Z"
  },
  "done": true,
  "response": {
    "@type": "type.googleapis.com/google.cloud.generativelanguage.v1beta.GenerateVideoResponse",
    "generatedSamples": [
      {
        "video": {
          "uri": "https://storage.googleapis.com/sample-videos/dialogue_example.mp4?alt=media&token=some_token",
          "mimeType": "video/mp4"
        }
      }
    ]
  }
}
```
```

--------------------------------

### POST /v1beta/models/gemini-embedding-001:embedContent

Source: https://ai.google.dev/gemini-api/docs/embeddings_hl=ko

Generates vector embeddings for a given set of text content, optimized for various machine learning tasks like semantic similarity, classification, and retrieval.

```APIDOC
## POST /v1beta/models/gemini-embedding-001:embedContent

### Description
Generates vector embeddings for a given set of text content, optimized for various machine learning tasks like semantic similarity, classification, and retrieval.

### Method
POST

### Endpoint
/v1beta/models/gemini-embedding-001:embedContent

### Parameters
#### Path Parameters
- No path parameters.

#### Query Parameters
- No query parameters.

#### Request Body
- **task_type** (string) - Required - The task type for which the embeddings will be used. Specifies the optimization goal for the embedding model.
    - **Supported values**: `SEMANTIC_SIMILARITY` (Optimized for text similarity), `CLASSIFICATION` (Optimized for classifying text into preset labels), `CLUSTERING` (Optimized for clustering text based on similarity), `RETRIEVAL_DOCUMENT` (Optimized for document retrieval), `RETRIEVAL_QUERY` (Optimized for general search queries), `CODE_RETRIEVAL_QUERY` (Optimized for retrieving code blocks based on natural language queries), `QUESTION_ANSWERING` (Optimized for question answering systems), `FACT_VERIFICATION` (Optimized for verifying statements by retrieving evidence).
- **content** (object) - Required - The content to embed.
    - **parts** (array of objects) - Required - A list of parts representing the content.
        - Each part object contains:
            - **text** (string) - Required - The text content to embed.

### Request Example
```json
{
  "task_type": "SEMANTIC_SIMILARITY",
  "content": {
    "parts": [
      {
        "text": "What is the meaning of life?"
      },
      {
        "text": "How much wood would a woodchuck chuck?"
      },
      {
        "text": "How does the brain work?"
      }
    ]
  }
}
```

### Response
#### Success Response (200)
- **embeddings** (array of objects) - A list of embedding objects, one for each input content part.
    - Each embedding object contains:
        - **values** (array of floats) - The embedding vector for the corresponding content part.

#### Response Example
```json
{
  "embeddings": [
    {
      "values": [0.0123, 0.4567, 0.8901, 0.1234, 0.5678, 0.9012, 0.3456, 0.7890]
    },
    {
      "values": [0.1122, 0.3344, 0.5566, 0.7788, 0.9900, 0.1100, 0.2211, 0.4433]
    },
    {
      "values": [0.9988, 0.7766, 0.1100, 0.3322, 0.5544, 0.7766, 0.9988, 0.1100]
    }
  ]
}
```
```

--------------------------------

### GET /operations/{operation_name} (Get Operation Status)

Source: https://ai.google.dev/gemini-api/docs/video_hl=vi

Retrieves the current status of a long-running video generation operation. The response indicates if the operation is complete and, if so, includes the generated video details.

```APIDOC
## GET /operations/{operation_name}

### Description
Retrieves the current status of a long-running video generation operation. The response indicates if the operation is complete (`done`) and, if so, includes the generated video details.

### Method
GET

### Endpoint
/operations/{operation_name}

### Parameters
#### Path Parameters
- **operation_name** (string) - Required - The name of the long-running operation obtained from the video generation request.

### Request Example
(No request body for GET)

### Response
#### Success Response (200)
- **name** (string) - The unique identifier for the operation.
- **metadata** (object) - Additional metadata about the operation.
  - **@type** (string) - Type of metadata (e.g., `type.googleapis.com/google.ai.generativelanguage.v1beta.GenerateVideoMetadata`).
  - **createTime** (string) - Timestamp of operation creation.
- **done** (boolean) - Indicates if the operation has completed.
- **response** (object) - Present if `done` is true, contains details of the generated video.
  - **@type** (string) - Type of response (e.g., `type.googleapis.com/google.ai.generativelanguage.v1beta.GenerateVideoResponse`).
  - **generatedSamples** (array of objects) - List of generated videos.
    - **video** (object) - Details of a single video.
      - **uri** (string) - A pre-signed URL to download the video.

#### Response Example
```json
{
  "name": "operations/generate-video-12345",
  "metadata": {
    "@type": "type.googleapis.com/google.ai.generativelanguage.v1beta.GenerateVideoMetadata",
    "createTime": "2024-01-01T12:00:00Z"
  },
  "done": true,
  "response": {
    "@type": "type.googleapis.com/google.ai.generativelanguage.v1beta.GenerateVideoResponse",
    "generatedSamples": [
      {
        "video": {
          "uri": "https://storage.googleapis.com/genai-api-videos/video_123.mp4?alt=media&token=abcdef"
        }
      }
    ]
  }
}
```
```

--------------------------------

### Generate Gemini TTS Audio and Save as WAV (Python)

Source: https://ai.google.dev/gemini-api/docs/speech-generation_hl=ar

This Python snippet showcases how to interact with the Gemini API to convert text into speech, utilizing a specific voice ("Kore"). It includes a custom `wave_file` helper function to properly format and save the raw PCM audio data received from the API into a standard `.wav` file on the local filesystem. This requires the `google-generativeai` library and the built-in `wave` module.

```python
def wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):
   with wave.open(filename, "wb") as wf:
      wf.setnchannels(channels
```

--------------------------------

### Calculate Cosine Similarity of Gemini Embeddings (Python, JavaScript, Go)

Source: https://ai.google.dev/gemini-api/docs/embeddings_hl=zh-tw

This code demonstrates how to obtain text embeddings (or assumes they are already available) and then calculate the cosine similarity between them. The Python example focuses on the calculation using NumPy, while the JavaScript and Go examples show the full workflow including embedding generation using their respective Google GenAI SDKs. Higher scores indicate greater semantic similarity.

```python
embeddings_matrix = np.array(result)
similarity_matrix = cosine_similarity(embeddings_matrix)

for i, text1 in enumerate(texts):
    for j in range(i + 1, len(texts)):
        text2 = texts[j]
        similarity = similarity_matrix[i, j]
        print(f"Similarity between '{text1}' and '{text2}': {similarity:.4f}")
```

```javascript
import { GoogleGenAI } from "@google/genai";
import * as cosineSimilarity from "compute-cosine-similarity";

async function main() {
    const ai = new GoogleGenAI({});

    const texts = [
        "What is the meaning of life?",
        "What is the purpose of existence?",
        "How do I bake a cake?",
    ];

    const response = await ai.models.embedContent({
        model: 'gemini-embedding-001',
        contents: texts,
        taskType: 'SEMANTIC_SIMILARITY'
    });

    const embeddings = response.embeddings.map(e => e.values);

    for (let i = 0; i < texts.length; i++) {
        for (let j = i + 1; j < texts.length; j++) {
            const text1 = texts[i];
            const text2 = texts[j];
            const similarity = cosineSimilarity(embeddings[i], embeddings[j]);
            console.log(`Similarity between '${text1}' and '${text2}': ${similarity.toFixed(4)}`);
        }
    }
}

main();
```

```go
package main

import (
    "context"
    "fmt"
    "log"
    "math"

    "google.golang.org/genai"
)

// cosineSimilarity calculates the similarity between two vectors.
func cosineSimilarity(a, b []float32) (float64, error) {
    if len(a) != len(b) {
        return 0, fmt.Errorf("vectors must have the same length")
    }

    var dotProduct, aMagnitude, bMagnitude float64
    for i := 0; i < len(a); i++ {
        dotProduct += float64(a[i] * b[i])
        aMagnitude += float64(a[i] * a[i])
        bMagnitude += float64(b[i] * b[i])
    }

    if aMagnitude == 0 || bMagnitude == 0 {
        return 0, nil
    }

    return dotProduct / (math.Sqrt(aMagnitude) * math.Sqrt(bMagnitude)), nil
}

func main() {
    ctx := context.Background()
    client, _ := genai.NewClient(ctx, nil)
    defer client.Close()

    texts := []string{
        "What is the meaning of life?",
        "What is the purpose of existence?",
        "How do I bake a cake?",
    }

    var contents []*genai.Content
    for _, text := range texts {
        contents = append(contents, genai.NewContentFromText(text, genai.RoleUser))
    }

    result, _ := client.Models.EmbedContent(ctx,
        "gemini-embedding-001",
        contents,
        &genai.EmbedContentRequest{TaskType: genai.TaskTypeSemanticSimilarity},
    )

    embeddings := result.Embeddings

    for i := 0; i < len(texts); i++ {
        for j := i + 1; j < len(texts); j++ {
            similarity, _ := cosineSimilarity(embeddings[i].Values, embeddings[j].Values)
            fmt.Printf("Similarity between '%s' and '%s': %.4f\n", texts[i], texts[j], similarity)
        }
    }
}
```

--------------------------------

### Edit Image with Gemini 2.5 Flash in Go

Source: https://ai.google.dev/gemini-api/docs/image-generation_hl=bn

This Go program demonstrates using the `google.golang.org/genai` package to perform image editing. It reads an image from the filesystem, constructs a multimodal request with the image and a text prompt, and then saves the output image generated by the `gemini-2.5-flash-image` model.

```go
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/your/living_room.png"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData,
      },
    },
    genai.NewPartFromText("Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa. Keep the rest of the room, including the pillows on the sofa and the lighting, unchanged."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash-image",
      contents,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "living_room_edited.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

--------------------------------

### Call Gemini Robotics-ER API with cURL and Base64 Encoded Image

Source: https://ai.google.dev/gemini-api/docs/robotics-overview

This script demonstrates how to prepare an image by base64 encoding it and then calling the Gemini Robotics-ER 1.5 API using `curl`. It sends an image and a text prompt to detect up to 10 items, expecting a JSON output of normalized [y, x] coordinates and labels. Ensure `GEMINI_API_KEY` is set and `my-image.png` exists.

```bash
IMAGE_BASE64=$(base64 -w 0 my-image.png)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-robotics-er-1.5-preview:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "inlineData": {
              "mimeType": "image/png",
              "data": "'""${IMAGE_BASE64}""'"
            }
          },
          {
            "text": "Point to no more than 10 items in the image. The label returned should be an identifying name for the object detected. The answer should follow the json format: [{"point": [y, x], "label": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000."
          }
        ]
      }
    ],
    "generationConfig": {
      "temperature": 0.5,
      "thinkingConfig": {
        "thinkingBudget": 0
      }
    }
  }'
```

--------------------------------

### Detect Objects with Bounding Boxes using Gemini Robotics-ER 1.5 (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview_hl=id

This Python script demonstrates how to utilize the Gemini Robotics-ER 1.5 model for detecting objects and extracting their 2D bounding boxes, along with descriptive labels. It initializes the `genai` client, loads an image, and constructs a sophisticated prompt that specifies output format as a JSON array of bounding boxes (normalized [ymin, xmin, ymax, xmax]) and unique labels. The prompt also includes constraints such as a limit of 25 objects and instructions for naming multiple instances based on their characteristics.

```python
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

prompt = """
          Return bounding boxes as a JSON array with labels. Never return masks
          or code fencing. Limit to 25 objects. Include as many objects as you
          can identify on the table.
          If an object is present multiple times, name them according to their
          unique characteristic (colors, size, position, unique characteristics, etc..).
          The format should be as follows: [{"box_2d": [ymin, xmin, ymax, xmax],
          "label": <label for the object>}] normalized to 0-1000. The values in
          box_2d must only be integers
          """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

--------------------------------

### POST /models/{model_id}:predictLongRunning

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=hi

Initiates a long-running operation to generate a video based on a textual prompt and configuration parameters. The API returns an operation object immediately, which must then be polled for completion.

```APIDOC
## POST /models/{model_id}:predictLongRunning

### Description
This endpoint starts the video generation process asynchronously. It takes a prompt and optional configuration to define the video content and characteristics. A long-running operation resource is returned immediately, which represents the ongoing video generation task.

### Method
POST

### Endpoint
/v1beta/models/{model_id}:predictLongRunning

### Parameters
#### Path Parameters
- **model_id** (string) - Required - The ID of the video generation model to use (e.g., `veo-3.0-generate-001`).

#### Request Body
- **instances** (array<object>) - Required - A list of video generation requests.
  - **prompt** (string) - Required - The text description that guides the video generation.
- **parameters** (object) - Optional - Configuration options for video generation.
  - **aspectRatio** (string) - Optional - The desired aspect ratio of the video (e.g., "16:9", "1:1").
  - **negativePrompt** (string) - Optional - A prompt specifying content to avoid or exclude from the generated video.

### Request Example
```json
{
  "instances": [
    {
      "prompt": "A cinematic shot of a majestic lion in the savannah."
    }
  ],
  "parameters": {
    "aspectRatio": "16:9",
    "negativePrompt": "cartoon, drawing, low quality"
  }
}
```

### Response
#### Success Response (200)
- **name** (string) - The full resource name of the long-running operation (e.g., `operations/generate-video-12345`).
- **metadata** (object) - Contains operation-specific metadata.
- **done** (boolean) - Indicates whether the operation has completed. Initially `false`.

#### Response Example
```json
{
  "name": "operations/generate-video-1234567890",
  "metadata": {
    "@type": "type.googleapis.com/google.cloud.generativelanguage.v1beta.GenerateVideoMetadata",
    "createTime": "2023-10-27T10:00:00Z"
  },
  "done": false
}
```
```

--------------------------------

### Detect Specific Objects by Points with Gemini Robotics API (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview_hl=th

This Python code loads an image, defines a list of objects to query, and constructs a prompt to detect these objects. It then calls the Gemini API to generate content, requesting point coordinates and labels for each detected object in a specified JSON format. The output points are normalized coordinates within the image.

```python
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

queries = [
    "bread",
    "starfruit",
    "banana",
]

prompt = f"""
    Get all points matching the following objects: {', '.join(queries)}. The
    label returned should be an identifying name for the object detected.
    The answer should follow the json format:

    [{{"point": , "label": }}, ...]. The points are in

    [y, x] format normalized to 0-1000.
    """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

--------------------------------

### POST /v1beta/models/gemini-2.5-flash:generateContent

Source: https://ai.google.dev/gemini-api/docs/prompting_with_media_lang=python

Generates content based on a combination of text and an uploaded file. This endpoint allows multimodal input for the Gemini model.

```APIDOC
## POST /v1beta/models/gemini-2.5-flash:generateContent

### Description
Generates content based on a combination of text and an uploaded file. This endpoint allows multimodal input for the Gemini model.

### Method
POST

### Endpoint
/v1beta/models/gemini-2.5-flash:generateContent

### Parameters
#### Path Parameters
- **model_id** (string) - Required - The identifier of the model to use (e.g., `gemini-2.5-flash`).

#### Query Parameters
None

#### Request Body
- **contents** (array of object) - Required - A list of content parts for the generation request.
  - **parts** (array of object) - Required - A list of parts that make up a content, which can be text or file data.
    - **text** (string) - Optional - Text content.
    - **file_data** (object) - Optional - Data representing an uploaded file.
      - **mime_type** (string) - Required - The MIME type of the file (e.g., `audio/mpeg`).
      - **file_uri** (string) - Required - The URI of the uploaded file, obtained after uploading to the Files API.

### Request Example
```json
{
  "contents": [{
    "parts":[
      {"text": "Describe this audio clip"},
      {"file_data":{"mime_type": "${MIME_TYPE}", "file_uri": "$file_uri"}}]
    }]
}
```

### Response
#### Success Response (200)
- **candidates** (array of object) - A list of generated content candidates.
  - **content** (object) - The generated content.
    - **parts** (array of object) - A list of parts that make up the content.
      - **text** (string) - The generated text content.

#### Response Example
```json
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "The audio clip contains sounds of a bird chirping and a gentle stream flowing."
          }
        ]
      }
    }
  ]
}
```
```

--------------------------------

### Detect Objects and Generate Bounding Boxes with Gemini Robotics API (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview_hl=th

This Python example initializes the Gemini API client and loads an image. It constructs a detailed prompt to request 2D bounding boxes and labels for identified objects on a table. The prompt specifies output limitations, unique naming conventions for multiple instances, and a JSON format for the bounding box coordinates, which are normalized integer values.

```python
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

prompt = """
      Return bounding boxes as a JSON array with labels. Never return masks
      or code fencing. Limit to 25 objects. Include as many objects as you
      can identify on the table.
      If an object is present multiple times, name them according to their
      unique characteristic (colors, size, position, unique characteristics, etc..).
      The format should be as follows: [{"box_2d": [ymin, xmin, ymax, xmax],
      "label": <label for the object>}] normalized to 0-1000. The values in
      box_2d must only be integers
      """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

--------------------------------

### Configure Gemini API for Android Control with Custom Functions (Python)

Source: https://ai.google.dev/gemini-api/docs/computer-use_hl=ko

 Python  Gemini API  Android      . `open_app`, `long_press_at`, `go_home`     ,  Android     . ,       ,     `GenerateContentConfig`    .

```python
from typing import Optional, Dict, Any

from google import genai
from google.genai import types
from google.genai.types import Content, Part

client = genai.Client()

SYSTEM_PROMPT = """You are operating an Android phone. Today's date is October 15, 2023, so ignore any other date provided.
* To provide an answer to the user, *do not use any tools* and output your answer on a separate line. IMPORTANT: Do not add any formatting or additional punctuation/text, just output the answer by itself after two empty lines.
* Make sure you scroll down to see everything before deciding something isn't available.
* You can open an app from anywhere. The icon doesn't have to currently be on screen.
* Unless explicitly told otherwise, make sure to save any changes you make.
* If text is cut off or incomplete, scroll or click into the element to get the full text before providing an answer.
* IMPORTANT: Complete the given task EXACTLY as stated. DO NOT make any assumptions that completing a similar task is correct.  If you can't find what you're looking for, SCROLL to find it.
* If you want to edit some text, ONLY USE THE `type` tool. Do not use the onscreen keyboard.
* Quick settings shouldn't be used to change settings. Use the Settings app instead.
* The given task may already be completed. If so, there is no need to do anything.
"""

def open_app(app_name: str, intent: Optional[str] = None) -> Dict[str, Any]:
    """Opens an app by name.

    Args:
        app_name: Name of the app to open (any string).
        intent: Optional deep-link or action to pass when launching, if the app supports it.

    Returns:
        JSON payload acknowledging the request (app name and optional intent).
    """
    return {"status": "requested_open", "app_name": app_name, "intent": intent}

def long_press_at(x: int, y: int) -> Dict[str, int]:
    """Long-press at a specific screen coordinate.

    Args:
        x: X coordinate (absolute), scaled to the device screen width (pixels).
        y: Y coordinate (absolute), scaled to the device screen height (pixels).

    Returns:
        Object with the coordinates pressed and the duration used.
    """
    return {"x": x, "y": y}

def go_home() -> Dict[str, str]:
    """Navigates to the device home screen.

    Returns:
        A small acknowledgment payload.
    """
    return {"status": "home_requested"}

#  Build function declarations
CUSTOM_FUNCTION_DECLARATIONS = [
    types.FunctionDeclaration.from_callable(client=client, callable=open_app),
    types.FunctionDeclaration.from_callable(client=client, callable=long_press_at),
    types.FunctionDeclaration.from_callable(client=client, callable=go_home),
]

#Exclude browser functions
EXCLUDED_PREDEFINED_FUNCTIONS = [
    "open_web_browser",
    "search",
    "navigate",
    "hover_at",
    "scroll_document",
    "go_forward",
    "key_combination",
    "drag_and_drop",
]

#Utility function to construct a GenerateContentConfig
def make_generate_content_config() -> genai.types.GenerateContentConfig:
    """Return a fixed GenerateContentConfig with Computer Use + custom functions."""
    return genai.types.GenerateContentConfig(
        system_instruction=SYSTEM_PROMPT,
        tools=[
            types.Tool(
                computer_use=types.ComputerUse(
                    environment=types.Environment.ENVIRONMENT_BROWSER,
                    excluded_predefined_functions=EXCLUDED_PREDEFINED_FUNCTIONS,
                )
            ),
            types.Tool(function_declarations=CUSTOM_FUNCTION_DECLARATIONS),
        ],
    )

# Create the content with user message
contents: list[Content] = [
    Content(
        role="user",
        parts=[
            # text instruction
            Part(text="Open Chrome, then long-press at 200,400."),
            # optional screenshot attachment
            Part.from_bytes(
                data=screenshot_image_bytes,
                mime_type="image/png",
            ),
        ],
    )
]

# Build your fixed config (from helper)
config = make_generate_content_config()

# Generate content with the configured settings
response = client.models.generate_content(
        model='gemini-2.5-computer-use-preview-10-2025',
        contents=contents,
        config=config,
    )

print(response)
```

--------------------------------

### Grounding Metadata Object Structure

Source: https://ai.google.dev/gemini-api/docs/google-search

Describes the `groundingMetadata` object, a key component of Gemini API responses that provides verifiable claims and citation information.

```APIDOC
## Grounding Metadata Object

### Description
The `groundingMetadata` object is included in successfully grounded responses from the Gemini API. It contains structured data essential for verifying claims and constructing citation experiences in applications.

### Object Structure
- **webSearchQueries** (array of strings) - Array of the search queries used by the model for grounding. Useful for debugging and understanding the model's reasoning process.
- **searchEntryPoint** (object) - Contains information for rendering search suggestions.
    - **renderedContent** (string) - HTML and CSS to render the required Search Suggestions. Full usage requirements are detailed in the Terms of Service.
- **groundingChunks** (array of objects) - An array of objects containing the web sources used for grounding.
    - **web** (object)
        - **uri** (string) - The URI of the web source.
        - **title** (string) - The title of the web source (e.g., "aljazeera.com").
- **groundingSupports** (array of objects) - An array of objects connecting segments of the model's response text to their corresponding sources in `groundingChunks`. This is crucial for building inline citations.
    - **segment** (object)
        - **startIndex** (integer) - The starting index (inclusive) of the text segment in the model's response.
        - **endIndex** (integer) - The ending index (exclusive) of the text segment in the model's response.
        - **text** (string) - The actual text segment extracted from the model's response.
    - **groundingChunkIndices** (array of integers) - An array of indices referencing entries in the `groundingChunks` array, indicating which sources support this specific text segment.
```

--------------------------------

### GET /operations/{operation_name} - Poll Video Generation Status

Source: https://ai.google.dev/gemini-api/docs/video_hl=hi

Retrieves the current status of a long-running video generation operation. This endpoint should be repeatedly called until the operation is marked as 'done'.

```APIDOC
## GET /operations/{operation_name}

### Description
This endpoint allows you to check the progress and final result of a video generation operation initiated via the `predictLongRunning` endpoint. Once the `done` field is `true`, the `response` field will contain details of the generated video, including its download URI.

### Method
GET

### Endpoint
`/v1beta/operations/{operation_name}`

### Parameters
#### Path Parameters
- **operation_name** (string) - Required - The unique name of the long-running operation to query (e.g., `operations/generate-video-1234567890`).

### Request Example
```bash
curl -s -H "x-goog-api-key: $GEMINI_API_KEY" \
  "https://generativelanguage.googleapis.com/v1beta/operations/generate-video-1234567890"
```

### Response
#### Pending Response (200)
- **name** (string) - The unique identifier for the long-running operation.
- **metadata** (object) - Additional information about the operation.
- **done** (boolean) - Indicates whether the operation has completed. `false` if still in progress.

#### Pending Response Example
```json
{
  "name": "operations/generate-video-1234567890",
  "metadata": {},
  "done": false
}
```

#### Success Response (200)
- **name** (string) - The unique identifier for the long-running operation.
- **metadata** (object) - Additional information about the operation.
- **done** (boolean) - Indicates whether the operation has completed. `true` if completed successfully.
- **response** (object) - The result of the operation, available when `done` is `true`.
  - **generateVideoResponse** (object) - Contains the details of the generated video.
    - **generatedSamples** (array of objects) - A list of generated video samples.
      - **video** (object) - Details of a single generated video.
        - **uri** (string) - The URI from which the generated video can be downloaded.

#### Success Response Example
```json
{
  "name": "operations/generate-video-1234567890",
  "metadata": {},
  "done": true,
  "response": {
    "@type": "type.googleapis.com/google.ai.generativelanguage.v1beta.GenerateVideoResponse",
    "generateVideoResponse": {
      "generatedSamples": [
        {
          "video": {
            "uri": "https://storage.googleapis.com/some-bucket/generated-video.mp4?token=..."
          }
        }
      ]
    }
  }
}
```
```

--------------------------------

### POST /models/{model_id}:predictLongRunning

Source: https://ai.google.dev/gemini-api/docs/video_hl=de

Initiates a long-running operation to generate a video based on a text prompt and configuration. The API returns an operation object which must be polled for completion.

```APIDOC
## POST /models/{model_id}:predictLongRunning

### Description
Initiates a long-running operation to generate a video based on a text prompt and configuration. The API returns an operation object which must be polled for completion.

### Method
POST

### Endpoint
/models/{model_id}:predictLongRunning

### Parameters
#### Path Parameters
- **model_id** (string) - Required - The ID of the video generation model to use (e.g., "veo-3.0-generate-001").

#### Query Parameters
- None

#### Request Body
- **instances** (array of objects) - Required - A list of video generation requests.
  - **prompt** (string) - Required - The text prompt describing the desired video content.
- **parameters** (object) - Optional - Configuration parameters for video generation.
  - **aspectRatio** (string) - Optional - The desired aspect ratio of the video (e.g., "16:9", "1:1").
  - **negativePrompt** (string) - Optional - A prompt to specify elements or styles to avoid in the generated video (e.g., "cartoon, drawing, low quality").

### Request Example
```json
{
  "instances": [{
      "prompt": "A cinematic shot of a majestic lion in the savannah."
    }
  ],
  "parameters": {
    "aspectRatio": "16:9",
    "negativePrompt": "cartoon, drawing, low quality"
  }
}
```

### Response
#### Success Response (200)
- **name** (string) - The resource name of the long-running operation (e.g., "operations/generate-video-12345").
- **metadata** (object) - Operation-specific metadata.
- **done** (boolean) - Indicates whether the operation has completed. Will be `false` initially.
- **error** (object) - Contains error details if the operation failed.

#### Response Example
```json
{
  "name": "operations/generate-video-12345",
  "metadata": {},
  "done": false
}
```
```

--------------------------------

### Handle Voice Activity Detection (VAD) Interruptions

Source: https://ai.google.dev/gemini-api/docs/live-guide

Voice Activity Detection (VAD) allows the model to recognize when a user interrupts the conversation. When an interruption is detected, the ongoing generation is canceled, and a `BidiGenerateContentServerContent` message with `interrupted` set to `true` is sent. Clients should handle this by stopping audio playback and clearing queued audio.

```python
async for response in session.receive():
    if response.server_content.interrupted is True:
        # The generation was interrupted

        # If realtime playback is implemented in your application,
        # you should stop playing audio and clear queued playback here.
```

```javascript
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.serverContent && turn.serverContent.interrupted) {
    // The generation was interrupted

    // If realtime playback is implemented in your application,
    // you should stop playing audio and clear queued playback here.
  }
}
```

--------------------------------

### GET /v1beta/{operation_name}

Source: https://ai.google.dev/gemini-api/docs/video_hl=fr

Retrieves the current status of a long-running video generation operation. This endpoint is polled until the 'done' field is true, at which point the generated video's details are available.

```APIDOC
## GET /v1beta/{operation_name}

### Description
Retrieves the current status of a long-running video generation operation. This endpoint is polled until the 'done' field is true, at which point the generated video's details are available.

### Method
GET

### Endpoint
/v1beta/{operation_name}

### Parameters
#### Path Parameters
- **operation_name** (string) - Required - The resource name of the long-running operation to retrieve (e.g., `operations/generate-video-12345`).

### Request Example
```bash
curl -s -H "x-goog-api-key: $GEMINI_API_KEY" \
"https://generativelanguage.googleapis.com/v1beta/operations/generate-video-12345"
```

### Response
#### Success Response (200)
Returns the updated long-running operation object. When `done` is true, the `response` field contains the result.
- **name** (string) - The resource name of the operation.
- **done** (boolean) - Indicates if the operation has finished. True if complete, false otherwise.
- **response** (object) - Present if `done` is true. Contains the result of the operation.
  - **@type** (string) - The type of the response payload.
  - **generateVideoResponse** (object) - The specific response for video generation.
    - **generatedSamples** (array) - An array of generated video samples.
      - **video** (object) - Details of the generated video.
        - **uri** (string) - The URI from which the generated video can be downloaded.

#### Response Example
```json
{
  "name": "operations/generate-video-12345",
  "metadata": {
    "@type": "type.googleapis.com/google.ai.generativelanguage.v1beta.GenerateVideoMetadata",
    "createTime": "2023-10-27T10:00:00Z",
    "updateTime": "2023-10-27T10:05:30Z"
  },
  "done": true,
  "response": {
    "@type": "type.googleapis.com/google.ai.generativelanguage.v1beta.GenerateVideoResponse",
    "generatedSamples": [
      {
        "video": {
          "uri": "https://storage.googleapis.com/gemini-video-output/video_12345.mp4"
        }
      }
    ]
  }
}
```
```

--------------------------------

### Define Gemini API Schema Object Structure (Pseudo-JSON)

Source: https://ai.google.dev/gemini-api/docs/json-mode

This pseudo-JSON snippet outlines the complete structure of the `Schema` object used for configuring `responseSchema` in the Gemini API. It includes standard OpenAPI 3.0 fields like 'type', 'format', 'description', 'properties', and 'required', along with the Gemini-specific 'propertyOrdering' field.

```json
{
  "type": enum (Type),
  "format": string,
  "description": string,
  "nullable": boolean,
  "enum": [
    string
  ],
  "maxItems": integer,
  "minItems": integer,
  "properties": {
    string: {
      object (Schema)
    },
    ...
  },
  "required": [
    string
  ],
  "propertyOrdering": [
    string
  ],
  "items": {
    object (Schema)
  }
}
```

--------------------------------

### Detect objects and get bounding boxes using Gemini API in Python

Source: https://ai.google.dev/gemini-api/docs/image-understanding

This Python example demonstrates how to use the `gemini-2.5-flash` model to detect prominent items in an image and extract their 2D bounding box coordinates. It leverages the `google-generativeai` library and `Pillow` for image handling, then descales the normalized coordinates (0-1000) back to the original image dimensions. The response is configured to be `application/json` for easy parsing of bounding box data.

```python
from google import genai
from google.genai import types
from PIL import Image
import json

client = genai.Client()
prompt = "Detect the all of the prominent items in the image. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000."

image = Image.open("/path/to/image.png")

config = types.GenerateContentConfig(
  response_mime_type="application/json"
  )

response = client.models.generate_content(model="gemini-2.5-flash",
                                          contents=[image, prompt],
                                          config=config
                                          )

width, height = image.size
bounding_boxes = json.loads(response.text)

converted_bounding_boxes = []
for bounding_box in bounding_boxes:
    abs_y1 = int(bounding_box["box_2d"][0]/1000 * height)
    abs_x1 = int(bounding_box["box_2d"][1]/1000 * width)
    abs_y2 = int(bounding_box["box_2d"][2]/1000 * height)
    abs_x2 = int(bounding_box["box_2d"][3]/1000 * width)
    converted_bounding_boxes.append([abs_x1, abs_y1, abs_x2, abs_y2])

print("Image size: ", width, height)
print("Bounding boxes:", converted_bounding_boxes)

```

--------------------------------

### GET /operations/{operationName}

Source: https://ai.google.dev/gemini-api/docs/video_hl=tr

Retrieves the current status and results of an asynchronous video generation operation.

```APIDOC
## GET /operations/{operationName}

### Description
Retrieves the current status and results of an asynchronous video generation operation. This endpoint is used for polling to check if a video generation task has completed.

### Method
GET

### Endpoint
/operations/{operationName}

### Parameters
#### Path Parameters
- **operationName** (string) - Required - The full resource name of the operation to retrieve (e.g., "operations/generateVideos-12345").

### Request Example
```bash
curl -X GET "https://[API_ENDPOINT]/v1/operations/generateVideos-12345"
```

### Response
#### Success Response (200 OK)
Returns an operation object, identical in structure to the response from the `generateVideos` call.
- **name** (string) - The unique name of the operation.
- **metadata** (object) - Additional operation-specific metadata.
- **done** (boolean) - Indicates if the operation has completed.
- **response** (object) - Populated if `done` is `true` and the operation succeeded. Contains the generated video details.
- **error** (object) - Populated if `done` is `true` and the operation failed. Contains error details.

#### Response Example (Completed Operation)
```json
{
  "name": "operations/generateVideos-12345",
  "metadata": {},
  "done": true,
  "response": {
    "generatedVideos": [
      {
        "video": {
          "name": "files/video-abcde",
          "mimeType": "video/mp4"
        },
        "assetId": "asset-123",
        "createTime": "2023-10-27T10:00:00Z"
      }
    ]
  }
}
```
```

--------------------------------

### Python Prompt for Object Location

Source: https://ai.google.dev/gemini-api/docs/robotics-overview

This Python string variable defines a prompt for the model to locate specific objects (blue block, orange bowl) and return their normalized coordinates and labels in a specified JSON format. It guides the model on the expected output structure and coordinate system, with points in [y, x] format normalized to 0-1000.

```python
prompt = """
            Locate and point to the blue block and the orange bowl. The label
            returned should be an identifying name for the object detected.
            The answer should follow the json format: [{"point": <point>, "label": <label1>}, ...].
            The points are in [y, x] format normalized to 0-1000.
          """
```

--------------------------------

### POST /models/{model_id}:predictLongRunning - Generate Video

Source: https://ai.google.dev/gemini-api/docs/video_hl=hi

Initiates a long-running operation to generate a video based on a textual prompt and optional configuration parameters. This endpoint returns an operation object which must be polled for completion.

```APIDOC
## POST /models/{model_id}:predictLongRunning

### Description
This endpoint starts an asynchronous video generation process. It takes a prompt and optional configuration to create a video. The immediate response is a long-running operation object, not the video itself.

### Method
POST

### Endpoint
`/v1beta/models/{model_id}:predictLongRunning`

### Parameters
#### Path Parameters
- **model_id** (string) - Required - The identifier of the video generation model to use (e.g., `veo-3.0-generate-001`).

#### Request Body
- **instances** (array of objects) - Required - A list of video generation requests.
  - **prompt** (string) - Required - The text description used to generate the video.
- **parameters** (object) - Optional - Configuration options for video generation.
  - **aspectRatio** (string) - Optional - The desired aspect ratio for the generated video (e.g., "16:9", "4:3").
  - **negativePrompt** (string) - Optional - A prompt to guide the generation away from certain undesirable characteristics (e.g., "cartoon, drawing, low quality").

### Request Example
```json
{
  "instances": [
    {
      "prompt": "A cinematic shot of a majestic lion in the savannah."
    }
  ],
  "parameters": {
    "aspectRatio": "16:9",
    "negativePrompt": "cartoon, drawing, low quality"
  }
}
```

### Response
#### Success Response (200)
- **name** (string) - The unique identifier for the long-running operation.
- **metadata** (object) - Additional information about the operation.
- **done** (boolean) - Indicates whether the operation has completed. Initially `false`.

#### Response Example
```json
{
  "name": "operations/generate-video-1234567890",
  "metadata": {},
  "done": false
}
```
```

--------------------------------

### Compare Images with Gemini API in Go using File Upload and Inline Data

Source: https://ai.google.dev/gemini-api/docs/image-understanding_hl=sq

This Go program sends two images to the Gemini API to find differences. It uploads one image using `Files.UploadFromPath` and reads the second image into a byte slice for inline inclusion. The request combines text, inline image data, and a URI reference to the uploaded file.

```go
// Upload the first image
image1Path := "path/to/image1.jpg"
uploadedFile, _ := client.Files.UploadFromPath(ctx, image1Path, nil)

// Prepare the second image as inline data
image2Path := "path/to/image2.jpeg"
imgBytes, _ := os.ReadFile(image2Path)

parts := []*genai.Part{
  genai.NewPartFromText("What is different between these two images?"),
  genai.NewPartFromBytes(imgBytes, "image/jpeg"),
  genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

--------------------------------

### Generate, Poll, and Download Video (Shell with cURL and JQ)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=he

This shell script uses `cURL` to interact with the Gemini API for video generation. It captures the asynchronous operation name, then polls its status in a loop using `jq` to parse JSON responses. Once complete, it extracts the video URI and downloads the MP4 file.

```shell
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{ \
    "instances": [{ \
        "prompt": "A cinematic shot of a majestic lion in the savannah." \
      } \
    ], \
    "parameters": { \
      "aspectRatio": "16:9", \
      "negativePrompt": "cartoon, drawing, low quality" \
    } \
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o parameters_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

--------------------------------

### Detect Objects with Bounding Boxes using Gemini Robotics-ER 1.5 (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview_hl=ja

This Python code loads a local image and constructs a detailed prompt to instruct the Gemini Robotics-ER 1.5 model to return object bounding boxes. The prompt specifies output as a JSON array with labels, normalized integer coordinates (0-1000), and limits the response to 25 objects, explicitly avoiding masks or code fencing. This snippet assumes the 'client' and 'MODEL_ID' are already initialized.

```Python
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

prompt = """
      Return bounding boxes as a JSON array with labels. Never return masks
      or code fencing. Limit to 25 objects. Include as many objects as you
      can identify on the table.
      If an object is present multiple times, name them according to their
      unique characteristic (colors, size, position, unique characteristics, etc..).
      The format should be as follows: [{"box_2d": [ymin, xmin, ymax, xmax],
      "label": <label for the object>}] normalized to 0-1000. The values in
      box_2d must only be integers
      """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

--------------------------------

### Python Imports for Multi-Speaker Text-to-Speech

Source: https://ai.google.dev/gemini-api/docs/speech-generation_hl=hi

Provides the necessary Python import statements for setting up a multi-speaker text-to-speech configuration with the Gemini API. These imports are foundational for implementing functionality that leverages `MultiSpeakerVoiceConfig` and `SpeakerVoiceConfig`.

```python
from google import genai
from google.genai import types
import wave
```

--------------------------------

### POST /models/{model_id}:predictLongRunning

Source: https://ai.google.dev/gemini-api/docs/video_hl=fr

Initiates a long-running operation to generate a video based on a text prompt and specified configuration. The API returns an operation object which must be polled for completion.

```APIDOC
## POST /models/veo-3.0-generate-001:predictLongRunning

### Description
Initiates a long-running operation to generate a video based on a text prompt and specified configuration. The API returns an operation object which must be polled for completion.

### Method
POST

### Endpoint
/models/veo-3.0-generate-001:predictLongRunning

### Parameters
#### Path Parameters
- **model_id** (string) - Required - The ID of the video generation model to use, e.g., `veo-3.0-generate-001`.

#### Request Body
- **instances** (array) - Required - An array containing video generation requests.
  - **prompt** (string) - Required - The text prompt describing the desired video content.
- **parameters** (object) - Optional - Configuration parameters for video generation.
  - **aspectRatio** (string) - Optional - The desired aspect ratio for the generated video (e.g., "16:9", "9:16", "1:1").
  - **negativePrompt** (string) - Optional - A prompt to specify content to avoid in the generated video (e.g., "cartoon, drawing, low quality").

### Request Example
```json
{
  "instances": [
    {
      "prompt": "A cinematic shot of a majestic lion in the savannah."
    }
  ],
  "parameters": {
    "aspectRatio": "16:9",
    "negativePrompt": "cartoon, drawing, low quality"
  }
}
```

### Response
#### Success Response (200)
A long-running operation object is returned immediately, indicating the start of the video generation process.
- **name** (string) - The resource name of the long-running operation (e.g., `operations/generate-video-12345`).
- **metadata** (object) - Optional - Additional metadata about the operation.

#### Response Example
```json
{
  "name": "operations/generate-video-12345",
  "metadata": {
    "@type": "type.googleapis.com/google.ai.generativelanguage.v1beta.GenerateVideoMetadata",
    "createTime": "2023-10-27T10:00:00Z"
  }
}
```
```

--------------------------------

### Define Agents for a Multi-Agent System (Python)

Source: https://ai.google.dev/gemini-api/docs/llama-index

This code defines three `FunctionAgent` instances: `ResearchAgent`, `WriteAgent`, and `ReviewAgent`. Each agent is configured with a name, description, system prompt, a Language Model (LLM), and a list of tools it can use. The `can_handoff_to` property specifies which other agents it can transfer control to, enabling structured collaboration.

```python
from llama_index.core.agent.workflow import (
    AgentInput,
    AgentOutput,
    ToolCall,
    ToolCallResult,
    AgentStream,
)
from llama_index.core.agent.workflow import FunctionAgent, ReActAgent

research_agent = FunctionAgent(
    name="ResearchAgent",
    description="Useful for searching the web for information on a given topic and recording notes on the topic.",
    system_prompt=(
        "You are the ResearchAgent that can search the web for information on a given topic and record notes on the topic. "
        "Once notes are recorded and you are satisfied, you should hand off control to the WriteAgent to write a report on the topic."
    ),
    llm=llm,
    tools=[search_web, record_notes],
    can_handoff_to=["WriteAgent"],
)

write_agent = FunctionAgent(
    name="WriteAgent",
    description="Useful for writing a report on a given topic.",
    system_prompt=(
        "You are the WriteAgent that can write a report on a given topic. "
        "Your report should be in a markdown format. The content should be grounded in the research notes. "
        "Once the report is written, you should get feedback at least once from the ReviewAgent."
    ),
    llm=llm,
    tools=[write_report],
    can_handoff_to=["ReviewAgent", "ResearchAgent"],
)

review_agent = FunctionAgent(
    name="ReviewAgent",
    description="Useful for reviewing a report and providing feedback.",
    system_prompt=(
        "You are the ReviewAgent that can review a report and provide feedback. "
        "Your feedback should either approve the current report or request changes for the WriteAgent to implement."
    ),
    llm=llm,
    tools=[review_report],
    can_handoff_to=["ResearchAgent","WriteAgent"],
)
```

--------------------------------

### Generate Content from Image with Older Gemini SDK (Python, JS, Go)

Source: https://ai.google.dev/gemini-api/docs/migrate_hl=fr

This set of examples demonstrates how to use the earlier versions of the Gemini API SDKs to generate text based on an input image. Each language shows how to load a local image, prepare it for the API (e.g., as a base64 string or byte array), and send it alongside a text prompt to the `gemini-1.5-flash` model. Dependencies include `google.generativeai` for Python, `@google/generative-ai` for JavaScript, and `genai` for Go.

```python
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content([
    'Tell me a story based on this image',
    Image.open(image_path)
])
print(response.text)
```

```javascript
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

function fileToGenerativePart(path, mimeType) {
  return {
    inlineData: {
      data: Buffer.from(fs.readFileSync(path)).toString("base64"),
      mimeType,
    },
  };
}

const prompt = "Tell me a story based on this image";

const imagePart = fileToGenerativePart(
  `path/to/organ.jpg`,
  "image/jpeg",
);

const result = await model.generateContent([prompt, imagePart]);
console.log(result.response.text());
```

```go
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")

imgData, err := os.ReadFile("path/to/organ.jpg")
if err != nil {
    log.Fatal(err)
}

resp, err := model.GenerateContent(ctx,
    genai.Text("Tell me about this instrument"),
    genai.ImageData("jpeg", imgData))
if err != nil {
    log.Fatal(err)
}

printResponse(resp) // utility for printing response
```

--------------------------------

### Python: Generate Content with Existing Image References

Source: https://ai.google.dev/gemini-api/docs/vision_hl=vi

This Python snippet focuses on making the `generate_content` API call. It assumes an `uploaded_file` object (from a previous upload) and `img2_bytes` (raw image data) are already available. It constructs the `contents` list with a text prompt, the uploaded file reference, and an inline image part, then prints the model's response.

```python
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[
        "What is different between these two images?",
        uploaded_file,  # Use the uploaded file reference
        types.Part.from_bytes(
            data=img2_bytes,
            mime_type='image/png'
        )
    ]
)

print(response.text)
```

--------------------------------

### Send Multimodal Gemini API Request with Text and Image (curl)

Source: https://ai.google.dev/gemini-api/docs/api-overview

This `curl` command illustrates how to send a multimodal prompt to the Gemini API, combining text and an image. The `parts` array within the `Content` object contains two `Part` objects: one for the `inline_data` (base64-encoded image) and another for the accompanying text.

```curl
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
    "contents": [{
    "parts":[
        {
            "inline_data": {
            "mime_type":"image/jpeg",
            "data": "/9j/4AAQSkZJRgABAQ... (base64-encoded image)"
            }
        },
        {"text": "What is in this picture?"},
      ]
    }]
  }'
```

--------------------------------

### Generate Multi-Step Instructions with Object Pointers using Gemini API (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview_hl=fr

This Python code snippet demonstrates how to use the Gemini API to generate detailed, multi-step instructions for a task, such as packing a lunch. It sends an image and a prompt asking for packing instructions, expecting the model to reference objects by pointing to them with normalized coordinates within the generated text and providing a summary list of objects and their locations.

```python
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-of-lunch.jpg', 'rb') as f:
    image_bytes = f.read()

prompt = """
          Explain how to pack the lunch box and lunch bag. Point to each
          object that you refer to. Each point should be in the format:
          [{"point": [y, x], "label": }], where the coordinates are
          normalized between 0-1000.
        """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

--------------------------------

### Python: Poll Video Generation Operation & Download

Source: https://ai.google.dev/gemini-api/docs/video_hl=hi

This Python snippet demonstrates how to poll a long-running video generation operation initiated via the Google Gemini API client. It repeatedly checks the `operation.done` status, pausing for 10 seconds between checks, until the video is ready. Once complete, it downloads the first generated video and saves it to a local file named 'parameters_example.mp4'.

```python
while not operation.done:
    print("Waiting for video generation to complete...")
    time.sleep(10)
    operation = client.operations.get(operation)

# Download the generated video.
generated_video = operation.response.generated_videos[0]
client.files.download(file=generated_video.video)
generated_video.video.save("parameters_example.mp4")
print("Generated video saved to parameters_example.mp4")
```

--------------------------------

### GET /operations/{operation_name} - Poll Video Generation Operation Status

Source: https://ai.google.dev/gemini-api/docs/video_hl=tr

Retrieves the current status of a long-running video generation operation. When the operation is complete (`done: true`), the response will include details about the generated video, including its download URI.

```APIDOC
## GET /operations/{operation_name}

### Description
Retrieves the current status of a long-running video generation operation. When the operation is complete (`done: true`), the response will include details about the generated video, including its download URI.

### Method
GET

### Endpoint
/operations/{operation_name}

### Parameters
#### Path Parameters
- **operation_name** (string) - Required - The unique identifier of the long-running operation (e.g., `operations/generate-video-12345`).

### Response
#### Success Response (200 - Operation In Progress)
- **name** (string) - The unique identifier for the long-running operation.
- **done** (boolean) - `false` indicates the operation is still in progress.

#### Response Example (Operation In Progress)
```json
{
  "name": "operations/generate-video-12345",
  "done": false
}
```

#### Success Response (200 - Operation Complete)
- **name** (string) - The unique identifier for the long-running operation.
- **done** (boolean) - `true` indicates the operation is complete.
- **response.generateVideoResponse.generatedSamples[0].video.uri** (string) - The URI to download the generated video.
- **response.generateVideoResponse.generatedSamples[0].video.mimeType** (string) - The MIME type of the generated video (e.g., `video/mp4`).

#### Response Example (Operation Complete)
```json
{
  "name": "operations/generate-video-12345",
  "done": true,
  "response": {
    "generateVideoResponse": {
      "generatedSamples": [
        {
          "video": {
            "uri": "https://storage.googleapis.com/path/to/generated_video_123.mp4",
            "mimeType": "video/mp4",
            "fileSizeBytes": "5000000"
          }
        }
      ]
    }
  }
}
```
```

--------------------------------

### Example JSON Output for Graded Recipe List

Source: https://ai.google.dev/gemini-api/docs/json-mode

This JSON snippet illustrates a possible response from the Gemini API when requesting a list of recipes with custom grades, as configured by the preceding Python example. It shows how the model generates structured data conforming to the defined `Recipe` Pydantic model and `Grade` enumeration.

```json
[
  {
    "recipe_name": "Chocolate Chip Cookies",
    "rating": "a+"
  },
  {
    "recipe_name": "Peanut Butter Cookies",
    "rating": "a"
  },
  {
    "recipe_name": "Oatmeal Raisin Cookies",
    "rating": "b"
  },
  ...
]
```

--------------------------------

### Download a Generated Video File (Python)

Source: https://ai.google.dev/gemini-api/docs/video_hl=pl

This Python snippet downloads a previously generated video from an operation response. It assumes the video generation operation has already completed and the `operation.response` object is available. The video is then saved to a specified MP4 file.

```python
# Download the generated video.\ngenerated_video = operation.response.generated_videos[0]\nclient.files.download(file=generated_video.video)\ngenerated_video.video.save("parameters_example.mp4")\nprint("Generated video saved to parameters_example.mp4")
```

--------------------------------

### Create Content for Gemini Computer Use Model

Source: https://ai.google.dev/gemini-api/docs/computer-use_hl=zh-cn

This Python code snippet demonstrates how to construct a `Content` object with a user message for the Gemini Computer Use model. It includes an example search query for smart fridges and shows how to generate content using the `generate_content` method with a specific model and configuration.

```python
# Create the content with user message
contents=[
    Content(
        role="user",
        parts=[
            Part(text="Search for highly rated smart fridges with touchscreen, 2 doors, around 25 cu ft, priced below 4000 dollars on Google Shopping. Create a bulleted list of the 3 cheapest options in the format of name, description, price in an easy-to-read layout."),
            # Optional: include a screenshot of the initial state
            #Part.from_bytes(
                #data=screenshot_image_bytes,
                #mime_type='image/png',
            #),
        ],
    )
]

# Generate content with the configured settings
response = client.models.generate_content(
    model='gemini-2.5-computer-use-preview-10-2025',
    contents=contents,
    config=generate_content_config,
)

# Print the response output
print(response)
```

--------------------------------

### POST /v1beta/models/{model}:generateContent (YouTube URL)

Source: https://ai.google.dev/gemini-api/docs/video-understanding_hl=th

This endpoint enables content generation from a publicly accessible YouTube video by specifying its URL. The API will process the video and generate text based on the provided prompt. Authentication is typically handled via an `x-goog-api-key` header.

```APIDOC
## POST /v1beta/models/{model}:generateContent

### Description
This endpoint enables content generation from a publicly accessible YouTube video by specifying its URL. The API will process the video and generate text based on the provided prompt. Authentication is handled via the `x-goog-api-key` header.

### Method
POST

### Endpoint
/v1beta/models/{model}:generateContent

### Parameters
#### Path Parameters
- **model** (string) - Required - The name of the model to use for content generation (e.g., `gemini-2.5-flash`).

#### Request Body
- **contents** (array of objects) - Required - An array containing the parts of the request, including video data and a text prompt.
  - **contents[].parts** (array of objects) - Required - An array of content parts.
    - **parts[].file_data** (object) - Required - Contains the YouTube video URL.
      - **file_data.file_uri** (string) - Required - The URL of the YouTube video (e.g., "https://www.youtube.com/watch?v=9hE5-98ZeCg").
    - **parts[].text** (string) - Required - The text prompt for content generation (e.g., "Please summarize the video in 3 sentences.").

### Request Example
```json
{
  "contents": [
    {
      "parts": [
        {
          "file_data": {
            "file_uri": "https://www.youtube.com/watch?v=9hE5-98ZeCg"
          }
        },
        {
          "text": "Please summarize the video in 3 sentences."
        }
      ]
    }
  ]
}
```

### Response
#### Success Response (200)
- **candidates** (array of objects) - Contains the generated content.
  - **candidates[].content** (object) - The generated content.
    - **content.parts** (array of objects) - Array of parts, typically containing text.
      - **parts[].text** (string) - The generated text response.

#### Response Example
```json
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "The YouTube video details... \nIt features... \nIn conclusion,..."
          }
        ]
      }
    }
  ]
}
```
```

--------------------------------

### Perform Multimodal Image Analysis with Gemini API

Source: https://ai.google.dev/gemini-api/docs/openai_hl=th

These snippets demonstrate how to send an image along with a text prompt to the Gemini API for multimodal analysis. They show how to encode a local image file to base64 and include it in a chat completion request to ask questions about the image's content. The examples cover Python, Node.js, and a direct cURL call.

```python
base64_image = encode_image("Path/to/agi/image.jpeg")

response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?"
        },
        {
          "type": "image_url",
          "image_url": {
            "url":  f"data:image/jpeg;base64,{base64_image}"
          }
        }
      ]
    }
  ]
)

print(response.choices[0])
```

```javascript
import OpenAI from "openai";
import fs from 'fs/promises';

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function encodeImage(imagePath) {
  try {
    const imageBuffer = await fs.readFile(imagePath);
    return imageBuffer.toString('base64');
  } catch (error) {
    console.error("Error encoding image:", error);
    return null;
  }
}

async function main() {
  const imagePath = "Path/to/agi/image.jpeg";
  const base64Image = await encodeImage(imagePath);

  const messages = [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?"
        },
        {
          "type": "image_url",
          "image_url": {
            "url": `data:image/jpeg;base64,${base64Image}`
          }
        }
      ]
    }
  ];

  try {
    const response = await openai.chat.completions.create({
      model: "gemini-2.0-flash",
      messages: messages,
    });

    console.log(response.choices[0]);
  } catch (error) {
    console.error("Error calling Gemini API:", error);
  }
}

main();
```

```bash
bash -c '
  base64_image=$(base64 -i "Path/to/agi/image.jpeg");
  curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer GEMINI_API_KEY" \
    -d "{
      \"model\": \"gemini-2.0-flash\",
      \"messages\": [
        {
          \"role\": \"user\",
          \"content\": [
            { \"type\": \"text\", \"text\": \"What is in this image?\" },
            {
              \"type\": \"image_url\",
              \"image_url\": { \"url\": \"data:image/jpeg;base64,${base64_image}\" }
            }
          ]
        }
      ]
    }"
'
```

--------------------------------

### Define Gemini API function declarations and mock implementations

Source: https://ai.google.dev/gemini-api/docs/function-calling/tutorial_hl=ja

This snippet demonstrates how to define a function declaration, adhering to an OpenAPI-compatible schema, for controlling smart lights. It also provides a mock implementation of the corresponding function (`set_light_values`) that would be executed based on the model's suggestion, handling brightness and color temperature.

```python
set_light_values_declaration = {
    "name": "set_light_values",
    "description": "Sets the brightness and color temperature of a light.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "integer",
                "description": "Light level from 0 to 100. Zero is off and 100 is full brightness",
            },
            "color_temp": {
                "type": "string",
                "enum": ["daylight", "cool", "warm"],
                "description": "Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.",
            },
        },
        "required": ["brightness", "color_temp"],
    },
}

def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:
    """Set the brightness and color temperature of a room light. (mock API).

    Args:
        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness
        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.

    Returns:
        A dictionary containing the set brightness and color temperature.
    """
    return {"brightness": brightness, "colorTemperature": color_temp}
```

```javascript
import { Type } from '@google/genai';

const setLightValuesFunctionDeclaration = {
  name: 'set_light_values',
  description: 'Sets the brightness and color temperature of a light.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',
      },
      color_temp: {
        type: Type.STRING,
        enum: ['daylight', 'cool', 'warm'],
        description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',
      },
    },
    required: ['brightness', 'color_temp'],
  },
};

/**

*   Set the brightness and color temperature of a room light. (mock API)
*   @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness
*   @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.
*   @return {Object} A dictionary containing the set brightness and color temperature.
*/
function setLightValues(brightness, color_temp) {
  return {
    brightness: brightness,
    colorTemperature: color_temp
  };
}
```

--------------------------------

### POST /v1beta/models/{model_id}:generateContent

Source: https://ai.google.dev/gemini-api/docs/json-mode_hl=ru

This endpoint allows users to send prompts to a Gemini model and receive a structured response, either as a simple enumeration or a complex JSON object defined by a schema.

```APIDOC
## POST /v1beta/models/{model_id}:generateContent

### Description
This endpoint allows users to send prompts to a Gemini model and receive a structured response, either as a simple enumeration or a complex JSON object defined by a schema. It supports specifying the expected `responseMimeType` and `responseSchema` to guide the model's output.

### Method
POST

### Endpoint
/v1beta/models/{model_id}:generateContent

### Parameters
#### Path Parameters
- **model_id** (string) - Required - The ID of the Gemini model to use for content generation (e.g., `gemini-2.5-flash`).

#### Request Body
- **contents** (array of objects) - Required - The input content for the model, typically a list of text parts.
  - **contents[].parts** (array of objects) - Required - A list of content parts.
    - **contents[].parts[].text** (string) - Required - The textual prompt for the model.
- **generationConfig** (object) - Optional - Configuration settings for the content generation.
  - **generationConfig.responseMimeType** (string) - Optional - The desired MIME type for the response (e.g., `"text/x.enum"`, `"application/json"`).
  - **generationConfig.responseSchema** (object) - Optional - Defines the schema for the structured response. This can be a simple `type` and `enum` or a more complex OpenAPI 3.0 schema.
    - **generationConfig.responseSchema.type** (string) - Required when `responseSchema` is present - The data type of the expected response (e.g., `"STRING"`, `"ARRAY"`, `"OBJECT"`).
    - **generationConfig.responseSchema.enum** (array of strings) - Optional, for `STRING` type - A list of allowed string values the model should choose from.
    - (Additional fields for complex schemas, e.g., `properties`, `items` as per OpenAPI 3.0 Schema Object specifications.)

### Request Example (Enum Response)
```json
{
  "contents": [
    {
      "parts": [
        {
          "text": "What type of instrument is an oboe?"
        }
      ]
    }
  ],
  "generationConfig": {
    "responseMimeType": "text/x.enum",
    "responseSchema": {
      "type": "STRING",
      "enum": ["Percussion", "String", "Woodwind", "Brass", "Keyboard"]
    }
  }
}
```

### Request Example (JSON Schema Response)
```json
{
  "contents": [
    {
      "parts": [
        {
          "text": "List 10 home-baked cookie recipes and give them grades based on tastiness."
        }
      ]
    }
  ],
  "generationConfig": {
    "responseMimeType": "application/json",
    "responseSchema": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "recipe_name": {
            "type": "string"
          },
          "rating": {
            "type": "string",
            "enum": ["a+", "a", "b", "c", "d", "f"]
          }
        },
        "required": ["recipe_name", "rating"]
      }
    }
  }
}
```

### Response
#### Success Response (200)
- The response structure depends entirely on the `responseMimeType` and `responseSchema` provided in the request.
- For `responseMimeType: "text/x.enum"`, the response is a plain text string representing one of the enum values.
- For `responseMimeType: "application/json"`, the response is a JSON object or array conforming to the specified `responseSchema`.

#### Response Example (Enum Response)
```json
"Woodwind"
```

#### Response Example (JSON Schema Response)
```json
[
  {
    "recipe_name": "Chocolate Chip Cookies",
    "rating": "a+"
  },
  {
    "recipe_name": "Peanut Butter Cookies",
    "rating": "a"
  },
  {
    "recipe_name": "Oatmeal Raisin Cookies",
    "rating": "b"
  }
]
```
```

--------------------------------

### Analyze Image with Gemini API Multimodal Chat

Source: https://ai.google.dev/gemini-api/docs/openai_hl=he

This code snippet demonstrates how to send an image (encoded as base64) along with a text prompt to the Gemini API's multimodal chat endpoint. It uses the OpenAI client library to interact with the Gemini API, providing a text-based question about the image. The response includes the model's analysis of the image content.

```python
base64_image = encode_image("Path/to/agi/image.jpeg")

response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url":  f"data:image/jpeg;base64,{base64_image}"
          },
        },
      ],
    }
  ],
)

print(response.choices[0])
```

```javascript
import OpenAI from "openai";
import fs from 'fs/promises';

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function encodeImage(imagePath) {
  try {
    const imageBuffer = await fs.readFile(imagePath);
    return imageBuffer.toString('base64');
  } catch (error) {
    console.error("Error encoding image:", error);
    return null;
  }
}

async function main() {
  const imagePath = "Path/to/agi/image.jpeg";
  const base64Image = await encodeImage(imagePath);

  const messages = [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url": `data:image/jpeg;base64,${base64Image}`
          },
        },
      ],
    }
  ];

  try {
    const response = await openai.chat.completions.create({
      model: "gemini-2.0-flash",
      messages: messages,
    });

    console.log(response.choices[0]);
  } catch (error) {
    console.error("Error calling Gemini API:", error);
  }
}

main();
```

```bash
bash -c '\n  base64_image=$(base64 -i "Path/to/agi/image.jpeg");\n  curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \\\
    -H "Content-Type: application/json" \\\\
    -H "Authorization: Bearer GEMINI_API_KEY" \\\\
    -d "{\n      \"model\": \"gemini-2.0-flash\",\n      \"messages\": [\n        {\n          \"role\": \"user\",\n          \"content\": [\n            { \"type\": \"text\", \"text\": \"What is in this image?\" },\n            {\n              \"type\": \"image_url\",\n              \"image_url\": { \"url\": \"data:image/jpeg;base64,${base64_image}\" }\n            }\n          ]\n        }\n      ]\n    }"\n'
```

--------------------------------

### Define Prompt for Object Tracking in Video Frames (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview_hl=th

This Python code defines a list of specific objects to track within video frames, including their contextual states. It then constructs a base prompt that instructs the Gemini model to identify the point coordinates and labels for these objects in a given image, specifying a JSON output format with normalized coordinates and handling cases where no objects are found.

```python
# Define the objects to find
queries = [
    "pen (on desk)",
    "pen (in robot hand)",
    "laptop (opened)",
    "laptop (closed)",
]

base_prompt = f"""
  Point to the following objects in the provided image: {', '.join(queries)}.
  The answer should follow the json format:

  [{{"point": , "label": }}, ...].

  The points are in [y, x] format normalized to 0-1000.
  If no objects are found, return an empty JSON list [].
  """
```

--------------------------------

### POST /v1beta/models/{model_id}:generateContent (YouTube URL)

Source: https://ai.google.dev/gemini-api/docs/video-understanding_hl=de

This endpoint enables summarization of publicly accessible YouTube videos by providing their URL in the request body, leveraging a specified Gemini model.

```APIDOC
## POST /v1beta/models/{model_id}:generateContent (YouTube URL)

### Description
This endpoint enables summarization of publicly accessible YouTube videos by providing their URL in the request body, leveraging a specified Gemini model.

### Method
POST

### Endpoint
/v1beta/models/{model_id}:generateContent

### Parameters
#### Path Parameters
- **model_id** (string) - Required - The Gemini model to use (e.g., `gemini-2.5-flash`).

#### Request Body
- **contents** (array of objects) - Required - The content to process for generation.
  - **parts** (array of objects) - Required - Individual content parts.
    - **file_data** (object) - Required (if providing a file URI) - Specifies the file data from a URI.
      - **file_uri** (string) - Required - The URI of the video file (e.g., `https://www.youtube.com/watch?v=9hE5-98ZeCg`).
    - **text** (string) - Required - The text prompt or instruction for the model (e.g., "Please summarize the video in 3 sentences.").

### Request Example
```json
{
  "contents": [{
    "parts":[
        {"text": "Please summarize the video in 3 sentences."},
        {
          "file_data": {
            "file_uri": "https://www.youtube.com/watch?v=9hE5-98ZeCg"
          }
        }
    ]
  }]
}
```

### Response
#### Success Response (200)
- **text** (string) - The generated summary of the video content.

#### Response Example
```json
{
  "text": "This YouTube video introduces the new features of the Gemini 2.5 Flash model, demonstrating its improved speed and efficiency for various AI tasks. It showcases examples of multimodal input processing, including video analysis capabilities, and discusses potential applications for developers."
}
```
```

--------------------------------

### Perform Multimodal Image Analysis with Gemini API

Source: https://ai.google.dev/gemini-api/docs/openai_hl=it

These examples demonstrate how to use the Gemini API to perform multimodal analysis on an image. They show the process of encoding a local image to base64 and sending it along with a text query to the model. The API response will contain the model's interpretation or description of the image.

```python
# Getting the base64 string
base64_image = encode_image("Path/to/agi/image.jpeg")

response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url":  f"data:image/jpeg;base64,{base64_image}"
          },
        },
      ],
    }
  ],
)

print(response.choices[0])
```

```javascript
import OpenAI from "openai";
import fs from 'fs/promises';

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function encodeImage(imagePath) {
  try {
    const imageBuffer = await fs.readFile(imagePath);
    return imageBuffer.toString('base64');
  } catch (error) {
    console.error("Error encoding image:", error);
    return null;
  }
}

async function main() {
  const imagePath = "Path/to/agi/image.jpeg";
  const base64Image = await encodeImage(imagePath);

  const messages = [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url": `data:image/jpeg;base64,${base64Image}`
          },
        },
      ],
    }
  ];

  try {
    const response = await openai.chat.completions.create({
      model: "gemini-2.0-flash",
      messages: messages,
    });

    console.log(response.choices[0]);
  } catch (error) {
    console.error("Error calling Gemini API:", error);
  }
}

main();
```

```bash
bash -c '
  base64_image=$(base64 -i "Path/to/agi/image.jpeg");
  curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer GEMINI_API_KEY" \
    -d "{
      \"model\": \"gemini-2.0-flash\",
      \"messages\": [
        {
          \"role\": \"user\",
          \"content\": [
            { \"type\": \"text\", \"text\": \"What is in this image?\" },
            {
              \"type\": \"image_url\",
              \"image_url\": { \"url\": \"data:image/jpeg;base64,${base64_image}\" }
            }
          ]
        }
      ]
    }"
'
```

--------------------------------

### Generate JSON Object with Recipe Attributes using Gemini

Source: https://ai.google.dev/gemini-api/docs/prompting_with_media_lang=python

This example illustrates how to instruct the Gemini model to extract specific attributes (ingredients, cuisine type, vegetarian status) and present them as a structured JSON object. This standardized output format is ideal for programmatic consumption, allowing applications to easily parse and utilize the model's response.

```json
{
  "ingredients": [
    "rice",
    "shrimp",
    "clams",
    "mussels",
    "peas",
    "tomatoes",
    "onions",
    "garlic",
    "olive oil",
    "paprika",
    "salt",
    "pepper"
  ],
  "type of cuisine": "Spanish",
  "vegetarian": false
}
```

--------------------------------

### POST /upload/v1beta/files

Source: https://ai.google.dev/gemini-api/docs/vision_hl=th

Uploads a file to the Google Generative Language API for later use in content generation. This is a resumable upload endpoint.

```APIDOC
## POST /upload/v1beta/files

### Description
This endpoint facilitates the upload of files to the Google Generative Language API. It supports resumable uploads, allowing large files to be uploaded in chunks. The uploaded file can then be referenced by its URI in subsequent API calls for content generation.

### Method
POST

### Endpoint
/upload/v1beta/files

### Parameters
#### Headers
- **x-goog-api-key** (string) - Required - Your API key for authentication.
- **X-Goog-Upload-Protocol** (string) - Required - Set to `resumable` for resumable uploads.
- **X-Goog-Upload-Command** (string) - Required - Set to `start` for initiating the upload, and `upload, finalize` for uploading data and completing the process.
- **X-Goog-Upload-Header-Content-Length** (integer) - Required (for `start` command) - The total byte size of the file to be uploaded.
- **X-Goog-Upload-Header-Content-Type** (string) - Required (for `start` command) - The MIME type of the file.
- **Content-Type** (string) - Required - `application/json` for the `start` command, or the file's MIME type for the `upload, finalize` command.
- **Content-Length** (integer) - Required (for `upload, finalize` command) - The byte size of the data being uploaded in the current chunk.
- **X-Goog-Upload-Offset** (integer) - Required (for `upload, finalize` command) - The byte offset from the beginning of the file for the current chunk.

#### Request Body (for initiating upload with `X-Goog-Upload-Command: start`)
- **file** (object) - Required - Object containing file metadata.
  - **display_name** (string) - Optional - A human-readable name for the file.

#### Request Body (for uploading data with `X-Goog-Upload-Command: upload, finalize`)
Raw binary content of the file or file chunk.

### Request Example (Initiate Upload)
```json
{
  "file": {
    "display_name": "MyImage.jpg"
  }
}
```

### Request Example (Upload Data)
```
<binary image data>
```

### Response
#### Success Response (200)
- **file** (object) - Details of the uploaded file.
  - **name** (string) - The resource name of the file (e.g., `files/12345`).
  - **display_name** (string) - The display name of the file.
  - **mimeType** (string) - The MIME type of the file.
  - **sizeBytes** (string) - The size of the file in bytes.
  - **createTime** (string) - The creation timestamp of the file.
  - **uri** (string) - The URI that can be used to reference this file in subsequent API calls.

#### Response Example
```json
{
  "file": {
    "name": "files/1234567890",
    "displayName": "IMAGE1",
    "mimeType": "image/jpeg",
    "sizeBytes": "12345",
    "createTime": "2023-10-27T10:00:00Z",
    "uri": "gcs://generative-ai-files-prod/files/1234567890"
  }
}
```
```

--------------------------------

### Poll Asynchronous Gemini API Video Generation Operation (Python)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=fa

This Python snippet shows how to start a video generation job and then continuously poll the returned 'operation' object until the video is ready. It refreshes the operation status every 10 seconds to check the 'done' flag, which is essential for managing asynchronous tasks.

```python
import time
from google import genai
from google.genai import types

client = genai.Client()

# After starting the job, you get an operation object.
operation = client.models.generate_videos(
    model="veo-3.0-generate-001",
    prompt="A cinematic shot of a majestic lion in the savannah.",
)

# Alternatively, you can use operation.name to get the operation.
operation = types.GenerateVideosOperation(name=operation.name)

# This loop checks the job status every 10 seconds.
while not operation.done:
    time.sleep(10)
    # Refresh the operation object to get the latest status.
    operation = client.operations.get(operation)
```

--------------------------------

### Implement Compositional Function Calling for Weather & Thermostat with Gemini API (Python)

Source: https://ai.google.dev/gemini-api/docs/function-calling/tutorial_hl=ja

This example illustrates compositional (sequential) function calling using the `google-genai` Python SDK. It defines `get_weather_forecast` and `set_thermostat_temperature` functions, then configures the Gemini model to use them to conditionally set a thermostat temperature based on a location's weather.

```python
import os
from google import genai
from google.genai import types

# Example Functions
def get_weather_forecast(location: str) -> dict:
    """Gets the current weather temperature for a given location."""
    print(f"Tool Call: get_weather_forecast(location={location})")
    # TODO: Make API call
    print("Tool Response: {'temperature': 25, 'unit': 'celsius'}")
    return {"temperature": 25, "unit": "celsius"}  # Dummy response

def set_thermostat_temperature(temperature: int) -> dict:
    """Sets the thermostat to a desired temperature."""
    print(f"Tool Call: set_thermostat_temperature(temperature={temperature})")
    # TODO: Interact with a thermostat API
    print("Tool Response: {'status': 'success'}")
    return {"status": "success"}

# Configure the client and model
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[get_weather_forecast, set_thermostat_temperature]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="If it's warmer than 20C in London, set the thermostat to 20C, otherwise set it to 18C.",
    config=config,
)
```

--------------------------------

### Perform Image Style Transfer with Gemini API

Source: https://ai.google.dev/gemini-api/docs/image-generation_hl=ja

This comprehensive example demonstrates how to use the Google Gemini API for image style transfer across Python, Node.js, Go, and cURL. It illustrates the common pattern of uploading an image and a text prompt to the `gemini-2.5-flash-image` model to transform the image's artistic style. Each implementation covers reading the source image, constructing the multi-modal request, calling the API, and saving the generated styled image.

```Python
city_image = Image.open('/path/to/your/city.png')
text_input = """Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows."""

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents=[city_image, text_input],
)

image_parts = [
    part.inline_data.data
    for part in response.candidates[0].content.parts
    if part.inline_data
]

if image_parts:
    image = Image.open(BytesIO(image_parts[0]))
    image.save('city_style_transfer.png')
    image.show()
```

```JavaScript
import { GoogleGenAI, Modality } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const imagePath = "/path/to/your/city.png";
  const imageData = fs.readFileSync(imagePath);
  const base64Image = imageData.toString("base64");

  const prompt = [
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image,
      },
    },
    { text: "Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows." },
  ];

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.candidates[0].content.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("city_style_transfer.png", buffer);
      console.log("Image saved as city_style_transfer.png");
    }
  }
}

main();
```

```Go
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/your/city.png"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "image/png",
        Data:     imgData,
      },
    },
    genai.NewPartFromText("Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash-image",
      contents,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "city_style_transfer.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

```Shell
IMG_PATH=/path/to/your/city.png

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {
              \"inline_data\": {
                \"mime_type\":\"image/png\",
                \"data\": \"$IMG_BASE64\"
              }
            },
            {\"text\": \"Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows.\"}
        ]
      }]
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > city_style_transfer.png
```

--------------------------------

### Initialize Google Gemini API Client and Import Audio Libraries (Python)

Source: https://ai.google.dev/gemini-api/docs/live_hl=de

This Python code block imports all necessary modules for file operations, audio processing (librosa, soundfile), and the Google Generative AI client. It then instantiates the `genai.Client`, preparing the environment for interacting with the Gemini API, especially for audio-related tasks.

```python
import asyncio
import io
from pathlib import Path
import wave
from google import genai
from google.genai import types
import soundfile as sf
import librosa

client = genai.Client()
```

--------------------------------

### Download a Generated Video File (Python)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=ko

This Python example shows how to retrieve a generated video from a completed operation response. It demonstrates accessing the video object from the `generated_videos` list and saving the content to a local MP4 file.

```python
generated_video = operation.response.generated_videos[0]
client.files.download(file=generated_video.video)
generated_video.video.save("parameters_example.mp4")
print("Generated video saved to parameters_example.mp4")
```

--------------------------------

### Analyze Image with Gemini API (Python, JS, cURL)

Source: https://ai.google.dev/gemini-api/docs/openai_hl=ru

This snippet demonstrates how to send an image (encoded in base64) to the Gemini API for analysis. It includes functions to encode an image from a local path and construct the appropriate API request for multimodal input, asking 'What is in this image?'. Dependencies include the OpenAI Python library, OpenAI Node.js library, or cURL.

```python
import base64
from openai import OpenAI

client = OpenAI(
  api_key="GEMINI_API_KEY",
  base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# Function to encode the image
def encode_image(image_path):
  with open(image_path, "rb") as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

# Getting the base64 string
base64_image = encode_image("Path/to/agi/image.jpeg")

response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url":  f"data:image/jpeg;base64,{base64_image}"
          },
        },
      ],
    }
  ],
)

print(response.choices[0])
```

```javascript
import OpenAI from "openai";
import fs from 'fs/promises';

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function encodeImage(imagePath) {
  try {
    const imageBuffer = await fs.readFile(imagePath);
    return imageBuffer.toString('base64');
  } catch (error) {
    console.error("Error encoding image:", error);
    return null;
  }
}

async function main() {
  const imagePath = "Path/to/agi/image.jpeg";
  const base64Image = await encodeImage(imagePath);

  const messages = [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url": `data:image/jpeg;base64,${base64Image}`
          },
        },
      ],
    }
  ];

  try {
    const response = await openai.chat.completions.create({
      model: "gemini-2.0-flash",
      messages: messages,
    });

    console.log(response.choices[0]);
  } catch (error) {
    console.error("Error calling Gemini API:", error);
  }
}

main();
```

```bash
bash -c '
  base64_image=$(base64 -i "Path/to/agi/image.jpeg");
  curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer GEMINI_API_KEY" \
    -d "{
      \"model\": \"gemini-2.0-flash\",
      \"messages\": [
        {
          \"role\": \"user\",
          \"content\": [
            { \"type\": \"text\", \"text\": \"What is in this image?\" },
            {
              \"type\": \"image_url\",
              \"image_url\": { \"url\": \"data:image/jpeg;base64,${base64_image}\" }
            }
          ]
        }
      ]
    }"
'
```

--------------------------------

### Handle GoAway Messages for Graceful Disconnection in Gemini API

Source: https://ai.google.dev/gemini-api/docs/live-session

This snippet illustrates how to detect and process `GoAway` messages sent by the server in the Gemini API. Upon receiving a `go_away` message, it extracts `timeLeft` to allow the client to perform cleanup or other actions before the WebSocket connection is terminated, preventing abrupt disconnections.

```python
async for response in session.receive():
    if response.go_away is not None:
        # The connection will soon be terminated
        print(response.go_away.time_left)
```

```javascript
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.goAway) {
    console.debug('Time left: %s\n', turn.goAway.timeLeft);
  }
}
```

--------------------------------

### Combine Google Search, Code Execution, and Custom Functions (Python, JavaScript)

Source: https://ai.google.dev/gemini-api/docs/function-calling/tutorial

Illustrates how to use multiple tools concurrently with the Gemini Live API by defining a `tools` array that includes `google_search`, `code_execution`, and custom `function_declarations`. The example shows a prompt that orchestrates these tools for various tasks. The `turn_on_the_lights_schema` and `turn_off_the_lights_schema` are placeholders not defined in this snippet, and the `run()` function (which handles asynchronous websocket setup) is omitted for brevity. This multi-tool use is a Live API-only feature.

```Python
# Multiple tasks example - combining lights, code execution, and search
prompt = """
  Hey, I need you to do three things for me.

    1.  Turn on the lights.
    2.  Then compute the largest prime palindrome under 100000.
    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

  Thanks!
  """

tools = [
    {'google_search': {}},
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.
]

# Execute the prompt with specified tools in audio modality
await run(prompt, tools=tools, modality="AUDIO")

```

```JavaScript
// Multiple tasks example - combining lights, code execution, and search
const prompt = `
  Hey, I need you to do three things for me.

    1.  Turn on the lights.
    2.  Then compute the largest prime palindrome under 100000.
    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

  Thanks!
`;

const tools = [
  { googleSearch: {} },
  { codeExecution: {} },
  { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.
];

// Execute the prompt with specified tools in audio modality
await run(prompt, {tools: tools, modality: "AUDIO"});

```

--------------------------------

### Detect Specific Objects by Points using Gemini Robotics-ER 1.5 (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview_hl=id

This Python snippet demonstrates how to load an image from a file, define specific objects for detection (e.g., 'bread', 'starfruit', 'banana'), and construct a prompt to query the Gemini Robotics-ER 1.5 model. It uses the `genai` client to send the image and prompt, expecting a JSON response containing normalized [y, x] point coordinates and labels for the detected objects, similar to the example output provided.

```python
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

queries = [
    "bread",
    "starfruit",
    "banana",
]

prompt = f"""
    Get all points matching the following objects: {', '.join(queries)}. The
    label returned should be an identifying name for the object detected.
    The answer should follow the json format:

    [{"point": , "label": }}, ...]. The points are in

    [y, x] format normalized to 0-1000.
    """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

--------------------------------

### Generate Robotic Trajectories with Gemini Robotics-ER 1.5 (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview_hl=ja

This Python example initializes the Gemini Robotics-ER 1.5 client and loads an image to generate a robotic trajectory. It prompts the model to define 16 points (start + 15 intermediate) for moving a red pen to an organizer, returning the trajectory as a JSON array of '[y, x]' coordinates normalized to 0-1000.

```Python
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

points_data = []
prompt = """
        Place a point on the red pen, then 15 points for the trajectory of
        moving the red pen to the top of the organizer on the left.
        The points should be labeled by order of the trajectory, from '0'
        (start point at left hand) to <n> (final point)
        The answer should follow the json format:
        [{"point": <point>, "label": <label1>}, ...].
        The points are in [y, x] format normalized to 0-1000.
        """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
  )
)

print(image_response.text)
```

--------------------------------

### POST /v1beta/models/{model}:generateContent (Inline Video File)

Source: https://ai.google.dev/gemini-api/docs/video-understanding_hl=th

This endpoint allows you to generate textual content from a small video file (under 20MB) by providing its base64 encoded bytes directly in the request body. Authentication is typically handled via an `x-goog-api-key` header.

```APIDOC
## POST /v1beta/models/{model}:generateContent

### Description
This endpoint allows you to generate textual content from a small video file by providing its base64 encoded bytes directly in the request body. The video file size should be less than 20MB. Authentication is handled via the `x-goog-api-key` header.

### Method
POST

### Endpoint
/v1beta/models/{model}:generateContent

### Parameters
#### Path Parameters
- **model** (string) - Required - The name of the model to use for content generation (e.g., `gemini-2.5-flash`).

#### Request Body
- **contents** (array of objects) - Required - An array containing the parts of the request, including video data and a text prompt.
  - **contents[].parts** (array of objects) - Required - An array of content parts.
    - **parts[].inline_data** (object) - Required - Contains the video file data.
      - **inline_data.mime_type** (string) - Required - The MIME type of the video file (e.g., "video/mp4").
      - **inline_data.data** (string) - Required - The base64 encoded bytes of the video file.
    - **parts[].text** (string) - Required - The text prompt for content generation (e.g., "Please summarize the video in 3 sentences.").

### Request Example
```json
{
  "contents": [
    {
      "parts": [
        {
          "inline_data": {
            "mime_type": "video/mp4",
            "data": "[BASE64_ENCODED_VIDEO_DATA]"
          }
        },
        {
          "text": "Please summarize the video in 3 sentences."
        }
      ]
    }
  ]
}
```

### Response
#### Success Response (200)
- **candidates** (array of objects) - Contains the generated content.
  - **candidates[].content** (object) - The generated content.
    - **content.parts** (array of objects) - Array of parts, typically containing text.
      - **parts[].text** (string) - The generated text response.

#### Response Example
```json
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "The video shows... \nIt demonstrates... \nIn summary,..."
          }
        ]
      }
    }
  ]
}
```
```

--------------------------------

### Send local file image data to Gemini API (Python, JS, Go, REST)

Source: https://ai.google.dev/gemini-api/docs/vision

This example demonstrates how to read an image from a local file, convert it to the appropriate format (e.g., bytes or Base64), and then send it to the Gemini API's `generateContent` method. It covers Python, JavaScript, Go, and a cURL-based REST API call.

```Python
from google import genai
from google.genai import types

with open('path/to/small-sample.jpg', 'rb') as f:
    image_bytes = f.read()

client = genai.Client()
response = client.models.generate_content(
  model='gemini-2.5-flash',
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    'Caption this image.'
  ]
)

print(response.text)
```

```JavaScript
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const base64ImageFile = fs.readFileSync("path/to/small-sample.jpg", {
  encoding: "base64",
});

const contents = [
  {
    inlineData: {
      mimeType: "image/jpeg",
      data: base64ImageFile,
    },
  },
  { text: "Caption this image." },
];

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: contents,
});
console.log(response.text);
```

```Go
bytes, _ := os.ReadFile("path/to/small-sample.jpg")

parts := []*genai.Part{
  genai.NewPartFromBytes(bytes, "image/jpeg"),
  genai.NewPartFromText("Caption this image."),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

```REST
IMG_PATH="/path/to/your/image1.jpg"

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
B64FLAGS="--input"
else
B64FLAGS="-w0"
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
    "contents": [{
    "parts":[
        {
            "inline_data": {
            "mime_type":"image/jpeg",
            "data": "'""$(base64 $B64FLAGS $IMG_PATH)""'"
            }
        },
        {"text": "Caption this image."},
    ]
    }]
}' 2> /dev/null
```

--------------------------------

### File Upload and Content Generation (Go)

Source: https://ai.google.dev/gemini-api/docs/document-processing_hl=pl

Demonstrates uploading a local PDF file and generating content using the Gemini API with Go. This example uses the `google.golang.org/genai` library.

```APIDOC
## File Upload and Content Generation (Go)

### Description
This code snippet shows how to upload a local PDF file to the Gemini API and then generate content based on the file's content using the `google.golang.org/genai` library.

### Method
Uses the `google.golang.org/genai` library's `Files.UploadFromPath` and `Models.GenerateContent` methods.

### Endpoint
Implicitly uses the Gemini API endpoints for file upload and content generation.

### Parameters
N/A - Parameters are passed through the `google.golang.org/genai` library methods.

#### Request Body
Not applicable as the library handles the request formatting.

### Request Example
N/A

### Response
#### Success Response (200)
- **Text** (string) - The generated text from the Gemini API.

#### Response Example
```text
This is a summary of the document.
```
```

--------------------------------

### Perform Multimodal Chat Completion with Gemini API (Image Analysis)

Source: https://ai.google.dev/gemini-api/docs/openai_hl=hi

This snippet demonstrates how to send an image (encoded as base64) and a text prompt to the Gemini API for multimodal chat completion. It uses the `gemini-2.0-flash` model to analyze the content of the provided image and returns a text response. Ensure your API key is correctly configured and the image encoding function is available.

```python
base64_image = encode_image("Path/to/agi/image.jpeg")

response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url":  f"data:image/jpeg;base64,{base64_image}"
          },
        },
      ],
    }
  ],
)

print(response.choices[0])
```

```javascript
import OpenAI from "openai";
import fs from 'fs/promises';

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function encodeImage(imagePath) {
  try {
    const imageBuffer = await fs.readFile(imagePath);
    return imageBuffer.toString('base64');
  } catch (error) {
    console.error
```

--------------------------------

### Poll Gemini API Operation Status and Download Video (Shell)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=zh-tw

This shell script continuously polls the Gemini API for the status of an ongoing operation identified by its `operation_name`. It uses `curl` to fetch the status, `jq` to parse the JSON response for the 'done' field, and once complete, extracts the video URI to download the generated video. It waits for 10 seconds between checks.

```shell
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

--------------------------------

### Perform Multimodal Chat with Gemini API (Image Analysis)

Source: https://ai.google.dev/gemini-api/docs/openai_hl=id

This section demonstrates how to use the Gemini API to analyze the content of an image by sending both an image and a text prompt. It covers encoding an image to base64 format and then constructing a request to the `chat.completions` endpoint for multimodal understanding. The API returns a textual response describing the image content.

```python
base64_image = encode_image("Path/to/agi/image.jpeg")

response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?"
        },
        {
          "type": "image_url",
          "image_url": {
            "url":  f"data:image/jpeg;base64,{base64_image}"
          }
        }
      ]
    }
  ]
)

print(response.choices[0])
```

```javascript
import OpenAI from "openai";
import fs from 'fs/promises';

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function encodeImage(imagePath) {
  try {
    const imageBuffer = await fs.readFile(imagePath);
    return imageBuffer.toString('base64');
  } catch (error) {
    console.error("Error encoding image:", error);
    return null;
  }
}

async function main() {
  const imagePath = "Path/to/agi/image.jpeg";
  const base64Image = await encodeImage(imagePath);

  const messages = [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?"
        },
        {
          "type": "image_url",
          "image_url": {
            "url": `data:image/jpeg;base64,${base64Image}`
          }
        }
      ]
    }
  ];

  try {
    const response = await openai.chat.completions.create({
      model: "gemini-2.0-flash",
      messages: messages
    });

    console.log(response.choices[0]);
  } catch (error) {
    console.error("Error calling Gemini API:", error);
  }
}

main();
```

```bash
bash -c '
  base64_image=$(base64 -i "Path/to/agi/image.jpeg");
  curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer GEMINI_API_KEY" \
    -d "{
      \"model\": \"gemini-2.0-flash\",
      \"messages\": [
        {
          \"role\": \"user\",
          \"content\": [
            { \"type\": \"text\", \"text\": \"What is in this image?\" },
            {
              \"type\": \"image_url\",
              \"image_url\": { \"url\": \"data:image/jpeg;base64,${base64_image}\" }
            }
          ]
        }
      ]
    }"
'
```

--------------------------------

### GET /operations/{operation_name}

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=es-419

Retrieves the current status of a long-running video generation operation. This endpoint is used for polling until the video generation is complete and the results are available.

```APIDOC
## GET /operations/{operation_name}

### Description
Retrieves the current status of a long-running video generation operation. This endpoint is used for polling until the video generation is complete and the results are available.

### Method
GET

### Endpoint
/operations/{operation_name}

### Parameters
#### Path Parameters
- **operation_name** (string) - Required - The unique name of the operation, obtained from the initial 'generateVideos' response (e.g., "operations/video-generation-12345").

### Request Example
```
GET /operations/video-generation-12345
```

### Response
#### Success Response (200)
- **name** (string) - The unique name of the operation.
- **done** (boolean) - Indicates if the operation has finished. If false, the operation is still in progress.
- **response** (object) - Present only if 'done' is true and the operation was successful. Contains the result of the operation.
  - **generatedVideos** (array) - A list of objects, each representing a generated video.
    - **video** (object) - Contains details about the generated video.
      - **fileId** (string) - An identifier that can be used to download the video.
      - **videoBytes** (string) - (Optional) Base64 encoded video data, primarily available in client library object for direct access or saving.
- **error** (object) - Present only if 'done' is true and the operation failed, containing details about the error.

#### Response Example (Operation in progress)
```json
{
  "name": "operations/video-generation-12345",
  "done": false
}
```

#### Response Example (Operation completed successfully)
```json
{
  "name": "operations/video-generation-12345",
  "done": true,
  "response": {
    "generatedVideos": [
      {
        "video": {
          "fileId": "file-xyz-789"
        }
      }
    ]
  }
}
```
```

--------------------------------

### Upload Local PDF and Summarize with Gemini API (Go)

Source: https://ai.google.dev/gemini-api/docs/document-processing_hl=zh-cn

This Go program initializes a Gemini API client and demonstrates how to upload a local PDF file using `client.Files.UploadFromPath`. It then constructs a `generateContent` request, providing the uploaded file's URI and a text prompt to obtain a summary, which is then printed to standard output. Ensure the `GEMINI_API_KEY` environment variable is set.

```go
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, _ := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey:  os.Getenv("GEMINI_API_KEY"),
        Backend: genai.BackendGeminiAPI,
    })
    localPdfPath := "/path/to/file.pdf"

    uploadConfig := &genai.UploadFileConfig{MIMEType: "application/pdf"}
    uploadedFile, _ := client.Files.UploadFromPath(ctx, localPdfPath, uploadConfig)

    promptParts := []*genai.Part{
        genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
        genai.NewPartFromText("Give me a summary of this pdf file."),
    }
    contents := []*genai.Content{
        genai.NewContentFromParts(promptParts, genai.RoleUser),
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        contents,
        nil,
    )

    fmt.Println(result.Text())
}

```

--------------------------------

### Generate Content with Gemini API Thinking Feature (Go)

Source: https://ai.google.dev/gemini-api/docs/thinking-mode

This Go example demonstrates how to use the Gemini API's 'Thinking' feature to generate content. It configures the `GenerateContentStream` call to include thoughts and then iterates through the response parts, printing 'Thought' or 'Answer' based on the part type. This allows observing the model's internal reasoning process.

```Go
package main

import (
  "context"
  "fmt"
  "log"
  "os"
  "google.golang.org/genai"
)

const prompt = `
Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.
The person who lives in the red house owns a cat.
Bob does not live in the green house.
Carol owns a dog.
The green house is to the left of the red house.
Alice does not own a cat.
Who lives in each house, and what pet do they own?
`

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  contents := genai.Text(prompt)
  model := "gemini-2.5-pro"

  resp := client.Models.GenerateContentStream(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      IncludeThoughts: true,
    },
  })

  for chunk := range resp {
    for _, part := range chunk.Candidates[0].Content.Parts {
      if len(part.Text) == 0 {
        continue
      }

      if part.Thought {
        fmt.Printf("Thought: %s\n", part.Text)
      } else {
        fmt.Printf("Answer: %s\n", part.Text)
      }
    }
  }
}
```

--------------------------------

### Define Tool Function and Declaration for Smart Light Control

Source: https://ai.google.dev/gemini-api/docs/function-calling_example=weather&hl=ru

This code defines a tool function declaration for controlling smart lights, specifying its name, description, and parameters like brightness and color temperature. It also includes the actual function implementation, which acts as a mock API to simulate setting light values. This setup allows the Gemini model to understand and propose calls to this function based on user prompts, demonstrated in both Python and JavaScript.

```python
set_light_values_declaration = {
    "name": "set_light_values",
    "description": "Sets the brightness and color temperature of a light.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "integer",
                "description": "Light level from 0 to 100. Zero is off and 100 is full brightness",
            },
            "color_temp": {
                "type": "string",
                "enum": ["daylight", "cool", "warm"],
                "description": "Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.",
            }
        },
        "required": ["brightness", "color_temp"]
    }
}

def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:
    """Set the brightness and color temperature of a room light. (mock API).

    Args:
        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness
        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.

    Returns:
        A dictionary containing the set brightness and color temperature.
    """
    return {"brightness": brightness, "colorTemperature": color_temp}
```

```javascript
import { Type } from '@google/genai';

const setLightValuesFunctionDeclaration = {
  name: 'set_light_values',
  description: 'Sets the brightness and color temperature of a light.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',
      },
      color_temp: {
        type: Type.STRING,
        enum: ['daylight', 'cool', 'warm'],
        description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm.`',
      }
    },
    required: ['brightness', 'color_temp']
  }
};

function setLightValues(brightness, color_temp) {
  return {
    brightness: brightness,
    colorTemperature: color_temp
  };
}
```

--------------------------------

### Initialize Gemini Robotics-ER 1.5 Client for Multi-Step Task Orchestration (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview_hl=ja

This Python snippet initializes the Gemini Robotics-ER 1.5 client and loads an image, serving as the starting point for orchestrating complex, multi-step tasks. It sets up the necessary components to send prompts and receive responses for advanced robotic actions, though the prompt and content generation parts are not fully shown.

```Python
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)
```

--------------------------------

### Call Gemini Robotics Model with Image and Code Execution Tool (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview_hl=he

This Python code demonstrates how to use the `google.genai` library to interact with the `gemini-robotics-er-1.5-preview` model. It sends an image and a text prompt to the model, enabling the `code_execution` tool to allow the model to generate and execute Python code for dynamic tasks. The script then processes the model's response, printing any text output, generated executable code, and code execution results. It requires the `google-generativeai` library and an API key.

```python
from google import genai
from google.genai import types

client = genai.Client(api_key=YOUR_API_KEY)

MODEL_ID = "gemini-robotics-er-1.5-preview"

# Load your image and set up your prompt
with open('path/to/image-of-object.jpg', 'rb') as f:
    image_bytes = f.read()
prompt = """
          What is the air quality reading? Using the code execution feature,
          zoom in on the image to take a closer look.
        """

response = client.models.generate_content(
    model=MODEL_ID,
    contents=[
        types.Part.from_bytes(
            data=image_bytes,
            mime_type='image/jpeg',
        ),
        prompt
    ],
    config = types.GenerateContentConfig(
        temperature=0.5,
        tools=[types.Tool(code_execution=types.ToolCodeExecution)]
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    if part.executable_code is not None:
        print(part.executable_code.code)
    if part.code_execution_result is not None:
        print(part.code_execution_result.output)
```

--------------------------------

### Define Parallel Function Schemas for Gemini API

Source: https://ai.google.dev/gemini-api/docs/function-calling

This snippet defines three function schemas (`power_disco_ball`, `start_music`, `dim_lights`) in Python and JavaScript. These schemas provide the Gemini API model with metadata, including function names, descriptions, and expected parameters (e.g., `power`, `energetic`, `loud`, `brightness`), enabling the model to understand and call these functions for a 'disco' setup.

```python
power_disco_ball = {
    "name": "power_disco_ball",
    "description": "Powers the spinning disco ball.",
    "parameters": {
        "type": "object",
        "properties": {
            "power": {
                "type": "boolean",
                "description": "Whether to turn the disco ball on or off.",
            }
        },
        "required": ["power"],
    },
}

start_music = {
    "name": "start_music",
    "description": "Play some music matching the specified parameters.",
    "parameters": {
        "type": "object",
        "properties": {
            "energetic": {
                "type": "boolean",
                "description": "Whether the music is energetic or not.",
            },
            "loud": {
                "type": "boolean",
                "description": "Whether the music is loud or not.",
            },
        },
        "required": ["energetic", "loud"],
    },
}

dim_lights = {
    "name": "dim_lights",
    "description": "Dim the lights.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "number",
                "description": "The brightness of the lights, 0.0 is off, 1.0 is full.",
            }
        },
        "required": ["brightness"],
    },
}

```

```javascript
import { Type } from '@google/genai';

const powerDiscoBall = {
  name: 'power_disco_ball',
  description: 'Powers the spinning disco ball.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      power: {
        type: Type.BOOLEAN,
        description: 'Whether to turn the disco ball on or off.'
      }
    },
    required: ['power']
  }
};

const startMusic = {
  name: 'start_music',
  description: 'Play some music matching the specified parameters.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      energetic: {
        type: Type.BOOLEAN,
        description: 'Whether the music is energetic or not.'
      },
      loud: {
        type: Type.BOOLEAN,
        description: 'Whether the music is loud or not.'
      }
    },
    required: ['energetic', 'loud']
  }
};

const dimLights = {
  name: 'dim_lights',
  description: 'Dim the lights.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'
      }
    },
    required: ['brightness']
  }
};

```

--------------------------------

### Generate and Save Gemini API Text-to-Speech Audio (Python, Node.js, cURL)

Source: https://ai.google.dev/gemini-api/docs/speech-generation_hl=pl

This section provides examples for generating speech audio from text using the Gemini API's text-to-speech model (`gemini-2.5-flash-preview-tts`). It configures the response to be audio, specifies a prebuilt voice ('Kore'), and then extracts the audio data from the API response. The generated audio data is then saved to a WAV file using the respective language's methods (Python's `wave` module, Node.js's `wav` package, or cURL with `base64` and `ffmpeg`).

```python
client = genai.Client()

response = client.models.generate_content(
   model="gemini-2.5-flash-preview-tts",
   contents="Say cheerfully: Have a wonderful day!",
   config=types.GenerateContentConfig(
      response_modalities=["AUDIO"],
      speech_config=types.SpeechConfig(
         voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(
               voice_name='Kore',
            )
         )
      ),
   )
)

data = response.candidates[0].content.parts[0].inline_data.data

file_name='out.wav'
wave_file(file_name, data) # Saves the file to current directory
```

```javascript
import {GoogleGenAI} from '@google/genai';

async function main() {
   const ai = new GoogleGenAI({});

   const response = await ai.models.generateContent({
      model: "gemini-2.5-flash-preview-tts",
      contents: [{ parts: [{ text: 'Say cheerfully: Have a wonderful day!' }] }],
      config: {
            responseModalities: ['AUDIO'],
            speechConfig: {
               voiceConfig: {
                  prebuiltVoiceConfig: { voiceName: 'Kore' },
               },
            },
      },
   });

   const data = response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
   const audioBuffer = Buffer.from(data, 'base64');

   const fileName = 'out.wav';
   await saveWaveFile(fileName, audioBuffer);
}
await main();
```

```shell
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{
        "contents": [{
          "parts":[{
            "text": "Say cheerfully: Have a wonderful day!"
          }]
        }],
        "generationConfig": {
          "responseModalities": ["AUDIO"],
          "speechConfig": {
            "voiceConfig": {
              "prebuiltVoiceConfig": {
                "voiceName": "Kore"
              }
            }
          }
        },
        "model": "gemini-2.5-flash-preview-tts"
    }' | jq -r '.candidates[0].content.parts[0].inlineData.data' | \
          base64 --decode >out.pcm
# You may need to install ffmpeg.
ffmpeg -f s16le -ar 24000 -ac 1 -i out.pcm out.wav
```

--------------------------------

### Normalize Gemini Embeddings for Smaller Dimensions (Python)

Source: https://ai.google.dev/gemini-api/docs/embeddings

This Python code snippet demonstrates how to normalize embedding values for dimensions other than 3072, specifically 768 and 1536, using the NumPy library. Normalization ensures that semantic similarity comparisons are based on vector direction rather than magnitude, improving accuracy. The code calculates the L2 norm of the embedding vector and then divides each element by this norm, printing the length and the norm of the resulting normalized embedding.

```python
import numpy as np
from numpy.linalg import norm

embedding_values_np = np.array(embedding_obj.values)
normed_embedding = embedding_values_np / np.linalg.norm(embedding_values_np)

print(f"Normed embedding length: {len(normed_embedding)}")
print(f"Norm of normed embedding: {np.linalg.norm(normed_embedding):.6f}") # Should be very close to 1
```

--------------------------------

### GET /operations/{operation_name} - Get Video Generation Operation Status

Source: https://ai.google.dev/gemini-api/docs/video_hl=th

This endpoint allows clients to poll for the status of a previously initiated long-running video generation operation. Once the operation is complete (indicated by `done: true`), the response will include details about the generated video, including its download URI.

```APIDOC
## GET /operations/{operation_name}

### Description
Retrieves the current status and results of a video generation operation. Clients should poll this endpoint periodically until the operation is marked as 'done'. Once complete, the response will contain the generated video's metadata and download URI.

### Method
GET

### Endpoint
`/operations/{operation_name}`

### Parameters
#### Path Parameters
- **operation_name** (string) - Required - The name of the operation returned by the `predictLongRunning` endpoint.

### Response
#### Success Response (200)
- **name** (string) - The operation name.
- **done** (boolean) - Indicates if the operation is complete. True if finished, false otherwise.
- **response** (object) - Present when `done` is true, containing the operation's result.
  - **generateVideoResponse** (object) - Specific details for video generation.
    - **generatedSamples** (array) - List of generated video samples.
      - **video** (object) - Details of a generated video.
        - **uri** (string) - The URI from which the generated video file can be downloaded.
- **error** (object) - Present if the operation failed, containing error details.

#### Response Example
```json
{
  "name": "operations/generate-video-12345",
  "done": false
}
```

```json
{
  "name": "operations/generate-video-12345",
  "done": true,
  "response": {
    "@type": "type.googleapis.com/google.ai.generativelanguage.v1beta.GenerateVideoResponse",
    "generateVideoResponse": {
      "generatedSamples": [
        {
          "video": {
            "uri": "https://storage.googleapis.com/path/to/dialogue_example.mp4"
          }
        }
      ]
    }
  }
}
```
```

--------------------------------

### POST /v1beta/models/{model}:generateContent with Google Search Retrieval

Source: https://ai.google.dev/gemini-api/docs/grounding_hl=vi

Generates content using a specified Gemini model, optionally leveraging Google Search Retrieval for dynamic information access based on a confidence threshold.

```APIDOC
## POST /v1beta/models/{model}:generateContent

### Description
This endpoint allows you to send a prompt to a specified Gemini model for content generation. It supports integrating the Google Search Retrieval tool to dynamically search for information when the model's confidence in its own knowledge falls below a defined threshold.

### Method
POST

### Endpoint
https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent

### Parameters
#### Path Parameters
- **model** (string) - Required - The ID of the Gemini model to use for content generation (e.g., `gemini-1.5-flash`).

#### Headers
- **x-goog-api-key** (string) - Required - Your Google Cloud API key.
- **Content-Type** (string) - Required - Must be `application/json`.

#### Request Body
- **contents** (array of object) - Required - The input content for the model.
  - **contents[].parts** (array of object) - Required - The parts of the content.
    - **contents[].parts[].text** (string) - Required - The text content of the prompt.
- **tools** (array of object) - Optional - A list of tools available to the model. Currently supports `google_search_retrieval`.
  - **tools[].google_search_retrieval** (object) - Optional - Configuration for enabling Google Search Retrieval.
    - **tools[].google_search_retrieval.dynamic_retrieval_config** (object) - Required - Configuration for dynamic retrieval based on confidence.
      - **tools[].google_search_retrieval.dynamic_retrieval_config.mode** (string) - Required - The mode for dynamic retrieval. Currently supports `MODE_DYNAMIC`.
      - **tools[].google_search_retrieval.dynamic_retrieval_config.dynamic_threshold** (number) - Required - A float value between 0.0 and 1.0 representing the confidence threshold. The model will perform a search only if its confidence in generating a response is below this threshold (e.g., 0.7 for 70%).

### Request Example
```json
{
  "contents": [
    {"parts": [{"text": "Who won the euro 2024?"}]}
  ],
  "tools": [{
    "google_search_retrieval": {
      "dynamic_retrieval_config": {
        "mode": "MODE_DYNAMIC",
        "dynamic_threshold": 0.7
      }
    }
  }]
}
```

### Response
#### Success Response (200 OK)
- **candidates** (array of object) - Generated candidates from the model.
  - **candidates[].content** (object) - The generated content parts.
    - **candidates[].content.parts** (array of object) - The parts of the generated content.
      - **candidates[].content.parts[].text** (string) - The generated text.
  - **candidates[].groundingMetadata** (object) - Optional - Metadata about whether Google Search was used for grounding the response.

#### Response Example
```json
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Spain won the Euro 2024, defeating England in the final."
          }
        ]
      },
      "groundingMetadata": {
        "retrievalQueries": [
          {
            "text": "who won euro 2024"
          }
        ]
      }
    }
  ]
}
```
```

--------------------------------

### Upload PDFs and Generate Content with Gemini API in Go

Source: https://ai.google.dev/gemini-api/docs/document-processing_hl=id

This Go program downloads two PDF files from specified URLs to local paths. It then uses the Gemini API client to upload these local PDF files, providing the MIME type. Finally, it constructs a multi-modal prompt using the URIs of the uploaded files and a text query, sending it to the `gemini-2.5-flash` model to generate content and print the result.

```go
package main

import (
    "context"
    "fmt"
    "io"
    "net/http"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, _ := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey:  os.Getenv("GEMINI_API_KEY"),
        Backend: genai.BackendGeminiAPI,
    })

    docUrl1 := "https://arxiv.org/pdf/2312.11805"
    docUrl2 := "https://arxiv.org/pdf/2403.05530"
    localPath1 := "doc1_downloaded.pdf"
    localPath2 := "doc2_downloaded.pdf"

    respHttp1, _ := http.Get(docUrl1)
    defer respHttp1.Body.Close()

    outFile1, _ := os.Create(localPath1)
    _, _ = io.Copy(outFile1, respHttp1.Body)
    outFile1.Close()

    respHttp2, _ := http.Get(docUrl2)
    defer respHttp2.Body.Close()

    outFile2, _ := os.Create(localPath2)
    _, _ = io.Copy(outFile2, respHttp2.Body)
    outFile2.Close()

    uploadConfig1 := &genai.UploadFileConfig{MIMEType: "application/pdf"}
    uploadedFile1, _ := client.Files.UploadFromPath(ctx, localPath1, uploadConfig1)

    uploadConfig2 := &genai.UploadFileConfig{MIMEType: "application/pdf"}
    uploadedFile2, _ := client.Files.UploadFromPath(ctx, localPath2, uploadConfig2)

    promptParts := []*genai.Part{
        genai.NewPartFromURI(uploadedFile1.URI, uploadedFile2.MIMEType),
        genai.NewPartFromURI(uploadedFile2.URI, uploadedFile2.MIMEType),
        genai.NewPartFromText("What is the difference between each of the " +
                              "main benchmarks between these two papers? " +
                              "Output these in a table."),
    }
    contents := []*genai.Content{
        genai.NewContentFromParts(promptParts, genai.RoleUser),
    }

    modelName := "gemini-2.5-flash"
    result, _ := client.Models.GenerateContent(
        ctx,
        modelName,
        contents,
        nil,
    )

    fmt.Println(result.Text())
}
```

--------------------------------

### Generate Object Movement Trajectory with Gemini Robotics API (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview

This Python code uses the Gemini Robotics-ER 1.5-preview model to calculate a precise trajectory for moving an object. It inputs an image and a prompt requesting a start point and a multi-point path (e.g., 15 points) to a target location, then outputs the trajectory as a series of normalized [y, x] coordinates in JSON format, labeled in order of movement. The temperature is set to 0.5 for controlled generation.

```python
MODEL_ID = "gemini-robotics-er-1.5-preview"
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image and set up your prompt
with open('path/to/image-with-objects.jpg', 'rb') as f:
    image_bytes = f.read()

points_data = []
prompt = """
        Place a point on the red pen, then 15 points for the trajectory of
        moving the red pen to the top of the organizer on the left.
        The points should be labeled by order of the trajectory, from '0'
        (start point at left hand) to <n> (final point)
        The answer should follow the json format:
        [{"point": <point>, "label": <label1>}, ...].
        The points are in [y, x] format normalized to 0-1000.
        """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
  )
)

print(image_response.text)
```

--------------------------------

### Generate and Download Video (Gemini API)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=fa

This collection of snippets demonstrates how to generate a video from a text prompt using the Gemini API, poll its status until completion, and then download the generated video to a local file. Examples are provided for TypeScript, Go, and Shell.

```TypeScript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.0-generate-001",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);
```

```Go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}
```

```Shell
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That'\''s the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
      }
    ]
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

--------------------------------

### Generate Prompt for Abstract Category Detection with Gemini Robotics-ER 1.5 (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview_hl=id

This Python code snippet illustrates how to craft a prompt for the Gemini Robotics-ER 1.5 model to detect objects based on abstract categories, such as 'fruit', rather than specific item names. The prompt instructs the model to return a JSON array with normalized [y, x] point coordinates and an identifying label for each detected instance. This approach allows for broader visual understanding tasks.

```python
prompt = f"""
        Get all points for fruit. The label returned should be an identifying
        name for the object detected.
        """ + """The answer should follow the json format:
        [{"point": <point>, "label": <label1>}, ...]. The points are in
        [y, x] format normalized to 0-1000."""
```

--------------------------------

### Generate Stylized Illustrations with Gemini Image Generation API

Source: https://ai.google.dev/gemini-api/docs/image-generation

This snippet demonstrates how to generate a stylized image, such as a sticker, using the Gemini API's image generation model (gemini-2.5-flash-image). It takes a detailed text prompt describing the desired style, subject, and characteristics, then saves the generated image to a local file. Dependencies include the Gemini client library for the respective language and an image processing library (Pillow for Python, fs for Node.js).

```python
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

client = genai.Client()

# Generate an image from a text prompt
response = client.models.generate_content(
    model="gemini-2.5-flash-image",
    contents="A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It's munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white.",
)

image_parts = [
    part.inline_data.data
    for part in response.candidates[0].content.parts
    if part.inline_data
]

if image_parts:
    image = Image.open(BytesIO(image_parts[0]))
    image.save('red_panda_sticker.png')
    image.show()
```

```javascript
import { GoogleGenAI, Modality } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const prompt =
    "A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It's munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash-image",
    contents: prompt,
  });
  for (const part of response.candidates[0].content.parts) {
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("red_panda_sticker.png", buffer);
      console.log("Image saved as red_panda_sticker.png");
    }
  }
}

main();
```

```go
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash-image",
        genai.Text("A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It's munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white."),
    )

    for _, part := range result.Candidates[0].Content.Parts {
        if part.Text != "" {
            fmt.Println(part.Text)
        } else if part.InlineData != nil {
            imageBytes := part.InlineData.Data
            outputFilename := "red_panda_sticker.png"
            _ = os.WriteFile(outputFilename, imageBytes, 0644)
        }
    }
}
```

```bash
curl -s -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It'\''s munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white."}
      ]
    }]
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > red_panda_sticker.png
```

--------------------------------

### GET /operations/{operation_name}

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=th

Retrieve the current status of a video generation long-running operation. This endpoint should be polled periodically until the `done` field in the response is `true`, at which point the generated video details will be available in the `response` field.

```APIDOC
## GET /operations/{operation_name}

### Description
Retrieve the current status of a video generation long-running operation. This endpoint should be polled periodically until the `done` field in the response is `true`, at which point the generated video details will be available in the `response` field.

### Method
GET

### Endpoint
/v1beta/operations/{operation_name}

### Parameters
#### Path Parameters
- **operation_name** (string) - Required - The full name of the operation (e.g., `operations/generate-video-1234567890`) returned by the `predictLongRunning` call.

### Request Example
N/A

### Response
#### Success Response (200)
- **name** (string) - The unique name of the long-running operation.
- **metadata** (object) - Additional metadata about the operation.
- **done** (boolean) - Indicates if the operation is complete.
- **response** (object) - Only present if `done` is true. Contains the result of the operation.
  - **generateVideoResponse** (object) - The video generation specific response.
    - **generatedSamples** (array of objects) - A list of generated video samples.
      - **video** (object) - Details of the generated video.
        - **uri** (string) - A pre-signed URL to download the generated video.

#### Response Example
```json
{
  "name": "operations/generate-video-1234567890",
  "metadata": {},
  "done": true,
  "response": {
    "@type": "type.googleapis.com/google.cloud.generativelanguage.v1beta.GenerateVideoResponse",
    "generatedSamples": [
      {
        "video": {
          "uri": "https://storage.googleapis.com/some-bucket/dialogue_example.mp4?Expires=...&Signature=..."
        }
      }
    ]
  }
}
```
(When not done)
```json
{
  "name": "operations/generate-video-1234567890",
  "metadata": {},
  "done": false
}
```
```

--------------------------------

### POST /models/{model}:predictLongRunning & GET /operations/{operation_name} (Generate & Retrieve Video)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=de

This endpoint initiates a long-running operation to generate a video from a text prompt and optionally a starting image. The operation's status must be polled using the GET /operations/{operation_name} endpoint until completion. Once the video is generated, it can be downloaded.

```APIDOC
## POST /models/{model}:predictLongRunning & GET /operations/{operation_name} (Generate & Retrieve Video)

### Description
This endpoint initiates a long-running operation to generate a video from a text prompt and optionally a starting image. The operation's status must be polled using the GET /operations/{operation_name} endpoint until completion. Once the video is generated, it can be downloaded.

### Method
POST (for initial generation), GET (for polling and download)

### Endpoint
Initial Generation: `/models/{model}:predictLongRunning`
Polling: `/operations/{operation_name}`
Download: `{video.uri}` (dynamic URI from polling response)

### Parameters
#### Path Parameters
- **model** (string) - Required - The model ID for video generation (e.g., `veo-3.0-generate-001`).
- **operation_name** (string) - Required - (For polling) The name of the long-running operation returned by the initial generation request.

#### Request Body (for initial generation)
- **instances** (array of objects) - Required - Contains the video generation request.
  - **prompt** (string) - Required - The text prompt describing the desired video.
  - **image** (object) - Optional - An image object returned from `generateImages` to be used as a video starting frame. Structure may vary by SDK.

### Request Example (Initial Generation)
```json
{
  "instances": [{
      "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That's the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
    }
  ]
}
```

### Response
#### Success Response (200/202 - Initial Generation)
- **name** (string) - The unique name of the long-running operation.

#### Success Response (200 - Polling, when `done` is true)
- **done** (boolean) - Indicates if the operation is complete (`true` when video is ready).
- **response** (object) - The actual result of the operation.
  - **generateVideoResponse** (object) - Contains the video details.
    - **generatedSamples** (array of objects) - An array of generated video samples.
      - **video** (object) - The generated video object.
        - **uri** (string) - A direct download URI for the generated video.
        - **videoBytes** (string) - (Go SDK specific) Base64 encoded video content.

#### Response Example (Polling, when `done` is true)
```json
{
  "done": true,
  "response": {
    "generateVideoResponse": {
      "generatedSamples": [
        {
          "video": {
            "uri": "https://example.com/path/to/generated_video.mp4"
          }
        }
      ]
    }
  }
}
```
```

--------------------------------

### JSON Orchestration Plan from Model

Source: https://ai.google.dev/gemini-api/docs/robotics-overview

This JSON snippet represents the robot's planned sequence of actions generated by the model based on the orchestration prompt. It lists function calls (`move`, `setGripperState`) with their respective arguments to perform the pick-and-place task, following prior reasoning provided by the model.

```json
[
  {
    "function": "move",
    "args": [
      163,
      427,
      true
    ]
  },
  {
    "function": "setGripperState",
    "args": [
      true
    ]
  },
  {
    "function": "move",
    "args": [
      163,
      427,
      false
    ]
  },
  {
    "function": "setGripperState",
    "args": [
      false
    ]
  },
  {
    "function": "move",
    "args": [
      163,
      427,
      true
    ]
  },
  {
    "function": "move",
    "args": [
      -247,
      90,
      true
    ]
  },
  {
    "function": "move",
    "args": [
      -247,
      90,
      false
    ]
  },
  {
    "function": "setGripperState",
    "args": [
      true
    ]
  }
]
```

--------------------------------

### Generate, Poll, and Download Video from Text (cURL/Shell)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=ar

This shell script demonstrates how to generate a video using the Gemini API via cURL requests. It polls the long-running operation status using `jq` to parse JSON responses, then extracts the download URI and downloads the video using cURL once generation is complete. It requires `jq` and the `GEMINI_API_KEY` environment variable.

```shell
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That\\\'s the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
      }
    ]
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done
```

--------------------------------

### Perform Multi-Speaker TTS and Save Audio (JavaScript)

Source: https://ai.google.dev/gemini-api/docs/speech-generation_hl=es-419

Shows how to make a multi-speaker TTS call to the Gemini API using the `@google/genai` library in Node.js. It specifies different prebuilt voices ('Kore' for Joe, 'Puck' for Jane) for speakers in a conversation and includes an asynchronous helper function to save the received audio buffer to a WAV file using the `wav` library.

```javascript
import {GoogleGenAI} from '@google/genai';
import wav from 'wav';

async function saveWaveFile(
   filename,
   pcmData,
   channels = 1,
   rate = 24000,
   sampleWidth = 2,
) {
   return new Promise((resolve, reject) => {
      const writer = new wav.FileWriter(filename, {
            channels,
            sampleRate: rate,
            bitDepth: sampleWidth * 8,
      });

      writer.on('finish', resolve);
      writer.on('error', reject);

      writer.write(pcmData);
      writer.end();
   });
}

async function main() {
   const ai = new GoogleGenAI({});

   const prompt = `TTS the following conversation between Joe and Jane:
         Joe: How's it going today Jane?
         Jane: Not too bad, how about you?`;

   const response = await ai.models.generateContent({
      model: "gemini-2.5-flash-preview-tts",
      contents: [{ parts: [{ text: prompt }] }],
      config: {
            responseModalities: ['AUDIO'],
            speechConfig: {
               multiSpeakerVoiceConfig: {
                  speakerVoiceConfigs: [
                        {
                           speaker: 'Joe',
                           voiceConfig: {
                              prebuiltVoiceConfig: { voiceName: 'Kore' }
                           }
                           },
                        {
                           speaker: 'Jane',
                           voiceConfig: {
                              prebuiltVoiceConfig: { voiceName: 'Puck' }
                           }
                        }
                  ]
               }
            }
      }
   });

   const data = response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
   const audioBuffer = Buffer.from(data, 'base64');

   const fileName = 'out.wav';
   await saveWaveFile(fileName, audioBuffer);
}

await main();
```

--------------------------------

### Generate Text Embeddings with Gemini API (Python)

Source: https://ai.google.dev/gemini-api/docs/context7

This Python example demonstrates how to generate vector embeddings for text using the Gemini embedding model. It covers generating single embeddings, specifying a 'task_type' for optimized performance in specific use cases like retrieval, and performing batch embedding generation for multiple texts.

```python
from google import genai

client = genai.Client()

# Generate single embedding
result = client.models.embed_content(
    model="gemini-embedding-001",
    contents="What is the meaning of life?"
)

print(f"Embedding dimension: {len(result.embeddings[0].values)}")
print(f"First few values: {result.embeddings[0].values[:5]}")

# Generate embeddings with task type for better performance
result = client.models.embed_content(
    model="gemini-embedding-001",
    contents="Machine learning is a subset of artificial intelligence",
    config={"task_type": "retrieval_document"}
)

# Batch embedding generation
texts = [
    "The quick brown fox jumps over the lazy dog",
    "Machine learning enables computers to learn from data",
    "Python is a popular programming language"
]

results = [
    client.models.embed_content(
        model="gemini-embedding-001",
        contents=text,
        config={"task_type": "retrieval_document"}
    )
    for text in texts
]

embeddings = [r.embeddings[0].values for r in results]
print(f"Generated {len(embeddings)} embeddings")
```

--------------------------------

### Stream Content with Google Gemini API (Avant)

Source: https://ai.google.dev/gemini-api/docs/migrate_hl=fr

This snippet demonstrates how to generate and stream content from the Google Gemini API using an earlier API structure. It shows examples in Python, JavaScript, and Go, iterating over response chunks to print them as they arrive. Users need to initialize the client and model appropriately, often requiring an API key for authentication.

```python
import google.generativeai as genai

response = model.generate_content(
    "Write a cute story about cats.",
    stream=True)
for chunk in response:
    print(chunk.text)
```

```javascript
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

const prompt = "Write a story about a magic backpack.";

const result = await model.generateContentStream(prompt);

// Print text as it comes in.
for await (const chunk of result.stream) {
  const chunkText = chunk.text();
  process.stdout.write(chunkText);
}
```

```go
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
iter := model.GenerateContentStream(ctx, genai.Text("Write a story about a magic backpack."))
for {
    resp, err := iter.Next()
    if err == iterator.Done {
        break
    }
    if err != nil {
        log.Fatal(err)
    }
    printResponse(resp) // utility for printing the response
}
```

--------------------------------

### Generate Graded Recipe List with Custom Enum Schema (Python)

Source: https://ai.google.dev/gemini-api/docs/json-mode

This Python example demonstrates how to use the Gemini API to generate a structured list of recipes, each assigned a custom grade. It leverages Pydantic's `BaseModel` and Python's `enum.Enum` to define a complex JSON schema, ensuring the model's output adheres to the specified structure and grade enumeration.

```python
from google import genai

import enum
from pydantic import BaseModel

class Grade(enum.Enum):
    A_PLUS = "a+"
    A = "a"
    B = "b"
    C = "c"
    D = "d"
    F = "f"

class Recipe(BaseModel):
  recipe_name: str
  rating: Grade

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='List 10 home-baked cookie recipes and give them grades based on tastiness.',
    config={
        'response_mime_type': 'application/json',
        'response_schema': list[Recipe],
    },
)

print(response.text)
```

--------------------------------

### Define Smart Light Control Tool Declarations and Mock Implementations

Source: https://ai.google.dev/gemini-api/docs/function-calling_example=weather&hl=ar

This snippet demonstrates how to define a tool declaration for controlling smart lights, specifying its name, description, and parameters (brightness, color temperature). It also provides mock Python and JavaScript functions that simulate setting light values based on the model's suggestions.

```python
set_light_values_declaration = {
    "name": "set_light_values",
    "description": "Sets the brightness and color temperature of a light.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "integer",
                "description": "Light level from 0 to 100. Zero is off and 100 is full brightness",
            },
            "color_temp": {
                "type": "string",
                "enum": ["daylight", "cool", "warm"],
                "description": "Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.",
            }
        },
        "required": ["brightness", "color_temp"]
    }
}

# This is the actual function that would be called based on the model's suggestion
def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:
    """Set the brightness and color temperature of a room light. (mock API).

    Args:
        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness
        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.

    Returns:
        A dictionary containing the set brightness and color temperature.
    """
    return {"brightness": brightness, "colorTemperature": color_temp}
```

```javascript
import { Type } from '@google/genai';

// Define a function that the model can call to control smart lights
const setLightValuesFunctionDeclaration = {
  name: 'set_light_values',
  description: 'Sets the brightness and color temperature of a light.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',
      },
      color_temp: {
        type: Type.STRING,
        enum: ['daylight', 'cool', 'warm'],
        description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',
      }
    },
    required: ['brightness', 'color_temp']
  }
};

/**

*   Set the brightness and color temperature of a room light. (mock API)
*   @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness
*   @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.
*   @return {Object} A dictionary containing the set brightness and color temperature.
*/
function setLightValues(brightness, color_temp) {
  return {
    brightness: brightness,
    colorTemperature: color_temp
  };
}
```

--------------------------------

### Compare Images with Gemini API using Go

Source: https://ai.google.dev/gemini-api/docs/image-understanding_hl=es-419

This Go snippet prepares a prompt with text and two images for the Gemini API. One image is uploaded via the client's file service, and the other is read from disk as bytes and included inline. It leverages the Google Generative AI Go client library for interaction.

```go
image1Path := "path/to/image1.jpg"
uploadedFile, _ := client.Files.UploadFromPath(ctx, image1Path, nil)

// Prepare the second image as inline data
image2Path := "path/to/image2.jpeg"
imgBytes, _ := os.ReadFile(image2Path)

parts := []*genai.Part{
  genai.NewPartFromText("What is different between these two images?"),
  genai.NewPartFromBytes(imgBytes, "image/jpeg"),
  genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

--------------------------------

### Generate Content with Gemini API from Image (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview

This Python code snippet loads an image from a specified path, constructs a detailed prompt for object detection and instruction generation, and then calls the Gemini API's `generate_content` method. The model analyzes the image and prompt to return instructions for packing a lunchbox, including coordinate-based object labeling. It requires the Google Gemini API client library.

```python
with open('path/to/image-of-lunch.jpg', 'rb') as f:
    image_bytes = f.read()

prompt = """
          Explain how to pack the lunch box and lunch bag. Point to each
          object that you refer to. Each point should be in the format:
          [{\"point\": [y, x], \"label\": }], where the coordinates are
          normalized between 0-1000.
        """

image_response = client.models.generate_content(
  model=MODEL_ID,
  contents=[
    types.Part.from_bytes(
      data=image_bytes,
      mime_type='image/jpeg',
    ),
    prompt
  ],
  config = types.GenerateContentConfig(
      temperature=0.5,
      thinking_config=types.ThinkingConfig(thinking_budget=0)
  )
)

print(image_response.text)
```

--------------------------------

### Process WAV Audio with Gemini Live API in Python

Source: https://ai.google.dev/gemini-api/docs/live

This Python example demonstrates how to read a WAV audio file, prepare it in the required 16-bit PCM, 16kHz, mono format for the Gemini Live API, and save the received audio data. It uses `asyncio`, `librosa`, `soundfile`, and the Google Generative AI client library. Dependencies include `librosa` and `soundfile` which need to be installed.

```python
# Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav
# Install helpers for converting files: pip install librosa soundfile
import asyncio
import io
from pathlib import Path
import wave
from google import genai
from google.genai import types
import soundfile as sf
import librosa

client = genai.Client()
```

--------------------------------

### Count Tokens for Audio Content

Source: https://ai.google.dev/gemini-api/docs/audio_hl=fa

These snippets demonstrate how to count the total number of tokens for an audio file using the Google Gemini API across Python, JavaScript, and Go. The `countTokens` method is called with the model name and the audio content (or a file reference).

```Python
from google import genai

client = genai.Client()
# 'myfile' would typically be an uploaded file object or similar content reference
response = client.models.count_tokens(
  model='gemini-2.5-flash',
  contents=[myfile]
)

print(response)
```

```JavaScript
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

// Upload the audio file first
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3", // Replace with your actual file path
  config: { mimeType: "audio/mpeg" },
});

const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.5-flash",
  contents: createUserContent([
    createPartFromUri(myfile.uri, myfile.mimeType),
  ]),
});
console.log(countTokensResponse.totalTokens);
```

```Go
package main

import (
  "context"
  "fmt"
  "log"
  "os"

  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  // Ensure GEMINI_API_KEY is set in your environment
  client, err := genai.NewClient(ctx, os.Getenv("GEMINI_API_KEY"))
  if err != nil {
      log.Fatal(err)
  }
  defer client.Close()

  localAudioPath := "/path/to/sample.mp3"
  // In a real application, you should handle the error for UploadFromPath
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  // In a real application, you should handle the error for CountTokens
  tokens, _ := client.Models.CountTokens(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Printf("File %s is %d tokens\n", localAudioPath, tokens.TotalTokens)
}
```

--------------------------------

### POST /v1beta/openai/chat/completions (Multimodal)

Source: https://ai.google.dev/gemini-api/docs/openai_hl=pt-br

This endpoint allows sending text prompts along with base64 encoded image data to the Gemini API for multimodal chat completions. It's compatible with the OpenAI chat completions interface.

```APIDOC
## POST /v1beta/openai/chat/completions

### Description
Sends a request to the Gemini API for chat completions, supporting multimodal input including text and base64-encoded images.

### Method
POST

### Endpoint
`/v1beta/openai/chat/completions`

### Parameters
#### Request Body
- **model** (string) - Required - The model to use for the completion, e.g., "gemini-2.0-flash".
- **messages** (array of objects) - Required - A list of message objects representing the conversation.
  - **role** (string) - Required - The role of the sender, typically "user".
  - **content** (array of objects) - Required - An array containing text and/or image objects.
    - **type** (string) - Required - Type of content, either "text" or "image_url".
    - **text** (string) - Conditional (if type is "text") - The text prompt or message content.
    - **image_url** (object) - Conditional (if type is "image_url") - Object containing image details.
      - **url** (string) - Required - The base64 encoded image data or a publicly accessible URL, e.g., "data:image/jpeg;base64,{base64_image}".

### Request Example
```json
{
  "model": "gemini-2.0-flash",
  "messages": [
    {
      "role": "user",
      "content": [
        { "type": "text", "text": "What is in this image?" },
        {
          "type": "image_url",
          "image_url": { "url": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" }
        }
      ]
    }
  ]
}
```

### Response
#### Success Response (200)
- **id** (string) - A unique identifier for the chat completion.
- **object** (string) - The type of object, typically "chat.completion".
- **created** (integer) - The Unix timestamp (in seconds) of when the chat completion was created.
- **model** (string) - The model used for the completion.
- **choices** (array of objects) - A list of chat completion choices.
  - **index** (integer) - The index of the choice in the list.
  - **message** (object) - The generated message.
    - **role** (string) - The role of the author of this message, typically "assistant".
    - **content** (string) - The contents of the message.
  - **finish_reason** (string) - The reason the model finished generating tokens.
- **usage** (object) - Usage statistics for the completion request.
  - **prompt_tokens** (integer) - The number of tokens in the prompt.
  - **completion_tokens** (integer) - The number of tokens in the generated completion.
  - **total_tokens** (integer) - The total number of tokens used in the request (prompt + completion).

#### Response Example
```json
{
  "id": "chatcmpl-1234567890abcdef",
  "object": "chat.completion",
  "created": 1678886400,
  "model": "gemini-2.0-flash",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "The image appears to show a [description of image content]."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 50,
    "completion_tokens": 20,
    "total_tokens": 70
  }
}
```
```

--------------------------------

### Install Puppeteer and Chart.js for UI and PDF Generation

Source: https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example

This snippet installs `puppeteer` for browser automation (e.g., PDF generation) and `chart.js` along with its TypeScript type definitions (`@types/chart.js`) for chart rendering. Note that `puppeteer` requires running a script to download the Chromium browser, which may ask for approval.

```bash
npm install puppeteer chart.js
npm install -D @types/chart.js
```

```bash
pnpm add puppeteer chart.js
pnpm add -D @types/chart.js
```

```bash
yarn add puppeteer chart.js
yarn add -D @types/chart.js
```

--------------------------------

### Summarize Local PDF with Gemini API in Go

Source: https://ai.google.dev/gemini-api/docs/document-processing_hl=it

This Go example demonstrates how to read a local PDF file, create an `InlineData` part with its byte content, and send it to the Gemini API for content generation. It uses the `google.golang.org/genai` package for interaction.

```go
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, _ := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey:  os.Getenv("GEMINI_API_KEY"),
        Backend: genai.BackendGeminiAPI,
    })

    pdfBytes, _ := os.ReadFile("path/to/your/file.pdf")

    parts := []*genai.Part{
        &genai.Part{
            InlineData: &genai.Blob{
                MIMEType: "application/pdf",
                Data:     pdfBytes,
            },
        },
        genai.NewPartFromText("Summarize this document"),
    }
    contents := []*genai.Content{
        genai.NewContentFromParts(parts, genai.RoleUser),
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        contents,
        nil,
    )

    fmt.Println(result.Text())
}
```

--------------------------------

### Implement Smart Light Control Function (Mock API)

Source: https://ai.google.dev/gemini-api/docs/function-calling_hl=zh-tw

This code provides the actual implementation of the 'set_light_values' function, acting as a mock API for controlling smart lights. It accepts 'brightness' and 'color_temp' as arguments and returns a dictionary/object representing the set values. This function would be executed based on the AI model's suggestions.

```python
def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:
    """Set the brightness and color temperature of a room light. (mock API).

    Args:
        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness
        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.

    Returns:
        A dictionary containing the set brightness and color temperature.
    """
    return {"brightness": brightness, "colorTemperature": color_temp}
```

```javascript
/**
*   Set the brightness and color temperature of a room light. (mock API)
*   @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness
*   @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.
*   @return {Object} A dictionary containing the set brightness and color temperature.
*/
function setLightValues(brightness, color_temp) {
  return {
    brightness: brightness,
    colorTemperature: color_temp
  };
}
```

--------------------------------

### Define Function Schemas for Disco Party Actions

Source: https://ai.google.dev/gemini-api/docs/function-calling_example=meeting

These code blocks define the schemas for three distinct functions: `power_disco_ball`, `start_music`, and `dim_lights`. Each schema specifies the function's name, a description of its purpose, and the parameters it accepts, including their types and descriptions. These definitions allow the Gemini API to understand and invoke these custom tools.

```python
power_disco_ball = {
    "name": "power_disco_ball",
    "description": "Powers the spinning disco ball.",
    "parameters": {
        "type": "object",
        "properties": {
            "power": {
                "type": "boolean",
                "description": "Whether to turn the disco ball on or off.",
            }
        },
        "required": ["power"],
    },
}

start_music = {
    "name": "start_music",
    "description": "Play some music matching the specified parameters.",
    "parameters": {
        "type": "object",
        "properties": {
            "energetic": {
                "type": "boolean",
                "description": "Whether the music is energetic or not.",
            },
            "loud": {
                "type": "boolean",
                "description": "Whether the music is loud or not.",
            }
        },
        "required": ["energetic", "loud"],
    },
}

dim_lights = {
    "name": "dim_lights",
    "description": "Dim the lights.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "number",
                "description": "The brightness of the lights, 0.0 is off, 1.0 is full.",
            }
        },
        "required": ["brightness"],
    },
}
```

```javascript
import { Type } from '@google/genai';

const powerDiscoBall = {
  name: 'power_disco_ball',
  description: 'Powers the spinning disco ball.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      power: {
        type: Type.BOOLEAN,
        description: 'Whether to turn the disco ball on or off.'
      }
    },
    required: ['power']
  }
};

const startMusic = {
  name: 'start_music',
  description: 'Play some music matching the specified parameters.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      energetic: {
        type: Type.BOOLEAN,
        description: 'Whether the music is energetic or not.'
      },
      loud: {
        type: Type.BOOLEAN,
        description: 'Whether the music is loud or not.'
      }
    },
    required: ['energetic', 'loud']
  }
};

const dimLights = {
  name: 'dim_lights',
  description: 'Dim the lights.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'
      }
    },
    required: ['brightness']
  }
};
```

--------------------------------

### Generate Multi-Speaker TTS Audio and Save to WAV in Python

Source: https://ai.google.dev/gemini-api/docs/speech-generation_hl=de

This Python snippet demonstrates how to use the `google.generativeai` library to perform text-to-speech with multiple speakers. It includes a helper function `wave_file` to save the raw PCM audio data received from the API into a standard WAV file. The example configures distinct prebuilt voices ('Kore' for Joe, 'Puck' for Jane) for a conversational prompt.

```python
import wave
import google.generativeai as genai
import google.generativeai.types as types

def wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):
   with wave.open(filename, "wb") as wf:
      wf.setnchannels(channels)
      wf.setsampwidth(sample_width)
      wf.setframerate(rate)
      wf.writeframes(pcm)

client = genai.Client()

prompt = """TTS the following conversation between Joe and Jane:
         Joe: How's it going today Jane?
         Jane: Not too bad, how about you?"""

response = client.models.generate_content(
   model="gemini-2.5-flash-preview-tts",
   contents=prompt,
   config=types.GenerateContentConfig(
      response_modalities=["AUDIO"],
      speech_config=types.SpeechConfig(
         multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(
            speaker_voice_configs=[
               types.SpeakerVoiceConfig(
                  speaker='Joe',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Kore',
                     )
                  )
               ),
               types.SpeakerVoiceConfig(
                  speaker='Jane',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Puck',
                     )
                  )
               ),
            ]
         )
      )
   )
)

data = response.candidates[0].content.parts[0].inline_data.data

file_name='out.wav'
wave_file(file_name, data) # Saves the file to current directory
```

--------------------------------

### Generate, Poll, and Download Video (Python)

Source: https://ai.google.dev/gemini-api/docs/video_hl=tr

This Python example uses the `google.genai` library to initiate video generation, asynchronously poll the operation status until completion, and then download and save the generated video to a local MP4 file.

```python
import time
from google import genai
from google.genai import types

client = genai.Client()

# After starting the job, you get an operation object.
operation = client.models.generate_videos(
    model="veo-3.0-generate-001",
    prompt="A cinematic shot of a majestic lion in the savannah.",
)

# Alternatively, you can use operation.name to get the operation.
operation = types.GenerateVideosOperation(name=operation.name)

# This loop checks the job status every 10 seconds.
while not operation.done:
    time.sleep(10)
    # Refresh the operation object to get the latest status.
    operation = client.operations.get(operation)

# Once done, the result is in operation.response.
generated_video = operation.response.generated_videos[0]
client.files.download(file=generated_video.video)
generated_video.video.save("parameters_example.mp4")
print("Generated video saved to parameters_example.mp4")
```

--------------------------------

### Analyze Image Content with Gemini API (Python, JavaScript, cURL)

Source: https://ai.google.dev/gemini-api/docs/openai_hl=es-419

These snippets demonstrate how to encode a local image into a base64 string and send it to the Gemini API's chat completions endpoint for content analysis. The API key and base URL are configured to use Gemini via the OpenAI client library. The response contains a description of the image content.

```python
# Function to encode the image
def encode_image(image_path):
  with open(image_path, "rb") as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

# Getting the base64 string
base64_image = encode_image("Path/to/agi/image.jpeg")

response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url":  f"data:image/jpeg;base64,{base64_image}"
          },
        },
      ],
    }
  ],
)

print(response.choices[0])
```

```javascript
import OpenAI from "openai";
import fs from 'fs/promises';

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function encodeImage(imagePath) {
  try {
    const imageBuffer = await fs.readFile(imagePath);
    return imageBuffer.toString('base64');
  } catch (error) {
    console.error("Error encoding image:", error);
    return null;
  }
}

async function main() {
  const imagePath = "Path/to/agi/image.jpeg";
  const base64Image = await encodeImage(imagePath);

  const messages = [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url": `data:image/jpeg;base64,${base64Image}`
          },
        },
      ],
    }
  ];

  try {
    const response = await openai.chat.completions.create({
      model: "gemini-2.0-flash",
      messages: messages,
    });

    console.log(response.choices[0]);
  } catch (error) {
    console.error("Error calling Gemini API:", error);
  }
}

main();
```

```bash
bash -c '
  base64_image=$(base64 -i "Path/to/agi/image.jpeg");
  curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer GEMINI_API_KEY" \
    -d "{
      \"model\": \"gemini-2.0-flash\",
      \"messages\": [
        {
          \"role\": \"user\",
          \"content\": [
            { \"type\": \"text\", \"text\": \"What is in this image?\" },
            {
              \"type\": \"image_url\",
              \"image_url\": { \"url\": \"data:image/jpeg;base64,${base64_image}\" }
            }
          ]
        }
      ]
    }"'

```

--------------------------------

### Example Prompt for Van Gogh Style Transfer with Gemini API

Source: https://ai.google.dev/gemini-api/docs/image-generation_hl=de

This concrete example demonstrates a style transfer prompt, instructing the Gemini API to transform a city street image into Vincent van Gogh's 'Starry Night' style. It details the preservation of composition while applying specific visual effects like swirling brushstrokes and a dramatic color palette.

```text
"Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows."
```

--------------------------------

### Define Function Declaration and Implementation for Smart Lights

Source: https://ai.google.dev/gemini-api/docs/function-calling_example=weather&hl=hi

This snippet illustrates how to define a function declaration for the Gemini API, including its name, description, and parameter schema, and implement the corresponding function to control smart lights. It showcases both Python and JavaScript implementations for API consumption and the actual function logic.

```python
set_light_values_declaration = {
    "name": "set_light_values",
    "description": "Sets the brightness and color temperature of a light.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "integer",
                "description": "Light level from 0 to 100. Zero is off and 100 is full brightness",
            },
            "color_temp": {
                "type": "string",
                "enum": ["daylight", "cool", "warm"],
                "description": "Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.",
            },
        },
        "required": ["brightness", "color_temp"],
    },
}

# This is the actual function that would be called based on the model's suggestion
def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:
    """Set the brightness and color temperature of a room light. (mock API).

    Args:
        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness
        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.

    Returns:
        A dictionary containing the set brightness and color temperature.
    """
    return {"brightness": brightness, "colorTemperature": color_temp}
```

```javascript
import { Type } from '@google/genai';

// Define a function that the model can call to control smart lights
const setLightValuesFunctionDeclaration = {
  name: 'set_light_values',
  description: 'Sets the brightness and color temperature of a light.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',
      },
      color_temp: {
        type: Type.STRING,
        enum: ['daylight', 'cool', 'warm'],
        description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',
      },
    },
    required: ['brightness', 'color_temp'],
  },
};

/**

*   Set the brightness and color temperature of a room light. (mock API)
*   @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness
*   @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.
*   @return {Object} A dictionary containing the set brightness and color temperature.
*/
function setLightValues(brightness, color_temp) {
  return {
    brightness: brightness,
    colorTemperature: color_temp
  };
}
```

--------------------------------

### Configure Gemini API Key in Python

Source: https://ai.google.dev/gemini-api/docs/langgraph-example

This Python code snippet demonstrates how to retrieve the Google Gemini API key from an environment variable. It's crucial for authenticating requests to the Gemini API when interacting with the LLM, ensuring secure access to Google's generative models.

```python
import os

# Read your API key from the environment variable or set it manually
api_key = os.getenv("GEMINI_API_KEY")
```

--------------------------------

### Generate and Download Video from Text Prompt (Multi-language)

Source: https://ai.google.dev/gemini-api/docs/video_example=dialogue&hl=th

This collection of examples demonstrates how to generate a video from a text prompt using the Gemini API, poll for the operation's completion, and then download the final video file. It covers JavaScript, Go, and cURL/Shell.

```javascript
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

let operation = await ai.models.generateVideos({
    model: "veo-3.0-generate-001",
    prompt: prompt,
});

// Poll the operation status until the video is ready.
while (!operation.done) {
    console.log("Waiting for video generation to complete...")
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
        operation: operation,
    });
}

// Download the generated video.
ai.files.download({
    file: operation.response.generatedVideos[0].video,
    downloadPath: "dialogue_example.mp4",
});
console.log(`Generated video saved to dialogue_example.mp4`);

```

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
    A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-3.0-generate-001",
        prompt,
        nil,
        nil,
    )

    // Poll the operation status until the video is ready.
    for !operation.Done {
    log.Println("Waiting for video generation to complete...")
        time.Sleep(10 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    // Download the generated video.
    video := operation.Response.GeneratedVideos[0]
    client.Files.Download(ctx, video.Video, nil)
    fname := "dialogue_example.mp4"
    _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    log.Printf("Generated video saved to %s\n", fname)
}

```

```shell
# Note: This script uses jq to parse the JSON response.
# GEMINI API Base URL
BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Send request to generate video and capture the operation name into a variable.
operation_name=$(curl -s "${BASE_URL}/models/veo-3.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That's the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
      }
    ]
  }' | jq -r .name)

# Poll the operation status until the video is ready
while true; do
  # Get the full JSON status and store it in a variable.
  status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

  # Check the "done" field from the JSON stored in the variable.
  is_done=$(echo "${status_response}" | jq .done)

  if [ "${is_done}" = "true" ]; then
    # Extract the download URI from the final response.
    video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
    echo "Downloading video from: ${video_uri}"

    # Download the video using the URI and API key and follow redirects.
    curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
    break
  fi
  # Wait for 5 seconds before checking again.
  sleep 10
done

```

--------------------------------

### POST /v1beta/models/gemini-embedding-001:embedContent

Source: https://ai.google.dev/gemini-api/docs/embeddings_hl=ja

Generates vector embeddings for input content using a specified Gemini embedding model. These embeddings are numerical representations of text that capture semantic meaning, enabling tasks like similarity comparison, classification, and clustering.

```APIDOC
## POST /v1beta/models/gemini-embedding-001:embedContent

### Description
Generates vector embeddings for input content using a specified Gemini embedding model. These embeddings are numerical representations of text that capture semantic meaning, enabling tasks like similarity comparison, classification, and clustering.

### Method
POST

### Endpoint
/v1beta/models/gemini-embedding-001:embedContent

### Parameters
#### Request Body
- **task_type** (string) - Required - The task type for which the embeddings will be used. This parameter helps optimize the embedding generation for specific use cases. Supported values include:
  - **SEMANTIC_SIMILARITY**: Optimized for evaluating text similarity (e.g., recommendation systems, duplicate detection).
  - **CLASSIFICATION**: Optimized for classifying text according to preset labels (e.g., sentiment analysis, spam detection).
  - **CLUSTERING**: Optimized for clustering text based on similarity (e.g., document organization, market research, anomaly detection).
  - **RETRIEVAL_DOCUMENT**: Optimized for document retrieval (e.g., indexing articles, books, web pages for search).
  - **RETRIEVAL_QUERY**: Optimized for general search queries. Use `RETRIEVAL_QUERY` for the query and `RETRIEVAL_DOCUMENT` for documents.
  - **CODE_RETRIEVAL_QUERY**: Optimized for retrieving code blocks based on natural language queries. Use `CODE_RETRIEVAL_QUERY` for the query and `RETRIEVAL_DOCUMENT` for code blocks.
  - **QUESTION_ANSWERING**: Optimized for questions in question-answering systems to retrieve relevant documents. Use `QUESTION_ANSWERING` for the question and `RETRIEVAL_DOCUMENT` for documents.
  - **FACT_VERIFICATION**: Optimized for statements needing verification to retrieve evidence. Use `FACT_VERIFICATION` for the target text and `RETRIEVAL_DOCUMENT` for documents.
- **content** (object) - Required - The content to be embedded.
  - **parts** (array of objects) - Required - An array of content parts.
    - **text** (string) - Required - The text content to embed.

### Request Example
```json
{
  "task_type": "SEMANTIC_SIMILARITY",
  "content": {
    "parts":[
      {
        "text": "What is the meaning of life?"
      }, 
      {
        "text": "How much wood would a woodchuck chuck?"
      }, 
      {
        "text": "How does the brain work?"
      }
    ]
  }
}
```

### Response
#### Success Response (200)
- **embeddings** (array of objects) - An array containing the generated embeddings for each input content part.
  - **values** (array of numbers) - The numerical vector representation of the embedded content.

#### Response Example
```json
{
  "embeddings": [
    {
      "values": [0.012345, -0.005678, 0.034567, ..., -0.012345]
    },
    {
      "values": [-0.021098, 0.018765, -0.010987, ..., 0.023456]
    },
    {
      "values": [0.003456, 0.000123, 0.025678, ..., -0.004567]
    }
  ]
}
```
```

--------------------------------

### Send Multimodal Input (Text and Image) to Gemini API

Source: https://ai.google.dev/gemini-api/docs/text-generation_hl=fa&lang=python

These examples illustrate how to send multimodal input, specifically combining text with an image, to the Gemini API. Each snippet shows a different programming language's approach to preparing image data (e.g., local file, URI, base64 encoding) and sending it alongside a text prompt to the Gemini 2.5 Flash model. Ensure proper API key setup and file paths are configured.

```python
from PIL import Image
from google import genai

client = genai.Client()

image = Image.open("/path/to/organ.png")
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[image, "Tell me about this instrument"]
)
print(response.text)
```

```javascript
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const image = await ai.files.upload({
    file: "/path/to/organ.png",
  });
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
      createUserContent([
        "Tell me about this instrument",
        createPartFromUri(image.uri, image.mimeType),
      ]),
    ],
  });
  console.log(response.text);
}

await main();
```

```go
package main

import (
  "context"
  "fmt"
  "os"
  "log"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/organ.jpg"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
      genai.NewPartFromText("Tell me about this instrument"),
      &genai.Part{
          InlineData: &genai.Blob{
              MIMEType: "image/jpeg",
              Data:     imgData,
          },
      },
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

```bash
# Use a temporary file to hold the base64 encoded image data
TEMP_B64=$(mktemp)
trap 'rm -f "$TEMP_B64"' EXIT
base64 $B64FLAGS $IMG_PATH > "$TEMP_B64"

# Use a temporary file to hold the JSON payload
TEMP_JSON=$(mktemp)
trap 'rm -f "$TEMP_JSON"' EXIT

cat > "$TEMP_JSON" << EOF
{
  "contents": [
    {
      "parts": [
        {
          "text": "Tell me about this instrument"
        },
        {
          "inline_data": {
            "mime_type": "image/jpeg",
            "data": "$(cat "$TEMP_B64")"
          }
        }
      ]
    }
  ]
}
EOF

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d "@$TEMP_JSON"
```

```javascript
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const imageUrl = 'http://image/url';
  const image = getImageData(imageUrl);
  const payload = {
    contents: [
      {
        parts: [
          { image },
          { text: 'Tell me about this instrument' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}

function getImageData(url) {
  const blob = UrlFetchApp.fetch(url).getBlob();

  return {
    mimeType: blob.getContentType(),
    data: Utilities.base64Encode(blob.getBytes())
  };
}
```

--------------------------------

### Generate Structured JSON Output from Gemini API (Python, JavaScript)

Source: https://ai.google.dev/gemini-api/docs/openai

This snippet shows how to define a schema (Pydantic in Python, Zod in JavaScript) for desired output and instruct the Gemini API to parse text into that specific JSON structure. It utilizes the `response_format` parameter in the `chat.completions.parse` method to ensure the model returns data conforming to the defined type.

```python
from pydantic import BaseModel
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

completion = client.beta.chat.completions.parse(
    model="gemini-2.0-flash",
    messages=[
        {"role": "system", "content": "Extract the event information."},
        {"role": "user", "content": "John and Susan are going to an AI conference on Friday."},
    ],
    response_format=CalendarEvent,
)

print(completion.choices[0].message.parsed)
```

```javascript
import OpenAI from "openai";
import { zodResponseFormat } from "openai/helpers/zod";
import { z } from "zod";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai"
});

const CalendarEvent = z.object({
  name: z.string(),
  date: z.string(),
  participants: z.array(z.string()),
});

const completion = await openai.chat.completions.parse({
  model: "gemini-2.0-flash",
  messages: [
    { role: "system", content: "Extract the event information." },
    { role: "user", content: "John and Susan are going to an AI conference on Friday" },
  ],
  response_format: zodResponseFormat(CalendarEvent, "event"),
});

const event = completion.choices[0].message.parsed;
console.log(event);
```

--------------------------------

### Identify Objects in an Image using Gemini Robotics-ER 1.5 (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview

This Python example demonstrates how to use the Gemini Robotics-ER 1.5 model to identify objects within an image. It takes an image file and a structured text prompt as input, and returns a JSON string containing a list of detected objects with their normalized 2D coordinates and labels. This output can then be used with robotics APIs or other vision-language-action models.

```python
from google import genai
from google.genai import types

# Initialize the GenAI client and specify the model
MODEL_ID = "gemini-robotics-er-1.5-preview"
PROMPT = """
          Point to no more than 10 items in the image. The label returned
          should be an identifying name for the object detected.
          The answer should follow the json format: [{"point": <point>,
          "label": <label1>}, ...]. The points are in [y, x] format
          normalized to 0-1000.
        """
client = genai.Client(api_key=YOUR_API_KEY)

# Load your image
with open("my-image.png", 'rb') as f:
    image_bytes = f.read()

image_response = client.models.generate_content(
    model=MODEL_ID,
    contents=[
        types.Part.from_bytes(
            data=image_bytes,
            mime_type='image/png',
        ),
        PROMPT
    ],
    config = types.GenerateContentConfig(
        temperature=0.5,
        thinking_config=types.ThinkingConfig(thinking_budget=0)
    )
)

print(image_response.text)
```

--------------------------------

### Install Google GenAI SDK for Go

Source: https://ai.google.dev/gemini-api/docs/libraries

Instructions for installing the official Google GenAI SDK for Go using the `go get` command. This library provides the recommended interface for the Gemini API in Go applications, ensuring stability and up-to-date features.

```go
go get google.golang.org/genai
```

--------------------------------

### Initialize Playwright Browser with Custom Dimensions

Source: https://ai.google.dev/gemini-api/docs/computer-use_hl=pt-br

This code defines screen dimensions and initializes a Playwright Chromium browser instance. It sets up a new browser context with a specified viewport size, enabling controlled web page interactions for an AI agent.

```python
SCREEN_WIDTH = 1440
SCREEN_HEIGHT = 900

# Setup Playwright
print("Initializing browser...")
playwright = sync_playwright().start()
browser = playwright.chromium.launch(headless=False)
context = browser.new_context(viewport={"width": SCREEN_WIDTH, "height": SCREEN_HEIGHT})
page = context.new_page()
```

--------------------------------

### Declare Enum Types Using Python Literal

Source: https://ai.google.dev/gemini-api/docs/json-mode_hl=hi

This Python snippet demonstrates an alternative way to define an enumeration using `Literal` for type declarations, which can be used to specify allowed values for structured responses in API calls.

```python
Literal["Percussion", "String", "Woodwind", "Brass", "Keyboard"]
```

--------------------------------

### Multilingual: Full Workflow for Generating Content with Uploaded and Inline Images

Source: https://ai.google.dev/gemini-api/docs/vision_hl=vi

These examples provide a complete, end-to-end workflow for interacting with the Gemini API to generate content from a text prompt and two images. They demonstrate how to upload one image (Node.js, Go, cURL), prepare a second image as inline base64 data, and then combine these with a text prompt in a single `generateContent` request to the `gemini-2.5-flash` model. The model is prompted to identify differences between the provided images.

```javascript
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});

async function main() {
  // Upload the first image
  const image1_path = "path/to/image1.jpg";
  const uploadedFile = await ai.files.upload({
    file: image1_path,
    config: { mimeType: "image/jpeg" },
  });

  // Prepare the second image as inline data
  const image2_path = "path/to/image2.png";
  const base64Image2File = fs.readFileSync(image2_path, {
    encoding: "base64",
  });

  // Create the prompt with text and multiple images

  const response = await ai.models.generateContent({

    model: "gemini-2.5-flash",
    contents: createUserContent([
      "What is different between these two images?",
      createPartFromUri(uploadedFile.uri, uploadedFile.mimeType),
      {
        inlineData: {
          mimeType: "image/png",
          data: base64Image2File,
        },
      },
    ]),
  });
  console.log(response.text);
}

await main();
```

```go
// Upload the first image
image1Path := "path/to/image1.jpg"
uploadedFile, _ := client.Files.UploadFromPath(ctx, image1Path, nil)

// Prepare the second image as inline data
image2Path := "path/to/image2.jpeg"
imgBytes, _ := os.ReadFile(image2Path)

parts := []*genai.Part{
  genai.NewPartFromText("What is different between these two images?"),
  genai.NewPartFromBytes(imgBytes, "image/jpeg"),
  genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

```bash
# Upload the first image
IMAGE1_PATH="path/to/image1.jpg"
MIME1_TYPE=$(file -b --mime-type "${IMAGE1_PATH}")
NUM1_BYTES=$(wc -c < "${IMAGE1_PATH}")
DISPLAY_NAME1=IMAGE1

tmp_header_file1=upload-header1.tmp

curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header1.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM1_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME1_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME1}'}}" 2> /dev/null

upload_url1=$(grep -i "x-goog-upload-url: " "${tmp_header_file1}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file1}"

curl "${upload_url1}" \
  -H "Content-Length: ${NUM1_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${IMAGE1_PATH}" 2> /dev/null > file_info1.json

file1_uri=$(jq ".file.uri" file_info1.json)
echo file1_uri=$file1_uri

# Prepare the second image (inline)
IMAGE2_PATH="path/to/image2.png"
MIME2_TYPE=$(file -b --mime-type "${IMAGE2_PATH}")

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi
IMAGE2_BASE64=$(base64 $B64FLAGS $IMAGE2_PATH)

# Now generate content using both images
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "What is different between these two images?"},
          {"file_data":{"mime_type": "'"${MIME1_TYPE}"'", "file_uri": '$file1_uri'}},
          {
            "inline_data": {
              "mime_type":"'"${MIME2_TYPE}"'",
              "data": "'"$IMAGE2_BASE64"'"
            }
          }
        ]
      }]
    }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

--------------------------------

### POST /v1beta/openai/embeddings

Source: https://ai.google.dev/gemini-api/docs/openai

Generates a vector representation (embedding) for a given input text string using a specified model. These embeddings measure the relatedness of text.

```APIDOC
## POST /v1beta/openai/embeddings

### Description
Generates a vector representation (embedding) for a given input text string using a specified model. These embeddings measure the relatedness of text.

### Method
POST

### Endpoint
https://generativelanguage.googleapis.com/v1beta/openai/embeddings

### Parameters
#### Request Body
- **input** (string) - Required - The text string for which to generate an embedding.
- **model** (string) - Required - The identifier of the model to use for embedding generation (e.g., "gemini-embedding-001").

### Request Example
```json
{
  "input": "Your text string goes here",
  "model": "gemini-embedding-001"
}
```

### Response
#### Success Response (200)
- **data** (array of object) - A list of embedding objects.
    - **index** (integer) - The index of the embedding in the list.
    - **embedding** (array of float) - The embedding vector for the input text.
- **model** (string) - The model used to generate the embeddings.

#### Response Example
```json
{
  "data": [
    {
      "embedding": [
        0.0023064255,
        -0.009363584,
        0.015090717,
        ...
      ],
      "index": 0
    }
  ],
  "model": "gemini-embedding-001"
}
```
```

--------------------------------

### Define Prompt for Abstract Object Category Detection (Python)

Source: https://ai.google.dev/gemini-api/docs/robotics-overview_hl=th

This Python snippet defines a prompt string to instruct the Gemini model to identify objects belonging to an abstract category, such as 'fruit'. The prompt specifies that the output should include point coordinates and an identifying label for each detected instance, formatted as a JSON array with normalized point values.

```python
prompt = f"""
        Get all points for fruit. The label returned should be an identifying
        name for the object detected.
        """ + """The answer should follow the json format:
        [{{"point": <point>, "label": <label1>}}, ...]. The points are in
        [y, x] format normalized to 0-1000."""
```

--------------------------------

### Generate Content with Multimodal Input (Image and Text) using Gemini API

Source: https://ai.google.dev/gemini-api/docs/text-generation_hl=bn

These examples illustrate how to send multimodal input (an image and a text prompt) to the Gemini API to generate a response. They demonstrate the process of including image data alongside textual queries for models like `gemini-2.5-flash`. Each language handles image loading and API request construction differently, but all aim to send combined image and text data for interpretation.

```Python
from PIL import Image
from google import genai

client = genai.Client()

image = Image.open("/path/to/organ.png")
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[image, "Tell me about this instrument"]
)
print(response.text)
```

```JavaScript
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const image = await ai.files.upload({
    file: "/path/to/organ.png",
  });
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
      createUserContent([
        "Tell me about this instrument",
        createPartFromUri(image.uri, image.mimeType),
      ]),
    ],
  });
  console.log(response.text);
}

await main();
```

```Go
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/organ.jpg"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
      genai.NewPartFromText("Tell me about this instrument"),
      &genai.Part{
          InlineData: &genai.Blob{
              MIMEType: "image/jpeg",
              Data:     imgData,
          },
      },
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

```REST
# Use a temporary file to hold the base64 encoded image data
TEMP_B64=$(mktemp)
trap 'rm -f "$TEMP_B64"' EXIT
base64 $B64FLAGS $IMG_PATH > "$TEMP_B64"

# Use a temporary file to hold the JSON payload
TEMP_JSON=$(mktemp)
trap 'rm -f "$TEMP_JSON"' EXIT

cat > "$TEMP_JSON" << EOF
{
  "contents": [
    {
      "parts": [
        {
          "text": "Tell me about this instrument"
        },
        {
          "inline_data": {
            "mime_type": "image/jpeg",
            "data": "$(cat "$TEMP_B64")"
          }
        }
      ]
    }
  ]
}
EOF

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d "@$TEMP_JSON"
```

```Apps Script
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const imageUrl = 'http://image/url';
  const image = getImageData(imageUrl);
  const payload = {
    contents: [
      {
        parts: [
          { image },
          { text: 'Tell me about this instrument' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}

function getImageData(url) {
  const blob = UrlFetchApp.fetch(url).getBlob();

  return {
    mimeType: blob.getContentType(),
    data: Utilities.base64Encode(blob.getBytes())
  };
}
```

--------------------------------

### Define Screen Dimensions Constants in Python

Source: https://ai.google.dev/gemini-api/docs/computer-use_hl=pl

Defines SCREEN_WIDTH and SCREEN_HEIGHT constants for consistent browser viewport sizing in the Playwright setup. These values are used to configure the browser context.

```python
SCREEN_WIDTH = 1440
SCREEN_HEIGHT = 900
```